{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 1\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_realistic(y, u):\n",
    "    # split dataset into arrays of each class label\n",
    "    all_indices = [i for i, d in enumerate(y)]\n",
    "    shares_arr = [5000, 3000, 1000, 750, 250]\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append([all_indices.pop(0) for i in range(shares_arr[u])]) \n",
    "    return user_indices\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    #indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = np.array(source[1][:10000], dtype=np.int32)\n",
    "    X = np.array(source[0][:10000], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'i':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    elif distribution == 'n':\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    elif distribution == 'r':\n",
    "        indices = get_indices_realistic(source[1][:10000], u)[u]\n",
    "    else:\n",
    "        indices = np.array(get_indices_unbalanced_completely(source[1])[u])\n",
    "    \n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': np.array([source[1][b] for b in batch_samples], dtype=np.int32)})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'i') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'i') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=loss_fn,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 1.6453 - categorical_accuracy: 0.4105 - val_loss: 1.4564 - val_categorical_accuracy: 0.4887\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.2948 - categorical_accuracy: 0.5413 - val_loss: 1.3070 - val_categorical_accuracy: 0.5377\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.0989 - categorical_accuracy: 0.6127 - val_loss: 1.2251 - val_categorical_accuracy: 0.5706\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.9609 - categorical_accuracy: 0.6592 - val_loss: 1.2384 - val_categorical_accuracy: 0.5791\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.8286 - categorical_accuracy: 0.7077 - val_loss: 1.2404 - val_categorical_accuracy: 0.5842\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.6916 - categorical_accuracy: 0.7620 - val_loss: 1.2497 - val_categorical_accuracy: 0.5907\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.5374 - categorical_accuracy: 0.8224 - val_loss: 1.3085 - val_categorical_accuracy: 0.5914\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4065 - categorical_accuracy: 0.8718 - val_loss: 1.3898 - val_categorical_accuracy: 0.5857\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.2734 - categorical_accuracy: 0.9245 - val_loss: 1.4569 - val_categorical_accuracy: 0.5969\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1703 - categorical_accuracy: 0.9636 - val_loss: 1.5415 - val_categorical_accuracy: 0.5924\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.1021 - categorical_accuracy: 0.9831 - val_loss: 1.6110 - val_categorical_accuracy: 0.5882\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.0585 - categorical_accuracy: 0.9945 - val_loss: 1.6453 - val_categorical_accuracy: 0.5936\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0314 - categorical_accuracy: 0.9982 - val_loss: 1.7216 - val_categorical_accuracy: 0.5952\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0179 - categorical_accuracy: 0.9996 - val_loss: 1.7587 - val_categorical_accuracy: 0.6038\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0107 - categorical_accuracy: 0.9999 - val_loss: 1.8129 - val_categorical_accuracy: 0.6051\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8519 - val_categorical_accuracy: 0.6043\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0058 - categorical_accuracy: 1.0000 - val_loss: 1.9056 - val_categorical_accuracy: 0.6017\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0044 - categorical_accuracy: 1.0000 - val_loss: 1.9266 - val_categorical_accuracy: 0.6014\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0046 - categorical_accuracy: 0.9999 - val_loss: 1.9657 - val_categorical_accuracy: 0.6035\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 1.9959 - val_categorical_accuracy: 0.6053\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 2.0312 - val_categorical_accuracy: 0.6022\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 2.0627 - val_categorical_accuracy: 0.6027\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 2.0921 - val_categorical_accuracy: 0.6026\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 2.1258 - val_categorical_accuracy: 0.6026\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 9.9374e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1454 - val_categorical_accuracy: 0.6023\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 8.2107e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1764 - val_categorical_accuracy: 0.6021\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 6.8230e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2064 - val_categorical_accuracy: 0.6013\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 5.7202e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2441 - val_categorical_accuracy: 0.6027\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7293e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2656 - val_categorical_accuracy: 0.6025\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 4.0281e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2940 - val_categorical_accuracy: 0.6022\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.2552e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3168 - val_categorical_accuracy: 0.6021\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.7323e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3425 - val_categorical_accuracy: 0.6016\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 2.2841e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3709 - val_categorical_accuracy: 0.6003\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.9136e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3944 - val_categorical_accuracy: 0.6014\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.6010e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4202 - val_categorical_accuracy: 0.6012\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3392e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4532 - val_categorical_accuracy: 0.6016\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1291e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4735 - val_categorical_accuracy: 0.6004\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 9.5869e-05 - categorical_accuracy: 1.0000 - val_loss: 2.4977 - val_categorical_accuracy: 0.6009\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 7.9403e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5172 - val_categorical_accuracy: 0.6002\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 6.7607e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5467 - val_categorical_accuracy: 0.6008\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 5.8769e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5714 - val_categorical_accuracy: 0.6007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7460e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5931 - val_categorical_accuracy: 0.6008\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 3.9733e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6141 - val_categorical_accuracy: 0.5998\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.3674e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6374 - val_categorical_accuracy: 0.6006\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 2.8456e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6608 - val_categorical_accuracy: 0.6018\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 2.3901e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6810 - val_categorical_accuracy: 0.6009\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 2.0279e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7009 - val_categorical_accuracy: 0.6002\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.7068e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7247 - val_categorical_accuracy: 0.5999\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 1.4409e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7476 - val_categorical_accuracy: 0.5995\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.2274e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7663 - val_categorical_accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 444us/sample - loss: 1.7413 - sparse_categorical_accuracy: 0.3879\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.3325,loss=1.8894957>\n",
      "test accuracy: 0.3878999948501587\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "# test_metrics = evaluation(state.model, federated_test_data)\n",
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "(loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 512us/sample - loss: 1.6099 - sparse_categorical_accuracy: 0.4398\n",
      "round  0, metrics=<sparse_categorical_accuracy=0.3254,loss=1.8737928>\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.4950 - sparse_categorical_accuracy: 0.4791\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.4247,loss=1.6241635>\n",
      "10000/10000 [==============================] - 4s 391us/sample - loss: 1.4163 - sparse_categorical_accuracy: 0.5053\n",
      "round  2, metrics=<sparse_categorical_accuracy=0.4699,loss=1.5011286>\n",
      "10000/10000 [==============================] - 4s 355us/sample - loss: 1.3492 - sparse_categorical_accuracy: 0.5295\n",
      "round  3, metrics=<sparse_categorical_accuracy=0.5081,loss=1.400762>\n",
      "10000/10000 [==============================] - 4s 361us/sample - loss: 1.3037 - sparse_categorical_accuracy: 0.5456\n",
      "round  4, metrics=<sparse_categorical_accuracy=0.5372,loss=1.3147193>\n",
      "10000/10000 [==============================] - 4s 363us/sample - loss: 1.2635 - sparse_categorical_accuracy: 0.5595\n",
      "round  5, metrics=<sparse_categorical_accuracy=0.565,loss=1.2417524>\n",
      "10000/10000 [==============================] - 4s 359us/sample - loss: 1.2287 - sparse_categorical_accuracy: 0.5747\n",
      "round  6, metrics=<sparse_categorical_accuracy=0.5894,loss=1.1811721>\n",
      "10000/10000 [==============================] - 4s 362us/sample - loss: 1.2168 - sparse_categorical_accuracy: 0.5807\n",
      "round  7, metrics=<sparse_categorical_accuracy=0.6082,loss=1.1267872>\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 1.2048 - sparse_categorical_accuracy: 0.5853\n",
      "round  8, metrics=<sparse_categorical_accuracy=0.6274,loss=1.0763007>\n",
      "10000/10000 [==============================] - 4s 358us/sample - loss: 1.1933 - sparse_categorical_accuracy: 0.5906\n",
      "round  9, metrics=<sparse_categorical_accuracy=0.6462,loss=1.0278834>\n",
      "10000/10000 [==============================] - 4s 377us/sample - loss: 1.1932 - sparse_categorical_accuracy: 0.5918\n",
      "round 10, metrics=<sparse_categorical_accuracy=0.6627,loss=0.9857488>\n",
      "10000/10000 [==============================] - 4s 364us/sample - loss: 1.1947 - sparse_categorical_accuracy: 0.5935\n",
      "round 11, metrics=<sparse_categorical_accuracy=0.6773,loss=0.94307834>\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "epochs = 12\n",
    "state = iterative_process.initialize()\n",
    "fd_test_loss = []\n",
    "fd_train_loss = []\n",
    "fd_train_accuracy = []\n",
    "for round_num in range(12):\n",
    "    selected = np.random.choice(5, 5, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "    non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "    (loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "    fd_train_loss.append(metrics[1])\n",
    "    fd_test_accuracy.append(accuracy)\n",
    "    fd_test_loss.append(loss)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4527,\n",
       " 0.4398,\n",
       " 0.4791,\n",
       " 0.5053,\n",
       " 0.5295,\n",
       " 0.5456,\n",
       " 0.5595,\n",
       " 0.5747,\n",
       " 0.5807,\n",
       " 0.5853,\n",
       " 0.5906,\n",
       " 0.5918,\n",
       " 0.5935]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1190 - acc: 0.2440 - val_loss: 1.9590 - val_acc: 0.2846\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7290 - acc: 0.3980 - val_loss: 1.8585 - val_acc: 0.3410\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5974 - acc: 0.4520 - val_loss: 1.7614 - val_acc: 0.3788\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4253 - acc: 0.5290 - val_loss: 1.6777 - val_acc: 0.3944\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3012 - acc: 0.5550 - val_loss: 1.7050 - val_acc: 0.3966\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1126 - acc: 0.6500 - val_loss: 1.7451 - val_acc: 0.3902\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0079 - acc: 0.6770 - val_loss: 1.7554 - val_acc: 0.4156\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8570 - acc: 0.7340 - val_loss: 1.7414 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6845 - acc: 0.7880 - val_loss: 1.7564 - val_acc: 0.4180\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5715 - acc: 0.8260 - val_loss: 1.8183 - val_acc: 0.4128\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4442 - acc: 0.8840 - val_loss: 1.8667 - val_acc: 0.4148\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3486 - acc: 0.9150 - val_loss: 1.9364 - val_acc: 0.4170\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1904 - acc: 0.2270 - val_loss: 1.8969 - val_acc: 0.3414\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7393 - acc: 0.3940 - val_loss: 1.7504 - val_acc: 0.3786\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5385 - acc: 0.4590 - val_loss: 1.7045 - val_acc: 0.3994\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4037 - acc: 0.5270 - val_loss: 1.7093 - val_acc: 0.3978\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2525 - acc: 0.5940 - val_loss: 1.6606 - val_acc: 0.4252\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1121 - acc: 0.6220 - val_loss: 1.7862 - val_acc: 0.3834\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9881 - acc: 0.6790 - val_loss: 1.7424 - val_acc: 0.4152\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8295 - acc: 0.7400 - val_loss: 1.7525 - val_acc: 0.4222\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6734 - acc: 0.7910 - val_loss: 1.8202 - val_acc: 0.4188\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5470 - acc: 0.8420 - val_loss: 1.9658 - val_acc: 0.4050\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4330 - acc: 0.8860 - val_loss: 1.9533 - val_acc: 0.4152\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3282 - acc: 0.9350 - val_loss: 2.2226 - val_acc: 0.3824\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1216 - acc: 0.2290 - val_loss: 1.8406 - val_acc: 0.3444\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.7141 - acc: 0.3630 - val_loss: 1.7927 - val_acc: 0.3686\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5256 - acc: 0.4630 - val_loss: 1.7070 - val_acc: 0.3992\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3413 - acc: 0.5300 - val_loss: 1.7106 - val_acc: 0.3958\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2029 - acc: 0.5840 - val_loss: 1.7923 - val_acc: 0.3894\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0358 - acc: 0.6670 - val_loss: 1.7024 - val_acc: 0.4154\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.8298 - acc: 0.7350 - val_loss: 1.7616 - val_acc: 0.4126\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.6970 - acc: 0.7840 - val_loss: 1.8506 - val_acc: 0.4070\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5977 - acc: 0.8230 - val_loss: 1.8947 - val_acc: 0.4082\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4659 - acc: 0.8740 - val_loss: 1.9994 - val_acc: 0.4114\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3580 - acc: 0.9080 - val_loss: 1.9893 - val_acc: 0.4254\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.2505 - acc: 0.9530 - val_loss: 2.0713 - val_acc: 0.4284\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1673 - acc: 0.2270 - val_loss: 1.9708 - val_acc: 0.2932\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7302 - acc: 0.4100 - val_loss: 1.8838 - val_acc: 0.3152\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5661 - acc: 0.4640 - val_loss: 1.7030 - val_acc: 0.3990\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4182 - acc: 0.5130 - val_loss: 1.6961 - val_acc: 0.4008\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2230 - acc: 0.5880 - val_loss: 1.7060 - val_acc: 0.4108\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0768 - acc: 0.6410 - val_loss: 1.7573 - val_acc: 0.3950\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9539 - acc: 0.6810 - val_loss: 1.7299 - val_acc: 0.4208\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7782 - acc: 0.7540 - val_loss: 1.8079 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6448 - acc: 0.8150 - val_loss: 1.8731 - val_acc: 0.4050\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4922 - acc: 0.8830 - val_loss: 1.8941 - val_acc: 0.4170\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3899 - acc: 0.9130 - val_loss: 1.9983 - val_acc: 0.4066\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.2902 - acc: 0.9440 - val_loss: 2.1477 - val_acc: 0.3990\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1843 - acc: 0.2000 - val_loss: 1.8460 - val_acc: 0.3418\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7032 - acc: 0.4020 - val_loss: 1.7764 - val_acc: 0.3624\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5247 - acc: 0.4560 - val_loss: 1.6656 - val_acc: 0.4194\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3137 - acc: 0.5440 - val_loss: 1.6540 - val_acc: 0.4248\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1637 - acc: 0.5950 - val_loss: 1.7113 - val_acc: 0.4162\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9898 - acc: 0.6770 - val_loss: 1.7052 - val_acc: 0.4208\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8052 - acc: 0.7650 - val_loss: 1.7580 - val_acc: 0.4252\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7001 - acc: 0.7750 - val_loss: 1.8109 - val_acc: 0.4266\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5303 - acc: 0.8550 - val_loss: 2.1200 - val_acc: 0.3770\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4638 - acc: 0.8610 - val_loss: 2.0237 - val_acc: 0.4152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3019 - acc: 0.9360 - val_loss: 2.0376 - val_acc: 0.4356\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2049 - acc: 0.9710 - val_loss: 2.1072 - val_acc: 0.4270\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1833 - acc: 0.2120 - val_loss: 1.8398 - val_acc: 0.3330\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7961 - acc: 0.3650 - val_loss: 1.7796 - val_acc: 0.3564\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5951 - acc: 0.4550 - val_loss: 1.7328 - val_acc: 0.3836\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4091 - acc: 0.5300 - val_loss: 1.6611 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2312 - acc: 0.5690 - val_loss: 1.6687 - val_acc: 0.4140\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1084 - acc: 0.6350 - val_loss: 1.7513 - val_acc: 0.3966\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9635 - acc: 0.6980 - val_loss: 1.8730 - val_acc: 0.3842\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8351 - acc: 0.7250 - val_loss: 1.7396 - val_acc: 0.4218\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6489 - acc: 0.8040 - val_loss: 1.8236 - val_acc: 0.4134\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5207 - acc: 0.8610 - val_loss: 1.8635 - val_acc: 0.4214\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3937 - acc: 0.9020 - val_loss: 1.9437 - val_acc: 0.4190\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2924 - acc: 0.9390 - val_loss: 2.0512 - val_acc: 0.4084\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2154 - acc: 0.2210 - val_loss: 1.8781 - val_acc: 0.3338\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7271 - acc: 0.3790 - val_loss: 1.7542 - val_acc: 0.3642\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5133 - acc: 0.4900 - val_loss: 1.6979 - val_acc: 0.4008\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3777 - acc: 0.5350 - val_loss: 1.6525 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2020 - acc: 0.5940 - val_loss: 1.6840 - val_acc: 0.4078\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0491 - acc: 0.6510 - val_loss: 1.7220 - val_acc: 0.4080\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8811 - acc: 0.7210 - val_loss: 1.7670 - val_acc: 0.4066\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7533 - acc: 0.7690 - val_loss: 1.7549 - val_acc: 0.4316\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5863 - acc: 0.8300 - val_loss: 1.7999 - val_acc: 0.4266\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4708 - acc: 0.8730 - val_loss: 1.8575 - val_acc: 0.4328\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3447 - acc: 0.9260 - val_loss: 2.0079 - val_acc: 0.4236\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2577 - acc: 0.9590 - val_loss: 2.0882 - val_acc: 0.4254\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2528 - acc: 0.2120 - val_loss: 1.9859 - val_acc: 0.2762\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7971 - acc: 0.3790 - val_loss: 1.8326 - val_acc: 0.3454\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6003 - acc: 0.4540 - val_loss: 1.7923 - val_acc: 0.3544\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4322 - acc: 0.5120 - val_loss: 1.8342 - val_acc: 0.3598\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3258 - acc: 0.5510 - val_loss: 1.8195 - val_acc: 0.3658\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1475 - acc: 0.6180 - val_loss: 1.7666 - val_acc: 0.3860\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9750 - acc: 0.6900 - val_loss: 1.7335 - val_acc: 0.4164\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8189 - acc: 0.7460 - val_loss: 1.7597 - val_acc: 0.4226\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6622 - acc: 0.8130 - val_loss: 1.8394 - val_acc: 0.4212\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5298 - acc: 0.8460 - val_loss: 1.8898 - val_acc: 0.4306\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4155 - acc: 0.8960 - val_loss: 1.9948 - val_acc: 0.4172\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3297 - acc: 0.9180 - val_loss: 2.0476 - val_acc: 0.4172\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.3169 - acc: 0.1940 - val_loss: 1.9121 - val_acc: 0.3182\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7722 - acc: 0.3770 - val_loss: 1.8246 - val_acc: 0.3472\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6169 - acc: 0.4280 - val_loss: 1.7267 - val_acc: 0.3822\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4300 - acc: 0.5150 - val_loss: 1.7907 - val_acc: 0.3678\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3245 - acc: 0.5590 - val_loss: 1.7715 - val_acc: 0.3790\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1911 - acc: 0.5910 - val_loss: 1.7130 - val_acc: 0.3974\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0504 - acc: 0.6500 - val_loss: 1.7685 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8932 - acc: 0.7200 - val_loss: 1.7532 - val_acc: 0.4208\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7473 - acc: 0.7800 - val_loss: 1.8472 - val_acc: 0.4020\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6315 - acc: 0.8110 - val_loss: 1.8814 - val_acc: 0.4142\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5268 - acc: 0.8560 - val_loss: 1.9072 - val_acc: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4145 - acc: 0.9020 - val_loss: 1.9429 - val_acc: 0.4086\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2005 - acc: 0.2420 - val_loss: 1.9162 - val_acc: 0.3120\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7812 - acc: 0.3770 - val_loss: 1.7474 - val_acc: 0.3860\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6426 - acc: 0.4410 - val_loss: 1.7490 - val_acc: 0.3772\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4347 - acc: 0.5160 - val_loss: 1.7361 - val_acc: 0.3962\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2730 - acc: 0.5850 - val_loss: 1.6644 - val_acc: 0.4104\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0919 - acc: 0.6420 - val_loss: 1.7139 - val_acc: 0.4158\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9022 - acc: 0.7270 - val_loss: 1.7802 - val_acc: 0.3982\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7752 - acc: 0.7680 - val_loss: 1.7667 - val_acc: 0.4264\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5892 - acc: 0.8260 - val_loss: 1.8790 - val_acc: 0.4088\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4256 - acc: 0.8900 - val_loss: 1.9095 - val_acc: 0.4212\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3454 - acc: 0.9270 - val_loss: 2.0838 - val_acc: 0.4060\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2237 - acc: 0.9700 - val_loss: 2.1110 - val_acc: 0.4256\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Epoch 1/12\n",
      "1930/1930 [==============================] - 2s 973us/sample - loss: 0.5913 - acc: 0.7249\n",
      "Epoch 2/12\n",
      "1930/1930 [==============================] - 0s 153us/sample - loss: 0.4753 - acc: 0.7684\n",
      "Epoch 3/12\n",
      "1930/1930 [==============================] - 0s 83us/sample - loss: 0.4713 - acc: 0.7689\n",
      "Epoch 4/12\n",
      "1930/1930 [==============================] - 0s 108us/sample - loss: 0.4658 - acc: 0.7756\n",
      "Epoch 5/12\n",
      "1930/1930 [==============================] - 0s 120us/sample - loss: 0.4657 - acc: 0.7746\n",
      "Epoch 6/12\n",
      "1930/1930 [==============================] - 0s 88us/sample - loss: 0.4632 - acc: 0.7715\n",
      "Epoch 7/12\n",
      "1930/1930 [==============================] - 0s 78us/sample - loss: 0.4624 - acc: 0.7777\n",
      "Epoch 8/12\n",
      "1930/1930 [==============================] - 0s 84us/sample - loss: 0.4619 - acc: 0.7782\n",
      "Epoch 9/12\n",
      "1930/1930 [==============================] - 0s 197us/sample - loss: 0.4599 - acc: 0.7782\n",
      "Epoch 10/12\n",
      "1930/1930 [==============================] - 0s 100us/sample - loss: 0.4581 - acc: 0.7777\n",
      "Epoch 11/12\n",
      "1930/1930 [==============================] - 0s 76us/sample - loss: 0.4539 - acc: 0.7824\n",
      "Epoch 12/12\n",
      "1930/1930 [==============================] - 0s 105us/sample - loss: 0.4547 - acc: 0.7798\n",
      "Epoch 1/12\n",
      "1872/1872 [==============================] - 1s 589us/sample - loss: 0.5423 - acc: 0.7687\n",
      "Epoch 2/12\n",
      "1872/1872 [==============================] - 0s 76us/sample - loss: 0.4069 - acc: 0.8323\n",
      "Epoch 3/12\n",
      "1872/1872 [==============================] - 0s 80us/sample - loss: 0.3987 - acc: 0.8264\n",
      "Epoch 4/12\n",
      "1872/1872 [==============================] - 0s 82us/sample - loss: 0.3906 - acc: 0.8333\n",
      "Epoch 5/12\n",
      "1872/1872 [==============================] - 0s 75us/sample - loss: 0.3947 - acc: 0.8349\n",
      "Epoch 6/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3909 - acc: 0.8291\n",
      "Epoch 7/12\n",
      "1872/1872 [==============================] - 0s 78us/sample - loss: 0.3970 - acc: 0.8280\n",
      "Epoch 8/12\n",
      "1872/1872 [==============================] - 0s 118us/sample - loss: 0.3933 - acc: 0.8355\n",
      "Epoch 9/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3862 - acc: 0.8328\n",
      "Epoch 10/12\n",
      "1872/1872 [==============================] - 0s 101us/sample - loss: 0.3921 - acc: 0.8312\n",
      "Epoch 11/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3882 - acc: 0.8376\n",
      "Epoch 12/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3886 - acc: 0.8323\n",
      "Epoch 1/12\n",
      "2011/2011 [==============================] - 1s 616us/sample - loss: 0.5403 - acc: 0.7653\n",
      "Epoch 2/12\n",
      "2011/2011 [==============================] - 0s 75us/sample - loss: 0.3943 - acc: 0.8344\n",
      "Epoch 3/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3889 - acc: 0.8349\n",
      "Epoch 4/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3796 - acc: 0.8379\n",
      "Epoch 5/12\n",
      "2011/2011 [==============================] - 0s 73us/sample - loss: 0.3781 - acc: 0.8369\n",
      "Epoch 6/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3783 - acc: 0.8414\n",
      "Epoch 7/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3789 - acc: 0.8329\n",
      "Epoch 8/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3768 - acc: 0.8389\n",
      "Epoch 9/12\n",
      "2011/2011 [==============================] - 0s 85us/sample - loss: 0.3774 - acc: 0.8359\n",
      "Epoch 10/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3731 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "2011/2011 [==============================] - 0s 103us/sample - loss: 0.3727 - acc: 0.8339\n",
      "Epoch 12/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3747 - acc: 0.8364\n",
      "Epoch 1/12\n",
      "2104/2104 [==============================] - 1s 508us/sample - loss: 0.4769 - acc: 0.8427\n",
      "Epoch 2/12\n",
      "2104/2104 [==============================] - 0s 82us/sample - loss: 0.3295 - acc: 0.8736\n",
      "Epoch 3/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3237 - acc: 0.8707\n",
      "Epoch 4/12\n",
      "2104/2104 [==============================] - 0s 68us/sample - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 5/12\n",
      "2104/2104 [==============================] - 0s 71us/sample - loss: 0.3204 - acc: 0.8802\n",
      "Epoch 6/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3153 - acc: 0.8755\n",
      "Epoch 7/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3146 - acc: 0.8760\n",
      "Epoch 8/12\n",
      "2104/2104 [==============================] - 0s 69us/sample - loss: 0.3151 - acc: 0.8769\n",
      "Epoch 9/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3174 - acc: 0.8755\n",
      "Epoch 10/12\n",
      "2104/2104 [==============================] - 0s 113us/sample - loss: 0.3185 - acc: 0.8721\n",
      "Epoch 11/12\n",
      "2104/2104 [==============================] - 0s 110us/sample - loss: 0.3121 - acc: 0.8755\n",
      "Epoch 12/12\n",
      "2104/2104 [==============================] - 0s 101us/sample - loss: 0.3133 - acc: 0.8764\n",
      "Epoch 1/12\n",
      "1961/1961 [==============================] - 1s 624us/sample - loss: 0.5332 - acc: 0.8006\n",
      "Epoch 2/12\n",
      "1961/1961 [==============================] - 0s 71us/sample - loss: 0.3938 - acc: 0.8338\n",
      "Epoch 3/12\n",
      "1961/1961 [==============================] - 0s 64us/sample - loss: 0.3862 - acc: 0.8332\n",
      "Epoch 4/12\n",
      "1961/1961 [==============================] - 0s 76us/sample - loss: 0.3847 - acc: 0.8414\n",
      "Epoch 5/12\n",
      "1961/1961 [==============================] - 0s 73us/sample - loss: 0.3840 - acc: 0.8368\n",
      "Epoch 6/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3818 - acc: 0.8409\n",
      "Epoch 7/12\n",
      "1961/1961 [==============================] - 0s 65us/sample - loss: 0.3765 - acc: 0.8409\n",
      "Epoch 8/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3794 - acc: 0.8399\n",
      "Epoch 9/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3764 - acc: 0.8353\n",
      "Epoch 10/12\n",
      "1961/1961 [==============================] - 0s 66us/sample - loss: 0.3705 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "1961/1961 [==============================] - 0s 62us/sample - loss: 0.3706 - acc: 0.8404\n",
      "Epoch 12/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3750 - acc: 0.8404\n",
      "Epoch 1/12\n",
      "1922/1922 [==============================] - 1s 777us/sample - loss: 0.5168 - acc: 0.8293\n",
      "Epoch 2/12\n",
      "1922/1922 [==============================] - 0s 109us/sample - loss: 0.3586 - acc: 0.8574\n",
      "Epoch 3/12\n",
      "1922/1922 [==============================] - 0s 149us/sample - loss: 0.3523 - acc: 0.8585\n",
      "Epoch 4/12\n",
      "1922/1922 [==============================] - 0s 119us/sample - loss: 0.3535 - acc: 0.8611\n",
      "Epoch 5/12\n",
      "1922/1922 [==============================] - 0s 72us/sample - loss: 0.3469 - acc: 0.8580\n",
      "Epoch 6/12\n",
      "1922/1922 [==============================] - 0s 99us/sample - loss: 0.3490 - acc: 0.8595\n",
      "Epoch 7/12\n",
      "1922/1922 [==============================] - 0s 96us/sample - loss: 0.3515 - acc: 0.8600\n",
      "Epoch 8/12\n",
      "1922/1922 [==============================] - 0s 123us/sample - loss: 0.3522 - acc: 0.8590\n",
      "Epoch 9/12\n",
      "1922/1922 [==============================] - 0s 94us/sample - loss: 0.3459 - acc: 0.8626\n",
      "Epoch 10/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3446 - acc: 0.8585\n",
      "Epoch 11/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3440 - acc: 0.8658\n",
      "Epoch 12/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 1/12\n",
      "2073/2073 [==============================] - 1s 712us/sample - loss: 0.6026 - acc: 0.7159\n",
      "Epoch 2/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.5090 - acc: 0.7501\n",
      "Epoch 3/12\n",
      "2073/2073 [==============================] - 0s 90us/sample - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 4/12\n",
      "2073/2073 [==============================] - 0s 88us/sample - loss: 0.4939 - acc: 0.7574\n",
      "Epoch 5/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.4907 - acc: 0.7569\n",
      "Epoch 6/12\n",
      "2073/2073 [==============================] - 0s 86us/sample - loss: 0.4939 - acc: 0.7511\n",
      "Epoch 7/12\n",
      "2073/2073 [==============================] - 0s 120us/sample - loss: 0.4897 - acc: 0.7545\n",
      "Epoch 8/12\n",
      "2073/2073 [==============================] - 0s 112us/sample - loss: 0.4851 - acc: 0.7525\n",
      "Epoch 9/12\n",
      "2073/2073 [==============================] - 0s 89us/sample - loss: 0.4836 - acc: 0.7574\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 0s 73us/sample - loss: 0.4815 - acc: 0.7593\n",
      "Epoch 11/12\n",
      "2073/2073 [==============================] - 0s 67us/sample - loss: 0.4809 - acc: 0.7583\n",
      "Epoch 12/12\n",
      "2073/2073 [==============================] - 0s 74us/sample - loss: 0.4839 - acc: 0.7612\n",
      "Epoch 1/12\n",
      "2060/2060 [==============================] - 1s 666us/sample - loss: 0.5656 - acc: 0.7524\n",
      "Epoch 2/12\n",
      "2060/2060 [==============================] - 0s 96us/sample - loss: 0.4461 - acc: 0.8034\n",
      "Epoch 3/12\n",
      "2060/2060 [==============================] - 0s 115us/sample - loss: 0.4400 - acc: 0.8049\n",
      "Epoch 4/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4378 - acc: 0.8068\n",
      "Epoch 5/12\n",
      "2060/2060 [==============================] - 0s 82us/sample - loss: 0.4332 - acc: 0.8063\n",
      "Epoch 6/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4285 - acc: 0.8112\n",
      "Epoch 7/12\n",
      "2060/2060 [==============================] - 0s 99us/sample - loss: 0.4340 - acc: 0.8083\n",
      "Epoch 8/12\n",
      "2060/2060 [==============================] - 0s 95us/sample - loss: 0.4242 - acc: 0.8107\n",
      "Epoch 9/12\n",
      "2060/2060 [==============================] - 0s 69us/sample - loss: 0.4254 - acc: 0.8087\n",
      "Epoch 10/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4294 - acc: 0.8053\n",
      "Epoch 11/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4233 - acc: 0.8126\n",
      "Epoch 12/12\n",
      "2060/2060 [==============================] - 0s 73us/sample - loss: 0.4243 - acc: 0.8092\n",
      "Epoch 1/12\n",
      "2068/2068 [==============================] - 1s 625us/sample - loss: 0.6028 - acc: 0.7191\n",
      "Epoch 2/12\n",
      "2068/2068 [==============================] - 0s 111us/sample - loss: 0.5009 - acc: 0.7606\n",
      "Epoch 3/12\n",
      "2068/2068 [==============================] - 0s 100us/sample - loss: 0.4894 - acc: 0.7611\n",
      "Epoch 4/12\n",
      "2068/2068 [==============================] - 0s 105us/sample - loss: 0.4856 - acc: 0.7602\n",
      "Epoch 5/12\n",
      "2068/2068 [==============================] - 0s 99us/sample - loss: 0.4867 - acc: 0.7703\n",
      "Epoch 6/12\n",
      "2068/2068 [==============================] - 0s 113us/sample - loss: 0.4813 - acc: 0.7655\n",
      "Epoch 7/12\n",
      "2068/2068 [==============================] - 0s 85us/sample - loss: 0.4828 - acc: 0.7655\n",
      "Epoch 8/12\n",
      "2068/2068 [==============================] - 0s 63us/sample - loss: 0.4779 - acc: 0.7722\n",
      "Epoch 9/12\n",
      "2068/2068 [==============================] - 0s 65us/sample - loss: 0.4774 - acc: 0.7747\n",
      "Epoch 10/12\n",
      "2068/2068 [==============================] - 0s 64us/sample - loss: 0.4781 - acc: 0.7713\n",
      "Epoch 11/12\n",
      "2068/2068 [==============================] - 0s 77us/sample - loss: 0.4806 - acc: 0.7737\n",
      "Epoch 12/12\n",
      "2068/2068 [==============================] - 0s 71us/sample - loss: 0.4771 - acc: 0.7703\n",
      "Epoch 1/12\n",
      "1999/1999 [==============================] - 1s 609us/sample - loss: 0.5430 - acc: 0.7764\n",
      "Epoch 2/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.3979 - acc: 0.8339\n",
      "Epoch 3/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.4011 - acc: 0.8279\n",
      "Epoch 4/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3924 - acc: 0.8349\n",
      "Epoch 5/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3964 - acc: 0.8309\n",
      "Epoch 6/12\n",
      "1999/1999 [==============================] - 0s 75us/sample - loss: 0.3945 - acc: 0.8304\n",
      "Epoch 7/12\n",
      "1999/1999 [==============================] - 0s 72us/sample - loss: 0.3871 - acc: 0.8404\n",
      "Epoch 8/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3892 - acc: 0.8324\n",
      "Epoch 9/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3878 - acc: 0.8339\n",
      "Epoch 10/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3863 - acc: 0.8329\n",
      "Epoch 11/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3829 - acc: 0.8329\n",
      "Epoch 12/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3859 - acc: 0.8369\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948]\n",
      "[0.742772424017791, 0.7492307692307693, 0.8375510204081633, 0.8439597315436241, 0.8300764655904843, 0.8011996572407883, 0.7648809523809523, 0.7786438035853468, 0.7712121212121212, 0.7638347622759158]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Test the success of the attack.\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "mia_accuracy = []\n",
    "mia_precision = []\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(np.argmax(attacker_y_test, axis=1)) if d == c]\n",
    "    data_in = [X[target_indices], y[target_indices]]\n",
    "    data_out = [[attacker_X_test[test_indices]], attacker_y_test[test_indices]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    mia_accuracy.append(attack_accuracy)\n",
    "    mia_precision.append(precision_score(real_membership_labels, attack_guesses))\n",
    "print(mia_accuracy)\n",
    "print(mia_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/MIA/rE2C2.txt', 'w') as log:\n",
    "        print(\"rE2C2acc = {}\".format(mia_accuracy), file=log)\n",
    "        print(\"rE2C2pre = {}\".format(mia_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167537343955065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883361707485956"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.742772424017791, 0.7492307692307693, 0.8375510204081633, 0.8439597315436241, 0.8300764655904843, 0.8011996572407883, 0.7648809523809523, 0.7786438035853468, 0.7712121212121212, 0.7638347622759158])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
