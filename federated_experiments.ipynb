{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 12\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_realistic(y, u):\n",
    "    # split dataset into arrays of each class label\n",
    "    all_indices = [i for i, d in enumerate(y)]\n",
    "    shares_arr = [5000, 3000, 1000, 750, 250]\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append([all_indices.pop(0) for i in range(shares_arr[u])]) \n",
    "    return user_indices\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = np.array(source[1][indices], dtype=np.int32)\n",
    "    X = np.array(source[0][indices], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'i':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    elif distribution == 'n':\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    elif distribution == 'r':\n",
    "        indices = get_indices_realistic(source[1][:10000], u)[u]\n",
    "    else:\n",
    "        indices = np.array(get_indices_unbalanced_completely(source[1])[u])\n",
    "    \n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': np.array([source[1][b] for b in batch_samples], dtype=np.int32)})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'i') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'i') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=loss_fn,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 1.6453 - categorical_accuracy: 0.4105 - val_loss: 1.4564 - val_categorical_accuracy: 0.4887\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.2948 - categorical_accuracy: 0.5413 - val_loss: 1.3070 - val_categorical_accuracy: 0.5377\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.0989 - categorical_accuracy: 0.6127 - val_loss: 1.2251 - val_categorical_accuracy: 0.5706\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.9609 - categorical_accuracy: 0.6592 - val_loss: 1.2384 - val_categorical_accuracy: 0.5791\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.8286 - categorical_accuracy: 0.7077 - val_loss: 1.2404 - val_categorical_accuracy: 0.5842\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.6916 - categorical_accuracy: 0.7620 - val_loss: 1.2497 - val_categorical_accuracy: 0.5907\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.5374 - categorical_accuracy: 0.8224 - val_loss: 1.3085 - val_categorical_accuracy: 0.5914\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4065 - categorical_accuracy: 0.8718 - val_loss: 1.3898 - val_categorical_accuracy: 0.5857\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.2734 - categorical_accuracy: 0.9245 - val_loss: 1.4569 - val_categorical_accuracy: 0.5969\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1703 - categorical_accuracy: 0.9636 - val_loss: 1.5415 - val_categorical_accuracy: 0.5924\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.1021 - categorical_accuracy: 0.9831 - val_loss: 1.6110 - val_categorical_accuracy: 0.5882\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.0585 - categorical_accuracy: 0.9945 - val_loss: 1.6453 - val_categorical_accuracy: 0.5936\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0314 - categorical_accuracy: 0.9982 - val_loss: 1.7216 - val_categorical_accuracy: 0.5952\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0179 - categorical_accuracy: 0.9996 - val_loss: 1.7587 - val_categorical_accuracy: 0.6038\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0107 - categorical_accuracy: 0.9999 - val_loss: 1.8129 - val_categorical_accuracy: 0.6051\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8519 - val_categorical_accuracy: 0.6043\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0058 - categorical_accuracy: 1.0000 - val_loss: 1.9056 - val_categorical_accuracy: 0.6017\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0044 - categorical_accuracy: 1.0000 - val_loss: 1.9266 - val_categorical_accuracy: 0.6014\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0046 - categorical_accuracy: 0.9999 - val_loss: 1.9657 - val_categorical_accuracy: 0.6035\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 1.9959 - val_categorical_accuracy: 0.6053\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 2.0312 - val_categorical_accuracy: 0.6022\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 2.0627 - val_categorical_accuracy: 0.6027\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 2.0921 - val_categorical_accuracy: 0.6026\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 2.1258 - val_categorical_accuracy: 0.6026\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 9.9374e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1454 - val_categorical_accuracy: 0.6023\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 8.2107e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1764 - val_categorical_accuracy: 0.6021\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 6.8230e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2064 - val_categorical_accuracy: 0.6013\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 5.7202e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2441 - val_categorical_accuracy: 0.6027\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7293e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2656 - val_categorical_accuracy: 0.6025\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 4.0281e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2940 - val_categorical_accuracy: 0.6022\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.2552e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3168 - val_categorical_accuracy: 0.6021\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.7323e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3425 - val_categorical_accuracy: 0.6016\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 2.2841e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3709 - val_categorical_accuracy: 0.6003\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.9136e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3944 - val_categorical_accuracy: 0.6014\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.6010e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4202 - val_categorical_accuracy: 0.6012\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3392e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4532 - val_categorical_accuracy: 0.6016\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1291e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4735 - val_categorical_accuracy: 0.6004\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 9.5869e-05 - categorical_accuracy: 1.0000 - val_loss: 2.4977 - val_categorical_accuracy: 0.6009\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 7.9403e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5172 - val_categorical_accuracy: 0.6002\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 6.7607e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5467 - val_categorical_accuracy: 0.6008\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 5.8769e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5714 - val_categorical_accuracy: 0.6007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7460e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5931 - val_categorical_accuracy: 0.6008\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 3.9733e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6141 - val_categorical_accuracy: 0.5998\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.3674e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6374 - val_categorical_accuracy: 0.6006\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 2.8456e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6608 - val_categorical_accuracy: 0.6018\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 2.3901e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6810 - val_categorical_accuracy: 0.6009\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 2.0279e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7009 - val_categorical_accuracy: 0.6002\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.7068e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7247 - val_categorical_accuracy: 0.5999\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 1.4409e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7476 - val_categorical_accuracy: 0.5995\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.2274e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7663 - val_categorical_accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 442us/sample - loss: 1.6801 - sparse_categorical_accuracy: 0.4095\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.3275,loss=1.8733339>\n",
      "test accuracy: 0.40950000286102295\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "# test_metrics = evaluation(state.model, federated_test_data)\n",
    "non_federated_model.set_weights(state.model.trainable)\n",
    "(loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 6s 585us/sample - loss: 1.5458 - sparse_categorical_accuracy: 0.5505\n",
      "round  0, metrics=<sparse_categorical_accuracy=0.73686665,loss=0.8089468>\n",
      "10000/10000 [==============================] - 5s 541us/sample - loss: 1.7853 - sparse_categorical_accuracy: 0.5680\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.93691665,loss=0.24001373>\n",
      "10000/10000 [==============================] - 6s 619us/sample - loss: 1.8620 - sparse_categorical_accuracy: 0.5730\n",
      "round  2, metrics=<sparse_categorical_accuracy=0.96595,loss=0.13019271>\n",
      "10000/10000 [==============================] - 6s 624us/sample - loss: 1.9153 - sparse_categorical_accuracy: 0.5736\n",
      "round  3, metrics=<sparse_categorical_accuracy=0.9761583,loss=0.08687823>\n",
      "10000/10000 [==============================] - 6s 568us/sample - loss: 1.9688 - sparse_categorical_accuracy: 0.5744\n",
      "round  4, metrics=<sparse_categorical_accuracy=0.98205835,loss=0.06275885>\n",
      "10000/10000 [==============================] - 5s 541us/sample - loss: 2.0221 - sparse_categorical_accuracy: 0.5747\n",
      "round  5, metrics=<sparse_categorical_accuracy=0.98674166,loss=0.046714675>\n",
      "10000/10000 [==============================] - 7s 745us/sample - loss: 2.0730 - sparse_categorical_accuracy: 0.5748s - loss: 2.0589 - sparse_categorical_accura\n",
      "round  6, metrics=<sparse_categorical_accuracy=0.9898833,loss=0.03611477>\n",
      "10000/10000 [==============================] - 7s 709us/sample - loss: 2.1330 - sparse_categorical_accuracy: 0.5738\n",
      "round  7, metrics=<sparse_categorical_accuracy=0.9923583,loss=0.027575692>\n",
      "10000/10000 [==============================] - 7s 686us/sample - loss: 2.1894 - sparse_categorical_accuracy: 0.5712\n",
      "round  8, metrics=<sparse_categorical_accuracy=0.99354166,loss=0.023842132>\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 2.2286 - sparse_categorical_accuracy: 0.5695\n",
      "round  9, metrics=<sparse_categorical_accuracy=0.9943333,loss=0.020347139>\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "epochs = 12\n",
    "state = iterative_process.initialize()\n",
    "fd_test_loss = []\n",
    "fd_train_loss = []\n",
    "fd_train_accuracy = []\n",
    "for round_num in range(12):\n",
    "    selected = np.random.choice(5, 5, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "    non_federated_model.set_weights(state.model.trainable)\n",
    "    (loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "    fd_train_loss.append(metrics[1])\n",
    "    fd_test_accuracy.append(accuracy)\n",
    "    fd_test_loss.append(loss)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4527,\n",
       " 0.4398,\n",
       " 0.4791,\n",
       " 0.5053,\n",
       " 0.5295,\n",
       " 0.5456,\n",
       " 0.5595,\n",
       " 0.5747,\n",
       " 0.5807,\n",
       " 0.5853,\n",
       " 0.5906,\n",
       " 0.5918,\n",
       " 0.5935,\n",
       " 0.4413,\n",
       " 0.4805,\n",
       " 0.5017,\n",
       " 0.5206,\n",
       " 0.5365,\n",
       " 0.5511,\n",
       " 0.5626,\n",
       " 0.5729,\n",
       " 0.5803,\n",
       " 0.5867,\n",
       " 0.5858,\n",
       " 0.5863]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.3458 - acc: 0.1940 - val_loss: 2.0108 - val_acc: 0.2608\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.8298 - acc: 0.3430 - val_loss: 1.8013 - val_acc: 0.3462\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6427 - acc: 0.4390 - val_loss: 1.8508 - val_acc: 0.3174\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4939 - acc: 0.4780 - val_loss: 1.6926 - val_acc: 0.3868\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3206 - acc: 0.5480 - val_loss: 1.6522 - val_acc: 0.4196\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2116 - acc: 0.5850 - val_loss: 1.6790 - val_acc: 0.4022\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0443 - acc: 0.6600 - val_loss: 1.7660 - val_acc: 0.3968\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9299 - acc: 0.7090 - val_loss: 1.7165 - val_acc: 0.4212\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7792 - acc: 0.7600 - val_loss: 1.7765 - val_acc: 0.4100\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6403 - acc: 0.8190 - val_loss: 1.7967 - val_acc: 0.4192\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4914 - acc: 0.8730 - val_loss: 1.8812 - val_acc: 0.4190\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3686 - acc: 0.9140 - val_loss: 2.0142 - val_acc: 0.4110\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1227 - acc: 0.2680 - val_loss: 1.8438 - val_acc: 0.3422\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6681 - acc: 0.4270 - val_loss: 1.7609 - val_acc: 0.3712\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5051 - acc: 0.5000 - val_loss: 1.7513 - val_acc: 0.3832\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2948 - acc: 0.5780 - val_loss: 1.6517 - val_acc: 0.4062\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1089 - acc: 0.6390 - val_loss: 1.7109 - val_acc: 0.4072\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9389 - acc: 0.7100 - val_loss: 1.7643 - val_acc: 0.4226\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7953 - acc: 0.7720 - val_loss: 1.7159 - val_acc: 0.4224\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6607 - acc: 0.8100 - val_loss: 1.8440 - val_acc: 0.4162\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5449 - acc: 0.8400 - val_loss: 1.8941 - val_acc: 0.4228\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4078 - acc: 0.8930 - val_loss: 1.8962 - val_acc: 0.4284\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3148 - acc: 0.9270 - val_loss: 1.9980 - val_acc: 0.4342\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2112 - acc: 0.9660 - val_loss: 2.0777 - val_acc: 0.4342\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.3955 - acc: 0.1920 - val_loss: 1.9144 - val_acc: 0.3014\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.8246 - acc: 0.3700 - val_loss: 1.7720 - val_acc: 0.3722\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6460 - acc: 0.4330 - val_loss: 1.7545 - val_acc: 0.3844\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4661 - acc: 0.5040 - val_loss: 1.7113 - val_acc: 0.3872\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3022 - acc: 0.5790 - val_loss: 1.7423 - val_acc: 0.3998\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1351 - acc: 0.6310 - val_loss: 1.7202 - val_acc: 0.4072\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0129 - acc: 0.6920 - val_loss: 1.7165 - val_acc: 0.4266\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8126 - acc: 0.7370 - val_loss: 1.8531 - val_acc: 0.4076\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7090 - acc: 0.7930 - val_loss: 1.8632 - val_acc: 0.4218\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5492 - acc: 0.8510 - val_loss: 1.9192 - val_acc: 0.4138\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4779 - acc: 0.8700 - val_loss: 1.9568 - val_acc: 0.4264\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3319 - acc: 0.9190 - val_loss: 2.0510 - val_acc: 0.4238\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1660 - acc: 0.2490 - val_loss: 1.9060 - val_acc: 0.3090\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7092 - acc: 0.4160 - val_loss: 1.7264 - val_acc: 0.3900\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4981 - acc: 0.5020 - val_loss: 1.6753 - val_acc: 0.3978\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3316 - acc: 0.5590 - val_loss: 1.7546 - val_acc: 0.3758\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2193 - acc: 0.6040 - val_loss: 1.7626 - val_acc: 0.3898\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0473 - acc: 0.6630 - val_loss: 1.7415 - val_acc: 0.3992\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9016 - acc: 0.7110 - val_loss: 1.7393 - val_acc: 0.4046\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7301 - acc: 0.7760 - val_loss: 1.8737 - val_acc: 0.4114\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6037 - acc: 0.8250 - val_loss: 1.8685 - val_acc: 0.4168\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4735 - acc: 0.8800 - val_loss: 1.9953 - val_acc: 0.4130\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3843 - acc: 0.9080 - val_loss: 2.0752 - val_acc: 0.4046\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2714 - acc: 0.9520 - val_loss: 2.1590 - val_acc: 0.4042\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1347 - acc: 0.2390 - val_loss: 1.9257 - val_acc: 0.2882\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6986 - acc: 0.4210 - val_loss: 1.7468 - val_acc: 0.3756\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5176 - acc: 0.4600 - val_loss: 1.7290 - val_acc: 0.3852\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4338 - acc: 0.4970 - val_loss: 1.7671 - val_acc: 0.3710\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2612 - acc: 0.5780 - val_loss: 1.7453 - val_acc: 0.3932\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1089 - acc: 0.6360 - val_loss: 1.7423 - val_acc: 0.4050\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9750 - acc: 0.6780 - val_loss: 1.8299 - val_acc: 0.4154\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.8248 - acc: 0.7420 - val_loss: 1.9036 - val_acc: 0.3944\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.6699 - acc: 0.7790 - val_loss: 1.8769 - val_acc: 0.4164\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5454 - acc: 0.8420 - val_loss: 1.9388 - val_acc: 0.4158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.4043 - acc: 0.9030 - val_loss: 2.0470 - val_acc: 0.3960\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.2988 - acc: 0.9490 - val_loss: 2.0433 - val_acc: 0.4250\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.2199 - acc: 0.2030 - val_loss: 1.8998 - val_acc: 0.3294\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7552 - acc: 0.3920 - val_loss: 1.8590 - val_acc: 0.3430\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.5705 - acc: 0.4520 - val_loss: 1.7396 - val_acc: 0.3942\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.4145 - acc: 0.5200 - val_loss: 1.7354 - val_acc: 0.3920\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2600 - acc: 0.5510 - val_loss: 1.7529 - val_acc: 0.3830\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1134 - acc: 0.6230 - val_loss: 1.6932 - val_acc: 0.4124\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9970 - acc: 0.6650 - val_loss: 1.7488 - val_acc: 0.4116\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8380 - acc: 0.7360 - val_loss: 1.8201 - val_acc: 0.4018\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.6901 - acc: 0.7870 - val_loss: 1.8490 - val_acc: 0.4064\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5543 - acc: 0.8350 - val_loss: 1.8797 - val_acc: 0.4124\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.4115 - acc: 0.8950 - val_loss: 1.9496 - val_acc: 0.4246\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3237 - acc: 0.9230 - val_loss: 2.0502 - val_acc: 0.4118\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1586 - acc: 0.2380 - val_loss: 1.8485 - val_acc: 0.3426\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6981 - acc: 0.4020 - val_loss: 1.7523 - val_acc: 0.3802\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4971 - acc: 0.4800 - val_loss: 1.7578 - val_acc: 0.3744\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3732 - acc: 0.5100 - val_loss: 1.8170 - val_acc: 0.3546\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2054 - acc: 0.5870 - val_loss: 1.7747 - val_acc: 0.3886\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0717 - acc: 0.6430 - val_loss: 1.7369 - val_acc: 0.4008\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8966 - acc: 0.7280 - val_loss: 1.8095 - val_acc: 0.4012\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7590 - acc: 0.7580 - val_loss: 1.8615 - val_acc: 0.4082\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6309 - acc: 0.8210 - val_loss: 1.8960 - val_acc: 0.4004\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.4739 - acc: 0.8820 - val_loss: 1.9970 - val_acc: 0.3988\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3701 - acc: 0.9170 - val_loss: 2.0572 - val_acc: 0.4094\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2940 - acc: 0.9270 - val_loss: 2.1259 - val_acc: 0.4114\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1165 - acc: 0.2550 - val_loss: 1.8748 - val_acc: 0.3172\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6817 - acc: 0.4060 - val_loss: 1.7238 - val_acc: 0.4050\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4790 - acc: 0.4800 - val_loss: 1.7282 - val_acc: 0.3834\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2975 - acc: 0.5450 - val_loss: 1.7103 - val_acc: 0.4050\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1219 - acc: 0.6120 - val_loss: 1.7254 - val_acc: 0.4084\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9565 - acc: 0.6830 - val_loss: 1.7319 - val_acc: 0.4148\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.8216 - acc: 0.7330 - val_loss: 1.7896 - val_acc: 0.4226\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6638 - acc: 0.8050 - val_loss: 1.8903 - val_acc: 0.4152\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5382 - acc: 0.8500 - val_loss: 1.8994 - val_acc: 0.4314\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4132 - acc: 0.8970 - val_loss: 2.0090 - val_acc: 0.4278\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3342 - acc: 0.9200 - val_loss: 2.1575 - val_acc: 0.4216\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2440 - acc: 0.9540 - val_loss: 2.1286 - val_acc: 0.4316\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1351 - acc: 0.2620 - val_loss: 1.9012 - val_acc: 0.3520\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6980 - acc: 0.4200 - val_loss: 1.7482 - val_acc: 0.3740\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5123 - acc: 0.4810 - val_loss: 1.7079 - val_acc: 0.3930\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3568 - acc: 0.5130 - val_loss: 1.6858 - val_acc: 0.4050\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2076 - acc: 0.5920 - val_loss: 1.7485 - val_acc: 0.3868\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0239 - acc: 0.6730 - val_loss: 1.7628 - val_acc: 0.3948\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8496 - acc: 0.7380 - val_loss: 1.7724 - val_acc: 0.4112\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7079 - acc: 0.7850 - val_loss: 1.8163 - val_acc: 0.4082\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5985 - acc: 0.8250 - val_loss: 1.9375 - val_acc: 0.4040\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4776 - acc: 0.8610 - val_loss: 1.9458 - val_acc: 0.4206\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3389 - acc: 0.9270 - val_loss: 2.0640 - val_acc: 0.4036\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2633 - acc: 0.9490 - val_loss: 2.1797 - val_acc: 0.3978\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1596 - acc: 0.2250 - val_loss: 1.9403 - val_acc: 0.2812\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6736 - acc: 0.4040 - val_loss: 1.7654 - val_acc: 0.3596\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5113 - acc: 0.4690 - val_loss: 1.7491 - val_acc: 0.3778\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3539 - acc: 0.5490 - val_loss: 1.7369 - val_acc: 0.3710\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1913 - acc: 0.6110 - val_loss: 1.7194 - val_acc: 0.3912\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0209 - acc: 0.6880 - val_loss: 1.7859 - val_acc: 0.3918\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8759 - acc: 0.7120 - val_loss: 1.7949 - val_acc: 0.4022\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7238 - acc: 0.7780 - val_loss: 1.8521 - val_acc: 0.4056\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5937 - acc: 0.8360 - val_loss: 1.8598 - val_acc: 0.4122\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4732 - acc: 0.8830 - val_loss: 1.9552 - val_acc: 0.4158\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3614 - acc: 0.9190 - val_loss: 2.0105 - val_acc: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2838 - acc: 0.9400 - val_loss: 2.1034 - val_acc: 0.4052\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 19:07:52.771605 4439864768 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "2054/2054 [==============================] - 1s 355us/sample - loss: 0.5845 - acc: 0.7507\n",
      "Epoch 2/12\n",
      "2054/2054 [==============================] - 0s 65us/sample - loss: 0.4920 - acc: 0.7712\n",
      "Epoch 3/12\n",
      "2054/2054 [==============================] - 0s 69us/sample - loss: 0.4910 - acc: 0.7722\n",
      "Epoch 4/12\n",
      "2054/2054 [==============================] - 0s 71us/sample - loss: 0.4853 - acc: 0.7741\n",
      "Epoch 5/12\n",
      "2054/2054 [==============================] - 0s 72us/sample - loss: 0.4809 - acc: 0.7697\n",
      "Epoch 6/12\n",
      "2054/2054 [==============================] - 0s 63us/sample - loss: 0.4791 - acc: 0.7756\n",
      "Epoch 7/12\n",
      "2054/2054 [==============================] - 0s 148us/sample - loss: 0.4753 - acc: 0.7770\n",
      "Epoch 8/12\n",
      "2054/2054 [==============================] - 0s 116us/sample - loss: 0.4817 - acc: 0.7726\n",
      "Epoch 9/12\n",
      "2054/2054 [==============================] - 0s 116us/sample - loss: 0.4784 - acc: 0.7692\n",
      "Epoch 10/12\n",
      "2054/2054 [==============================] - 0s 98us/sample - loss: 0.4741 - acc: 0.7746\n",
      "Epoch 11/12\n",
      "2054/2054 [==============================] - 0s 109us/sample - loss: 0.4775 - acc: 0.7756\n",
      "Epoch 12/12\n",
      "2054/2054 [==============================] - 0s 85us/sample - loss: 0.4765 - acc: 0.7765\n",
      "Epoch 1/12\n",
      "1949/1949 [==============================] - 1s 537us/sample - loss: 0.5710 - acc: 0.7506\n",
      "Epoch 2/12\n",
      "1949/1949 [==============================] - 0s 68us/sample - loss: 0.4129 - acc: 0.8250\n",
      "Epoch 3/12\n",
      "1949/1949 [==============================] - 0s 87us/sample - loss: 0.4104 - acc: 0.8179\n",
      "Epoch 4/12\n",
      "1949/1949 [==============================] - 0s 74us/sample - loss: 0.4107 - acc: 0.8225\n",
      "Epoch 5/12\n",
      "1949/1949 [==============================] - 0s 92us/sample - loss: 0.4070 - acc: 0.8163\n",
      "Epoch 6/12\n",
      "1949/1949 [==============================] - 0s 66us/sample - loss: 0.4026 - acc: 0.8199\n",
      "Epoch 7/12\n",
      "1949/1949 [==============================] - 0s 79us/sample - loss: 0.4030 - acc: 0.8204\n",
      "Epoch 8/12\n",
      "1949/1949 [==============================] - 0s 62us/sample - loss: 0.3998 - acc: 0.8214\n",
      "Epoch 9/12\n",
      "1949/1949 [==============================] - 0s 64us/sample - loss: 0.4049 - acc: 0.8189\n",
      "Epoch 10/12\n",
      "1949/1949 [==============================] - 0s 59us/sample - loss: 0.4012 - acc: 0.8194\n",
      "Epoch 11/12\n",
      "1949/1949 [==============================] - 0s 59us/sample - loss: 0.3996 - acc: 0.8179\n",
      "Epoch 12/12\n",
      "1949/1949 [==============================] - 0s 60us/sample - loss: 0.4019 - acc: 0.8184\n",
      "Epoch 1/12\n",
      "1896/1896 [==============================] - 0s 259us/sample - loss: 0.5324 - acc: 0.8149\n",
      "Epoch 2/12\n",
      "1896/1896 [==============================] - 0s 73us/sample - loss: 0.3923 - acc: 0.8318\n",
      "Epoch 3/12\n",
      "1896/1896 [==============================] - 0s 85us/sample - loss: 0.3899 - acc: 0.8344\n",
      "Epoch 4/12\n",
      "1896/1896 [==============================] - 0s 112us/sample - loss: 0.3839 - acc: 0.8370\n",
      "Epoch 5/12\n",
      "1896/1896 [==============================] - 0s 71us/sample - loss: 0.3834 - acc: 0.8412\n",
      "Epoch 6/12\n",
      "1896/1896 [==============================] - 0s 58us/sample - loss: 0.3801 - acc: 0.8354\n",
      "Epoch 7/12\n",
      "1896/1896 [==============================] - 0s 61us/sample - loss: 0.3713 - acc: 0.8423\n",
      "Epoch 8/12\n",
      "1896/1896 [==============================] - 0s 56us/sample - loss: 0.3718 - acc: 0.8391\n",
      "Epoch 9/12\n",
      "1896/1896 [==============================] - 0s 55us/sample - loss: 0.3660 - acc: 0.8423\n",
      "Epoch 10/12\n",
      "1896/1896 [==============================] - 0s 61us/sample - loss: 0.3682 - acc: 0.8449\n",
      "Epoch 11/12\n",
      "1896/1896 [==============================] - 0s 54us/sample - loss: 0.3670 - acc: 0.8381\n",
      "Epoch 12/12\n",
      "1896/1896 [==============================] - 0s 57us/sample - loss: 0.3639 - acc: 0.8434\n",
      "Epoch 1/12\n",
      "1916/1916 [==============================] - 1s 273us/sample - loss: 0.4844 - acc: 0.8158\n",
      "Epoch 2/12\n",
      "1916/1916 [==============================] - 0s 56us/sample - loss: 0.3195 - acc: 0.8716\n",
      "Epoch 3/12\n",
      "1916/1916 [==============================] - 0s 54us/sample - loss: 0.3136 - acc: 0.8747\n",
      "Epoch 4/12\n",
      "1916/1916 [==============================] - 0s 54us/sample - loss: 0.3112 - acc: 0.8721\n",
      "Epoch 5/12\n",
      "1916/1916 [==============================] - 0s 66us/sample - loss: 0.3106 - acc: 0.8758\n",
      "Epoch 6/12\n",
      "1916/1916 [==============================] - 0s 70us/sample - loss: 0.3069 - acc: 0.8758\n",
      "Epoch 7/12\n",
      "1916/1916 [==============================] - 0s 53us/sample - loss: 0.3081 - acc: 0.8758\n",
      "Epoch 8/12\n",
      "1916/1916 [==============================] - 0s 54us/sample - loss: 0.3117 - acc: 0.8711\n",
      "Epoch 9/12\n",
      "1916/1916 [==============================] - 0s 54us/sample - loss: 0.3052 - acc: 0.8773\n",
      "Epoch 10/12\n",
      "1916/1916 [==============================] - 0s 53us/sample - loss: 0.3034 - acc: 0.8773\n",
      "Epoch 11/12\n",
      "1916/1916 [==============================] - 0s 64us/sample - loss: 0.2975 - acc: 0.8721\n",
      "Epoch 12/12\n",
      "1916/1916 [==============================] - 0s 56us/sample - loss: 0.2969 - acc: 0.8768\n",
      "Epoch 1/12\n",
      "2051/2051 [==============================] - 1s 249us/sample - loss: 0.5279 - acc: 0.7889\n",
      "Epoch 2/12\n",
      "2051/2051 [==============================] - 0s 65us/sample - loss: 0.4068 - acc: 0.8235\n",
      "Epoch 3/12\n",
      "2051/2051 [==============================] - 0s 64us/sample - loss: 0.4019 - acc: 0.8264\n",
      "Epoch 4/12\n",
      "2051/2051 [==============================] - 0s 60us/sample - loss: 0.3915 - acc: 0.8347\n",
      "Epoch 5/12\n",
      "2051/2051 [==============================] - 0s 59us/sample - loss: 0.3939 - acc: 0.8328\n",
      "Epoch 6/12\n",
      "2051/2051 [==============================] - 0s 54us/sample - loss: 0.3874 - acc: 0.8367\n",
      "Epoch 7/12\n",
      "2051/2051 [==============================] - 0s 55us/sample - loss: 0.3903 - acc: 0.8347\n",
      "Epoch 8/12\n",
      "2051/2051 [==============================] - 0s 67us/sample - loss: 0.3893 - acc: 0.8386\n",
      "Epoch 9/12\n",
      "2051/2051 [==============================] - 0s 57us/sample - loss: 0.3875 - acc: 0.8372\n",
      "Epoch 10/12\n",
      "2051/2051 [==============================] - 0s 55us/sample - loss: 0.3785 - acc: 0.8430\n",
      "Epoch 11/12\n",
      "2051/2051 [==============================] - 0s 59us/sample - loss: 0.3833 - acc: 0.8357\n",
      "Epoch 12/12\n",
      "2051/2051 [==============================] - 0s 62us/sample - loss: 0.3811 - acc: 0.8367\n",
      "Epoch 1/12\n",
      "1972/1972 [==============================] - 1s 344us/sample - loss: 0.5412 - acc: 0.7804\n",
      "Epoch 2/12\n",
      "1972/1972 [==============================] - 0s 72us/sample - loss: 0.3978 - acc: 0.8362\n",
      "Epoch 3/12\n",
      "1972/1972 [==============================] - 0s 67us/sample - loss: 0.3889 - acc: 0.8387\n",
      "Epoch 4/12\n",
      "1972/1972 [==============================] - 0s 99us/sample - loss: 0.3905 - acc: 0.8413\n",
      "Epoch 5/12\n",
      "1972/1972 [==============================] - 0s 89us/sample - loss: 0.3792 - acc: 0.8377\n",
      "Epoch 6/12\n",
      "1972/1972 [==============================] - 0s 77us/sample - loss: 0.3831 - acc: 0.8403\n",
      "Epoch 7/12\n",
      "1972/1972 [==============================] - 0s 95us/sample - loss: 0.3842 - acc: 0.8372\n",
      "Epoch 8/12\n",
      "1972/1972 [==============================] - 0s 99us/sample - loss: 0.3839 - acc: 0.8372\n",
      "Epoch 9/12\n",
      "1972/1972 [==============================] - 0s 72us/sample - loss: 0.3779 - acc: 0.8403\n",
      "Epoch 10/12\n",
      "1972/1972 [==============================] - 0s 106us/sample - loss: 0.3785 - acc: 0.8398\n",
      "Epoch 11/12\n",
      "1972/1972 [==============================] - 0s 89us/sample - loss: 0.3753 - acc: 0.8408\n",
      "Epoch 12/12\n",
      "1972/1972 [==============================] - 0s 71us/sample - loss: 0.3742 - acc: 0.8453\n",
      "Epoch 1/12\n",
      "2049/2049 [==============================] - 1s 363us/sample - loss: 0.5872 - acc: 0.7277\n",
      "Epoch 2/12\n",
      "2049/2049 [==============================] - 0s 81us/sample - loss: 0.5148 - acc: 0.7482\n",
      "Epoch 3/12\n",
      "2049/2049 [==============================] - 0s 74us/sample - loss: 0.5112 - acc: 0.7574\n",
      "Epoch 4/12\n",
      "2049/2049 [==============================] - 0s 62us/sample - loss: 0.5051 - acc: 0.7609\n",
      "Epoch 5/12\n",
      "2049/2049 [==============================] - 0s 61us/sample - loss: 0.5023 - acc: 0.7609\n",
      "Epoch 6/12\n",
      "2049/2049 [==============================] - 0s 62us/sample - loss: 0.4975 - acc: 0.7599\n",
      "Epoch 7/12\n",
      "2049/2049 [==============================] - 0s 67us/sample - loss: 0.4987 - acc: 0.7550\n",
      "Epoch 8/12\n",
      "2049/2049 [==============================] - 0s 71us/sample - loss: 0.4980 - acc: 0.7589\n",
      "Epoch 9/12\n",
      "2049/2049 [==============================] - 0s 67us/sample - loss: 0.4951 - acc: 0.7657\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049/2049 [==============================] - 0s 63us/sample - loss: 0.4957 - acc: 0.7618\n",
      "Epoch 11/12\n",
      "2049/2049 [==============================] - 0s 71us/sample - loss: 0.4954 - acc: 0.7609\n",
      "Epoch 12/12\n",
      "2049/2049 [==============================] - 0s 66us/sample - loss: 0.4938 - acc: 0.7609\n",
      "Epoch 1/12\n",
      "2059/2059 [==============================] - 1s 288us/sample - loss: 0.5479 - acc: 0.7601\n",
      "Epoch 2/12\n",
      "2059/2059 [==============================] - 0s 60us/sample - loss: 0.4512 - acc: 0.7902\n",
      "Epoch 3/12\n",
      "2059/2059 [==============================] - 0s 77us/sample - loss: 0.4402 - acc: 0.7950\n",
      "Epoch 4/12\n",
      "2059/2059 [==============================] - 0s 64us/sample - loss: 0.4420 - acc: 0.7941\n",
      "Epoch 5/12\n",
      "2059/2059 [==============================] - 0s 68us/sample - loss: 0.4423 - acc: 0.7931\n",
      "Epoch 6/12\n",
      "2059/2059 [==============================] - 0s 72us/sample - loss: 0.4346 - acc: 0.7941\n",
      "Epoch 7/12\n",
      "2059/2059 [==============================] - 0s 66us/sample - loss: 0.4350 - acc: 0.7950\n",
      "Epoch 8/12\n",
      "2059/2059 [==============================] - 0s 86us/sample - loss: 0.4394 - acc: 0.7936\n",
      "Epoch 9/12\n",
      "2059/2059 [==============================] - 0s 65us/sample - loss: 0.4393 - acc: 0.7955\n",
      "Epoch 10/12\n",
      "2059/2059 [==============================] - 0s 60us/sample - loss: 0.4354 - acc: 0.7989\n",
      "Epoch 11/12\n",
      "2059/2059 [==============================] - 0s 60us/sample - loss: 0.4324 - acc: 0.7941\n",
      "Epoch 12/12\n",
      "2059/2059 [==============================] - 0s 64us/sample - loss: 0.4345 - acc: 0.7950\n",
      "Epoch 1/12\n",
      "2060/2060 [==============================] - 1s 287us/sample - loss: 0.6089 - acc: 0.7044\n",
      "Epoch 2/12\n",
      "2060/2060 [==============================] - 0s 63us/sample - loss: 0.5131 - acc: 0.7471\n",
      "Epoch 3/12\n",
      "2060/2060 [==============================] - 0s 59us/sample - loss: 0.5026 - acc: 0.7539\n",
      "Epoch 4/12\n",
      "2060/2060 [==============================] - 0s 59us/sample - loss: 0.4994 - acc: 0.7476\n",
      "Epoch 5/12\n",
      "2060/2060 [==============================] - 0s 58us/sample - loss: 0.4944 - acc: 0.7539\n",
      "Epoch 6/12\n",
      "2060/2060 [==============================] - 0s 57us/sample - loss: 0.4973 - acc: 0.7505\n",
      "Epoch 7/12\n",
      "2060/2060 [==============================] - 0s 60us/sample - loss: 0.4935 - acc: 0.7510\n",
      "Epoch 8/12\n",
      "2060/2060 [==============================] - 0s 53us/sample - loss: 0.4916 - acc: 0.7583\n",
      "Epoch 9/12\n",
      "2060/2060 [==============================] - 0s 67us/sample - loss: 0.4912 - acc: 0.7529\n",
      "Epoch 10/12\n",
      "2060/2060 [==============================] - 0s 69us/sample - loss: 0.4913 - acc: 0.7553\n",
      "Epoch 11/12\n",
      "2060/2060 [==============================] - 0s 72us/sample - loss: 0.4891 - acc: 0.7568\n",
      "Epoch 12/12\n",
      "2060/2060 [==============================] - 0s 72us/sample - loss: 0.4887 - acc: 0.7641\n",
      "Epoch 1/12\n",
      "1994/1994 [==============================] - 1s 381us/sample - loss: 0.5481 - acc: 0.7758\n",
      "Epoch 2/12\n",
      "1994/1994 [==============================] - 0s 69us/sample - loss: 0.4056 - acc: 0.8235\n",
      "Epoch 3/12\n",
      "1994/1994 [==============================] - 0s 68us/sample - loss: 0.3970 - acc: 0.8335\n",
      "Epoch 4/12\n",
      "1994/1994 [==============================] - 0s 68us/sample - loss: 0.3937 - acc: 0.8275\n",
      "Epoch 5/12\n",
      "1994/1994 [==============================] - 0s 71us/sample - loss: 0.3930 - acc: 0.8305\n",
      "Epoch 6/12\n",
      "1994/1994 [==============================] - 0s 73us/sample - loss: 0.3908 - acc: 0.8290\n",
      "Epoch 7/12\n",
      "1994/1994 [==============================] - 0s 59us/sample - loss: 0.3885 - acc: 0.8265\n",
      "Epoch 8/12\n",
      "1994/1994 [==============================] - 0s 60us/sample - loss: 0.3854 - acc: 0.8355\n",
      "Epoch 9/12\n",
      "1994/1994 [==============================] - 0s 56us/sample - loss: 0.3793 - acc: 0.8345\n",
      "Epoch 10/12\n",
      "1994/1994 [==============================] - 0s 65us/sample - loss: 0.3828 - acc: 0.8305\n",
      "Epoch 11/12\n",
      "1994/1994 [==============================] - 0s 54us/sample - loss: 0.3817 - acc: 0.8300\n",
      "Epoch 12/12\n",
      "1994/1994 [==============================] - 0s 58us/sample - loss: 0.3808 - acc: 0.8325\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "target_model.set_weights(state.model.trainable)\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.567003367003367, 0.566622691292876, 0.5532194480946123, 0.4868073878627968, 0.542225201072386, 0.5072751322751323, 0.5129427792915532, 0.5383064516129032, 0.5513333333333333, 0.5749500333111259]\n",
      "[0.7007874015748031, 0.7162673392181589, 0.7325581395348837, 0.7803030303030303, 0.7536, 0.7272727272727273, 0.7209302325581395, 0.7194950911640954, 0.7036114570361146, 0.760806916426513]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Test the success of the attack.\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "mia_accuracy = []\n",
    "mia_precision = []\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(np.argmax(attacker_y_test, axis=1)) if d == c]\n",
    "    data_in = [X[target_indices], y[target_indices]]\n",
    "    data_out = [[attacker_X_test[test_indices]], attacker_y_test[test_indices]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    mia_accuracy.append(attack_accuracy)\n",
    "    mia_precision.append(precision_score(real_membership_labels, attack_guesses))\n",
    "print(mia_accuracy)\n",
    "print(mia_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/MIA/rE2C2.txt', 'w') as log:\n",
    "        print(\"rE2C2acc = {}\".format(mia_accuracy), file=log)\n",
    "        print(\"rE2C2pre = {}\".format(mia_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167537343955065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7296367659521652"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7021517553793885, 0.7156398104265402, 0.7280701754385965, 0.7718832891246684, 0.741307371349096, 0.7302504816955684, 0.7214885954381752, 0.7171428571428572, 0.7144535840188014, 0.7539797395079595])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
