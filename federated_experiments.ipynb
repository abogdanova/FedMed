{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 2\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_uneven(source):\n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': tf.keras.utils.to_categorical(np.array([source[1][b] for b in batch_samples], dtype=np.int32))})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    #indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = tf.keras.utils.to_categorical(np.array(source[1][:10000], dtype=np.int32))\n",
    "    X = np.array(source[0][:10000], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'iid':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    else:\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': tf.keras.utils.to_categorical(np.array([source[1][b] for b in batch_samples], dtype=np.int32))})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'non-iid') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'non-iid') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = len(federated_train_data[0][0]['x'][1])\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=tf.keras.losses.categorical_crossentropy,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.6744 - categorical_accuracy: 0.4056 - val_loss: 1.4629 - val_categorical_accuracy: 0.4793\n",
      "Epoch 2/12\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3089 - categorical_accuracy: 0.5372 - val_loss: 1.2925 - val_categorical_accuracy: 0.5491\n",
      "Epoch 3/12\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.1114 - categorical_accuracy: 0.6147 - val_loss: 1.2163 - val_categorical_accuracy: 0.5767\n",
      "Epoch 4/12\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.9623 - categorical_accuracy: 0.6629 - val_loss: 1.2198 - val_categorical_accuracy: 0.5789\n",
      "Epoch 5/12\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.8365 - categorical_accuracy: 0.7067 - val_loss: 1.2313 - val_categorical_accuracy: 0.5863\n",
      "Epoch 6/12\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.6936 - categorical_accuracy: 0.7573 - val_loss: 1.2179 - val_categorical_accuracy: 0.5986\n",
      "Epoch 7/12\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.5654 - categorical_accuracy: 0.8072 - val_loss: 1.3155 - val_categorical_accuracy: 0.5812\n",
      "Epoch 8/12\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.4129 - categorical_accuracy: 0.8695 - val_loss: 1.3661 - val_categorical_accuracy: 0.5825\n",
      "Epoch 9/12\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.2922 - categorical_accuracy: 0.9162 - val_loss: 1.4246 - val_categorical_accuracy: 0.5960\n",
      "Epoch 10/12\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1932 - categorical_accuracy: 0.9551 - val_loss: 1.4775 - val_categorical_accuracy: 0.5933\n",
      "Epoch 11/12\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1112 - categorical_accuracy: 0.9804 - val_loss: 1.5657 - val_categorical_accuracy: 0.5961\n",
      "Epoch 12/12\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0598 - categorical_accuracy: 0.9935 - val_loss: 1.6493 - val_categorical_accuracy: 0.5947\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp2/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(history_callback.history[\"val_sparse_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=<categorical_accuracy=0.44825,loss=1.5218289>\n",
      "test accuracy: 0.4037500023841858\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "test_metrics = evaluation(state.model, federated_test_data)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(test_metrics.categorical_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  0, metrics=<sparse_categorical_accuracy=0.441375,loss=1.5422242>\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.542,loss=1.2852292>\n",
      "round  2, metrics=<sparse_categorical_accuracy=0.61525,loss=1.0956982>\n",
      "round  3, metrics=<sparse_categorical_accuracy=0.608125,loss=1.1381797>\n",
      "round  4, metrics=<sparse_categorical_accuracy=0.671125,loss=0.94584024>\n",
      "round  5, metrics=<sparse_categorical_accuracy=0.7105,loss=0.83895534>\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "state = iterative_process.initialize()\n",
    "fd_test_accuracy = []\n",
    "fd_train_loss = []\n",
    "for round_num in range(6):\n",
    "    selected = np.random.choice(5, 2, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "    test_metrics = evaluation(state.model, federated_test_data)\n",
    "    fd_train_loss.append(metrics[1])\n",
    "    fd_test_accuracy.append(test_metrics.sparse_categorical_accuracy)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5948632"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1190 - acc: 0.2440 - val_loss: 1.9590 - val_acc: 0.2846\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7290 - acc: 0.3980 - val_loss: 1.8585 - val_acc: 0.3410\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5974 - acc: 0.4520 - val_loss: 1.7614 - val_acc: 0.3788\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4253 - acc: 0.5290 - val_loss: 1.6777 - val_acc: 0.3944\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3012 - acc: 0.5550 - val_loss: 1.7050 - val_acc: 0.3966\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1126 - acc: 0.6500 - val_loss: 1.7451 - val_acc: 0.3902\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0079 - acc: 0.6770 - val_loss: 1.7554 - val_acc: 0.4156\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8570 - acc: 0.7340 - val_loss: 1.7414 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6845 - acc: 0.7880 - val_loss: 1.7564 - val_acc: 0.4180\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5715 - acc: 0.8260 - val_loss: 1.8183 - val_acc: 0.4128\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4442 - acc: 0.8840 - val_loss: 1.8667 - val_acc: 0.4148\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3486 - acc: 0.9150 - val_loss: 1.9364 - val_acc: 0.4170\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1904 - acc: 0.2270 - val_loss: 1.8969 - val_acc: 0.3414\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7393 - acc: 0.3940 - val_loss: 1.7504 - val_acc: 0.3786\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5385 - acc: 0.4590 - val_loss: 1.7045 - val_acc: 0.3994\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4037 - acc: 0.5270 - val_loss: 1.7093 - val_acc: 0.3978\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2525 - acc: 0.5940 - val_loss: 1.6606 - val_acc: 0.4252\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1121 - acc: 0.6220 - val_loss: 1.7862 - val_acc: 0.3834\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9881 - acc: 0.6790 - val_loss: 1.7424 - val_acc: 0.4152\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8295 - acc: 0.7400 - val_loss: 1.7525 - val_acc: 0.4222\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6734 - acc: 0.7910 - val_loss: 1.8202 - val_acc: 0.4188\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5470 - acc: 0.8420 - val_loss: 1.9658 - val_acc: 0.4050\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4330 - acc: 0.8860 - val_loss: 1.9533 - val_acc: 0.4152\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3282 - acc: 0.9350 - val_loss: 2.2226 - val_acc: 0.3824\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1216 - acc: 0.2290 - val_loss: 1.8406 - val_acc: 0.3444\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.7141 - acc: 0.3630 - val_loss: 1.7927 - val_acc: 0.3686\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5256 - acc: 0.4630 - val_loss: 1.7070 - val_acc: 0.3992\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3413 - acc: 0.5300 - val_loss: 1.7106 - val_acc: 0.3958\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2029 - acc: 0.5840 - val_loss: 1.7923 - val_acc: 0.3894\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0358 - acc: 0.6670 - val_loss: 1.7024 - val_acc: 0.4154\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.8298 - acc: 0.7350 - val_loss: 1.7616 - val_acc: 0.4126\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.6970 - acc: 0.7840 - val_loss: 1.8506 - val_acc: 0.4070\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5977 - acc: 0.8230 - val_loss: 1.8947 - val_acc: 0.4082\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4659 - acc: 0.8740 - val_loss: 1.9994 - val_acc: 0.4114\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3580 - acc: 0.9080 - val_loss: 1.9893 - val_acc: 0.4254\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.2505 - acc: 0.9530 - val_loss: 2.0713 - val_acc: 0.4284\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1673 - acc: 0.2270 - val_loss: 1.9708 - val_acc: 0.2932\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7302 - acc: 0.4100 - val_loss: 1.8838 - val_acc: 0.3152\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5661 - acc: 0.4640 - val_loss: 1.7030 - val_acc: 0.3990\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4182 - acc: 0.5130 - val_loss: 1.6961 - val_acc: 0.4008\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2230 - acc: 0.5880 - val_loss: 1.7060 - val_acc: 0.4108\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0768 - acc: 0.6410 - val_loss: 1.7573 - val_acc: 0.3950\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9539 - acc: 0.6810 - val_loss: 1.7299 - val_acc: 0.4208\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7782 - acc: 0.7540 - val_loss: 1.8079 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6448 - acc: 0.8150 - val_loss: 1.8731 - val_acc: 0.4050\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4922 - acc: 0.8830 - val_loss: 1.8941 - val_acc: 0.4170\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3899 - acc: 0.9130 - val_loss: 1.9983 - val_acc: 0.4066\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.2902 - acc: 0.9440 - val_loss: 2.1477 - val_acc: 0.3990\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1843 - acc: 0.2000 - val_loss: 1.8460 - val_acc: 0.3418\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7032 - acc: 0.4020 - val_loss: 1.7764 - val_acc: 0.3624\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5247 - acc: 0.4560 - val_loss: 1.6656 - val_acc: 0.4194\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3137 - acc: 0.5440 - val_loss: 1.6540 - val_acc: 0.4248\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1637 - acc: 0.5950 - val_loss: 1.7113 - val_acc: 0.4162\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9898 - acc: 0.6770 - val_loss: 1.7052 - val_acc: 0.4208\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8052 - acc: 0.7650 - val_loss: 1.7580 - val_acc: 0.4252\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7001 - acc: 0.7750 - val_loss: 1.8109 - val_acc: 0.4266\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5303 - acc: 0.8550 - val_loss: 2.1200 - val_acc: 0.3770\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4638 - acc: 0.8610 - val_loss: 2.0237 - val_acc: 0.4152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3019 - acc: 0.9360 - val_loss: 2.0376 - val_acc: 0.4356\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2049 - acc: 0.9710 - val_loss: 2.1072 - val_acc: 0.4270\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1833 - acc: 0.2120 - val_loss: 1.8398 - val_acc: 0.3330\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7961 - acc: 0.3650 - val_loss: 1.7796 - val_acc: 0.3564\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5951 - acc: 0.4550 - val_loss: 1.7328 - val_acc: 0.3836\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4091 - acc: 0.5300 - val_loss: 1.6611 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2312 - acc: 0.5690 - val_loss: 1.6687 - val_acc: 0.4140\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1084 - acc: 0.6350 - val_loss: 1.7513 - val_acc: 0.3966\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9635 - acc: 0.6980 - val_loss: 1.8730 - val_acc: 0.3842\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8351 - acc: 0.7250 - val_loss: 1.7396 - val_acc: 0.4218\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6489 - acc: 0.8040 - val_loss: 1.8236 - val_acc: 0.4134\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5207 - acc: 0.8610 - val_loss: 1.8635 - val_acc: 0.4214\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3937 - acc: 0.9020 - val_loss: 1.9437 - val_acc: 0.4190\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2924 - acc: 0.9390 - val_loss: 2.0512 - val_acc: 0.4084\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2154 - acc: 0.2210 - val_loss: 1.8781 - val_acc: 0.3338\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7271 - acc: 0.3790 - val_loss: 1.7542 - val_acc: 0.3642\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5133 - acc: 0.4900 - val_loss: 1.6979 - val_acc: 0.4008\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3777 - acc: 0.5350 - val_loss: 1.6525 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2020 - acc: 0.5940 - val_loss: 1.6840 - val_acc: 0.4078\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0491 - acc: 0.6510 - val_loss: 1.7220 - val_acc: 0.4080\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8811 - acc: 0.7210 - val_loss: 1.7670 - val_acc: 0.4066\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7533 - acc: 0.7690 - val_loss: 1.7549 - val_acc: 0.4316\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5863 - acc: 0.8300 - val_loss: 1.7999 - val_acc: 0.4266\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4708 - acc: 0.8730 - val_loss: 1.8575 - val_acc: 0.4328\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3447 - acc: 0.9260 - val_loss: 2.0079 - val_acc: 0.4236\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2577 - acc: 0.9590 - val_loss: 2.0882 - val_acc: 0.4254\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2528 - acc: 0.2120 - val_loss: 1.9859 - val_acc: 0.2762\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7971 - acc: 0.3790 - val_loss: 1.8326 - val_acc: 0.3454\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6003 - acc: 0.4540 - val_loss: 1.7923 - val_acc: 0.3544\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4322 - acc: 0.5120 - val_loss: 1.8342 - val_acc: 0.3598\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3258 - acc: 0.5510 - val_loss: 1.8195 - val_acc: 0.3658\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1475 - acc: 0.6180 - val_loss: 1.7666 - val_acc: 0.3860\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9750 - acc: 0.6900 - val_loss: 1.7335 - val_acc: 0.4164\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8189 - acc: 0.7460 - val_loss: 1.7597 - val_acc: 0.4226\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6622 - acc: 0.8130 - val_loss: 1.8394 - val_acc: 0.4212\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5298 - acc: 0.8460 - val_loss: 1.8898 - val_acc: 0.4306\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4155 - acc: 0.8960 - val_loss: 1.9948 - val_acc: 0.4172\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3297 - acc: 0.9180 - val_loss: 2.0476 - val_acc: 0.4172\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.3169 - acc: 0.1940 - val_loss: 1.9121 - val_acc: 0.3182\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7722 - acc: 0.3770 - val_loss: 1.8246 - val_acc: 0.3472\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6169 - acc: 0.4280 - val_loss: 1.7267 - val_acc: 0.3822\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4300 - acc: 0.5150 - val_loss: 1.7907 - val_acc: 0.3678\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3245 - acc: 0.5590 - val_loss: 1.7715 - val_acc: 0.3790\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1911 - acc: 0.5910 - val_loss: 1.7130 - val_acc: 0.3974\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0504 - acc: 0.6500 - val_loss: 1.7685 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8932 - acc: 0.7200 - val_loss: 1.7532 - val_acc: 0.4208\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7473 - acc: 0.7800 - val_loss: 1.8472 - val_acc: 0.4020\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6315 - acc: 0.8110 - val_loss: 1.8814 - val_acc: 0.4142\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5268 - acc: 0.8560 - val_loss: 1.9072 - val_acc: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4145 - acc: 0.9020 - val_loss: 1.9429 - val_acc: 0.4086\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2005 - acc: 0.2420 - val_loss: 1.9162 - val_acc: 0.3120\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7812 - acc: 0.3770 - val_loss: 1.7474 - val_acc: 0.3860\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6426 - acc: 0.4410 - val_loss: 1.7490 - val_acc: 0.3772\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4347 - acc: 0.5160 - val_loss: 1.7361 - val_acc: 0.3962\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2730 - acc: 0.5850 - val_loss: 1.6644 - val_acc: 0.4104\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0919 - acc: 0.6420 - val_loss: 1.7139 - val_acc: 0.4158\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9022 - acc: 0.7270 - val_loss: 1.7802 - val_acc: 0.3982\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7752 - acc: 0.7680 - val_loss: 1.7667 - val_acc: 0.4264\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5892 - acc: 0.8260 - val_loss: 1.8790 - val_acc: 0.4088\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4256 - acc: 0.8900 - val_loss: 1.9095 - val_acc: 0.4212\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3454 - acc: 0.9270 - val_loss: 2.0838 - val_acc: 0.4060\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2237 - acc: 0.9700 - val_loss: 2.1110 - val_acc: 0.4256\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Epoch 1/12\n",
      "1930/1930 [==============================] - 2s 973us/sample - loss: 0.5913 - acc: 0.7249\n",
      "Epoch 2/12\n",
      "1930/1930 [==============================] - 0s 153us/sample - loss: 0.4753 - acc: 0.7684\n",
      "Epoch 3/12\n",
      "1930/1930 [==============================] - 0s 83us/sample - loss: 0.4713 - acc: 0.7689\n",
      "Epoch 4/12\n",
      "1930/1930 [==============================] - 0s 108us/sample - loss: 0.4658 - acc: 0.7756\n",
      "Epoch 5/12\n",
      "1930/1930 [==============================] - 0s 120us/sample - loss: 0.4657 - acc: 0.7746\n",
      "Epoch 6/12\n",
      "1930/1930 [==============================] - 0s 88us/sample - loss: 0.4632 - acc: 0.7715\n",
      "Epoch 7/12\n",
      "1930/1930 [==============================] - 0s 78us/sample - loss: 0.4624 - acc: 0.7777\n",
      "Epoch 8/12\n",
      "1930/1930 [==============================] - 0s 84us/sample - loss: 0.4619 - acc: 0.7782\n",
      "Epoch 9/12\n",
      "1930/1930 [==============================] - 0s 197us/sample - loss: 0.4599 - acc: 0.7782\n",
      "Epoch 10/12\n",
      "1930/1930 [==============================] - 0s 100us/sample - loss: 0.4581 - acc: 0.7777\n",
      "Epoch 11/12\n",
      "1930/1930 [==============================] - 0s 76us/sample - loss: 0.4539 - acc: 0.7824\n",
      "Epoch 12/12\n",
      "1930/1930 [==============================] - 0s 105us/sample - loss: 0.4547 - acc: 0.7798\n",
      "Epoch 1/12\n",
      "1872/1872 [==============================] - 1s 589us/sample - loss: 0.5423 - acc: 0.7687\n",
      "Epoch 2/12\n",
      "1872/1872 [==============================] - 0s 76us/sample - loss: 0.4069 - acc: 0.8323\n",
      "Epoch 3/12\n",
      "1872/1872 [==============================] - 0s 80us/sample - loss: 0.3987 - acc: 0.8264\n",
      "Epoch 4/12\n",
      "1872/1872 [==============================] - 0s 82us/sample - loss: 0.3906 - acc: 0.8333\n",
      "Epoch 5/12\n",
      "1872/1872 [==============================] - 0s 75us/sample - loss: 0.3947 - acc: 0.8349\n",
      "Epoch 6/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3909 - acc: 0.8291\n",
      "Epoch 7/12\n",
      "1872/1872 [==============================] - 0s 78us/sample - loss: 0.3970 - acc: 0.8280\n",
      "Epoch 8/12\n",
      "1872/1872 [==============================] - 0s 118us/sample - loss: 0.3933 - acc: 0.8355\n",
      "Epoch 9/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3862 - acc: 0.8328\n",
      "Epoch 10/12\n",
      "1872/1872 [==============================] - 0s 101us/sample - loss: 0.3921 - acc: 0.8312\n",
      "Epoch 11/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3882 - acc: 0.8376\n",
      "Epoch 12/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3886 - acc: 0.8323\n",
      "Epoch 1/12\n",
      "2011/2011 [==============================] - 1s 616us/sample - loss: 0.5403 - acc: 0.7653\n",
      "Epoch 2/12\n",
      "2011/2011 [==============================] - 0s 75us/sample - loss: 0.3943 - acc: 0.8344\n",
      "Epoch 3/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3889 - acc: 0.8349\n",
      "Epoch 4/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3796 - acc: 0.8379\n",
      "Epoch 5/12\n",
      "2011/2011 [==============================] - 0s 73us/sample - loss: 0.3781 - acc: 0.8369\n",
      "Epoch 6/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3783 - acc: 0.8414\n",
      "Epoch 7/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3789 - acc: 0.8329\n",
      "Epoch 8/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3768 - acc: 0.8389\n",
      "Epoch 9/12\n",
      "2011/2011 [==============================] - 0s 85us/sample - loss: 0.3774 - acc: 0.8359\n",
      "Epoch 10/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3731 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "2011/2011 [==============================] - 0s 103us/sample - loss: 0.3727 - acc: 0.8339\n",
      "Epoch 12/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3747 - acc: 0.8364\n",
      "Epoch 1/12\n",
      "2104/2104 [==============================] - 1s 508us/sample - loss: 0.4769 - acc: 0.8427\n",
      "Epoch 2/12\n",
      "2104/2104 [==============================] - 0s 82us/sample - loss: 0.3295 - acc: 0.8736\n",
      "Epoch 3/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3237 - acc: 0.8707\n",
      "Epoch 4/12\n",
      "2104/2104 [==============================] - 0s 68us/sample - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 5/12\n",
      "2104/2104 [==============================] - 0s 71us/sample - loss: 0.3204 - acc: 0.8802\n",
      "Epoch 6/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3153 - acc: 0.8755\n",
      "Epoch 7/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3146 - acc: 0.8760\n",
      "Epoch 8/12\n",
      "2104/2104 [==============================] - 0s 69us/sample - loss: 0.3151 - acc: 0.8769\n",
      "Epoch 9/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3174 - acc: 0.8755\n",
      "Epoch 10/12\n",
      "2104/2104 [==============================] - 0s 113us/sample - loss: 0.3185 - acc: 0.8721\n",
      "Epoch 11/12\n",
      "2104/2104 [==============================] - 0s 110us/sample - loss: 0.3121 - acc: 0.8755\n",
      "Epoch 12/12\n",
      "2104/2104 [==============================] - 0s 101us/sample - loss: 0.3133 - acc: 0.8764\n",
      "Epoch 1/12\n",
      "1961/1961 [==============================] - 1s 624us/sample - loss: 0.5332 - acc: 0.8006\n",
      "Epoch 2/12\n",
      "1961/1961 [==============================] - 0s 71us/sample - loss: 0.3938 - acc: 0.8338\n",
      "Epoch 3/12\n",
      "1961/1961 [==============================] - 0s 64us/sample - loss: 0.3862 - acc: 0.8332\n",
      "Epoch 4/12\n",
      "1961/1961 [==============================] - 0s 76us/sample - loss: 0.3847 - acc: 0.8414\n",
      "Epoch 5/12\n",
      "1961/1961 [==============================] - 0s 73us/sample - loss: 0.3840 - acc: 0.8368\n",
      "Epoch 6/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3818 - acc: 0.8409\n",
      "Epoch 7/12\n",
      "1961/1961 [==============================] - 0s 65us/sample - loss: 0.3765 - acc: 0.8409\n",
      "Epoch 8/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3794 - acc: 0.8399\n",
      "Epoch 9/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3764 - acc: 0.8353\n",
      "Epoch 10/12\n",
      "1961/1961 [==============================] - 0s 66us/sample - loss: 0.3705 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "1961/1961 [==============================] - 0s 62us/sample - loss: 0.3706 - acc: 0.8404\n",
      "Epoch 12/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3750 - acc: 0.8404\n",
      "Epoch 1/12\n",
      "1922/1922 [==============================] - 1s 777us/sample - loss: 0.5168 - acc: 0.8293\n",
      "Epoch 2/12\n",
      "1922/1922 [==============================] - 0s 109us/sample - loss: 0.3586 - acc: 0.8574\n",
      "Epoch 3/12\n",
      "1922/1922 [==============================] - 0s 149us/sample - loss: 0.3523 - acc: 0.8585\n",
      "Epoch 4/12\n",
      "1922/1922 [==============================] - 0s 119us/sample - loss: 0.3535 - acc: 0.8611\n",
      "Epoch 5/12\n",
      "1922/1922 [==============================] - 0s 72us/sample - loss: 0.3469 - acc: 0.8580\n",
      "Epoch 6/12\n",
      "1922/1922 [==============================] - 0s 99us/sample - loss: 0.3490 - acc: 0.8595\n",
      "Epoch 7/12\n",
      "1922/1922 [==============================] - 0s 96us/sample - loss: 0.3515 - acc: 0.8600\n",
      "Epoch 8/12\n",
      "1922/1922 [==============================] - 0s 123us/sample - loss: 0.3522 - acc: 0.8590\n",
      "Epoch 9/12\n",
      "1922/1922 [==============================] - 0s 94us/sample - loss: 0.3459 - acc: 0.8626\n",
      "Epoch 10/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3446 - acc: 0.8585\n",
      "Epoch 11/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3440 - acc: 0.8658\n",
      "Epoch 12/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 1/12\n",
      "2073/2073 [==============================] - 1s 712us/sample - loss: 0.6026 - acc: 0.7159\n",
      "Epoch 2/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.5090 - acc: 0.7501\n",
      "Epoch 3/12\n",
      "2073/2073 [==============================] - 0s 90us/sample - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 4/12\n",
      "2073/2073 [==============================] - 0s 88us/sample - loss: 0.4939 - acc: 0.7574\n",
      "Epoch 5/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.4907 - acc: 0.7569\n",
      "Epoch 6/12\n",
      "2073/2073 [==============================] - 0s 86us/sample - loss: 0.4939 - acc: 0.7511\n",
      "Epoch 7/12\n",
      "2073/2073 [==============================] - 0s 120us/sample - loss: 0.4897 - acc: 0.7545\n",
      "Epoch 8/12\n",
      "2073/2073 [==============================] - 0s 112us/sample - loss: 0.4851 - acc: 0.7525\n",
      "Epoch 9/12\n",
      "2073/2073 [==============================] - 0s 89us/sample - loss: 0.4836 - acc: 0.7574\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 0s 73us/sample - loss: 0.4815 - acc: 0.7593\n",
      "Epoch 11/12\n",
      "2073/2073 [==============================] - 0s 67us/sample - loss: 0.4809 - acc: 0.7583\n",
      "Epoch 12/12\n",
      "2073/2073 [==============================] - 0s 74us/sample - loss: 0.4839 - acc: 0.7612\n",
      "Epoch 1/12\n",
      "2060/2060 [==============================] - 1s 666us/sample - loss: 0.5656 - acc: 0.7524\n",
      "Epoch 2/12\n",
      "2060/2060 [==============================] - 0s 96us/sample - loss: 0.4461 - acc: 0.8034\n",
      "Epoch 3/12\n",
      "2060/2060 [==============================] - 0s 115us/sample - loss: 0.4400 - acc: 0.8049\n",
      "Epoch 4/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4378 - acc: 0.8068\n",
      "Epoch 5/12\n",
      "2060/2060 [==============================] - 0s 82us/sample - loss: 0.4332 - acc: 0.8063\n",
      "Epoch 6/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4285 - acc: 0.8112\n",
      "Epoch 7/12\n",
      "2060/2060 [==============================] - 0s 99us/sample - loss: 0.4340 - acc: 0.8083\n",
      "Epoch 8/12\n",
      "2060/2060 [==============================] - 0s 95us/sample - loss: 0.4242 - acc: 0.8107\n",
      "Epoch 9/12\n",
      "2060/2060 [==============================] - 0s 69us/sample - loss: 0.4254 - acc: 0.8087\n",
      "Epoch 10/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4294 - acc: 0.8053\n",
      "Epoch 11/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4233 - acc: 0.8126\n",
      "Epoch 12/12\n",
      "2060/2060 [==============================] - 0s 73us/sample - loss: 0.4243 - acc: 0.8092\n",
      "Epoch 1/12\n",
      "2068/2068 [==============================] - 1s 625us/sample - loss: 0.6028 - acc: 0.7191\n",
      "Epoch 2/12\n",
      "2068/2068 [==============================] - 0s 111us/sample - loss: 0.5009 - acc: 0.7606\n",
      "Epoch 3/12\n",
      "2068/2068 [==============================] - 0s 100us/sample - loss: 0.4894 - acc: 0.7611\n",
      "Epoch 4/12\n",
      "2068/2068 [==============================] - 0s 105us/sample - loss: 0.4856 - acc: 0.7602\n",
      "Epoch 5/12\n",
      "2068/2068 [==============================] - 0s 99us/sample - loss: 0.4867 - acc: 0.7703\n",
      "Epoch 6/12\n",
      "2068/2068 [==============================] - 0s 113us/sample - loss: 0.4813 - acc: 0.7655\n",
      "Epoch 7/12\n",
      "2068/2068 [==============================] - 0s 85us/sample - loss: 0.4828 - acc: 0.7655\n",
      "Epoch 8/12\n",
      "2068/2068 [==============================] - 0s 63us/sample - loss: 0.4779 - acc: 0.7722\n",
      "Epoch 9/12\n",
      "2068/2068 [==============================] - 0s 65us/sample - loss: 0.4774 - acc: 0.7747\n",
      "Epoch 10/12\n",
      "2068/2068 [==============================] - 0s 64us/sample - loss: 0.4781 - acc: 0.7713\n",
      "Epoch 11/12\n",
      "2068/2068 [==============================] - 0s 77us/sample - loss: 0.4806 - acc: 0.7737\n",
      "Epoch 12/12\n",
      "2068/2068 [==============================] - 0s 71us/sample - loss: 0.4771 - acc: 0.7703\n",
      "Epoch 1/12\n",
      "1999/1999 [==============================] - 1s 609us/sample - loss: 0.5430 - acc: 0.7764\n",
      "Epoch 2/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.3979 - acc: 0.8339\n",
      "Epoch 3/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.4011 - acc: 0.8279\n",
      "Epoch 4/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3924 - acc: 0.8349\n",
      "Epoch 5/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3964 - acc: 0.8309\n",
      "Epoch 6/12\n",
      "1999/1999 [==============================] - 0s 75us/sample - loss: 0.3945 - acc: 0.8304\n",
      "Epoch 7/12\n",
      "1999/1999 [==============================] - 0s 72us/sample - loss: 0.3871 - acc: 0.8404\n",
      "Epoch 8/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3892 - acc: 0.8324\n",
      "Epoch 9/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3878 - acc: 0.8339\n",
      "Epoch 10/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3863 - acc: 0.8329\n",
      "Epoch 11/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3829 - acc: 0.8329\n",
      "Epoch 12/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3859 - acc: 0.8369\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "#non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.57      0.60      5000\n",
      "\n",
      "0.5654\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.32      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.57      0.60      5000\n",
      "\n",
      "0.5642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      0.99      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.56      0.60      5000\n",
      "\n",
      "0.5628\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.46      0.63      4000\n",
      "        1.0       0.31      0.99      0.47      1000\n",
      "\n",
      "avg / total       0.86      0.56      0.60      5000\n",
      "\n",
      "0.5648\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.56      0.60      5000\n",
      "\n",
      "0.565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.56      0.60      5000\n",
      "\n",
      "0.5644\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      0.99      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.56      0.60      5000\n",
      "\n",
      "0.5652\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.31      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.57      0.60      5000\n",
      "\n",
      "0.5572\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.46      0.62      4000\n",
      "        1.0       0.31      0.96      0.46      1000\n",
      "\n",
      "avg / total       0.84      0.56      0.59      5000\n",
      "\n",
      "0.5654\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.46      0.63      4000\n",
      "        1.0       0.32      1.00      0.48      1000\n",
      "\n",
      "avg / total       0.86      0.57      0.60      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the success of the attack.\n",
    "\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    indices_array = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    \n",
    "    data_in = [X[indices_array], y[indices_array]]\n",
    "    data_out = [[attacker_X_test[:ATTACK_TEST_DATASET_SIZE]], attacker_y_test[:ATTACK_TEST_DATASET_SIZE]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "\n",
    "    print (attack_accuracy)\n",
    "    print (classification_report(real_membership_labels, attack_guesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
