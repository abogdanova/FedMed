{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 2\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_realistic(y, u):\n",
    "    # split dataset into arrays of each class label\n",
    "    all_indices = [i for i, d in enumerate(y)]\n",
    "    shares_arr = [5000, 3000, 1000, 750, 250]\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append([all_indices.pop(0) for i in range(shares_arr[u])]) \n",
    "    return user_indices\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    #indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = tf.keras.utils.to_categorical(np.array(source[1][:10000], dtype=np.int32))\n",
    "    X = np.array(source[0][:10000], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'i':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    elif distribution == 'n':\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    elif distribution == 'r':\n",
    "        indices = get_indices_realistic(source[1][:10000], u)[u]\n",
    "    else:\n",
    "        indices = np.array(get_indices_unbalanced_completely(source[1])[u])\n",
    "    \n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': np.array([source[1][b] for b in batch_samples], dtype=np.int32)})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'r') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'r') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = get_indices_realistic(cifar_test[1], 0)\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 3)"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=loss_fn,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.6744 - categorical_accuracy: 0.4056 - val_loss: 1.4629 - val_categorical_accuracy: 0.4793\n",
      "Epoch 2/12\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3089 - categorical_accuracy: 0.5372 - val_loss: 1.2925 - val_categorical_accuracy: 0.5491\n",
      "Epoch 3/12\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 1.1114 - categorical_accuracy: 0.6147 - val_loss: 1.2163 - val_categorical_accuracy: 0.5767\n",
      "Epoch 4/12\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.9623 - categorical_accuracy: 0.6629 - val_loss: 1.2198 - val_categorical_accuracy: 0.5789\n",
      "Epoch 5/12\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.8365 - categorical_accuracy: 0.7067 - val_loss: 1.2313 - val_categorical_accuracy: 0.5863\n",
      "Epoch 6/12\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.6936 - categorical_accuracy: 0.7573 - val_loss: 1.2179 - val_categorical_accuracy: 0.5986\n",
      "Epoch 7/12\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.5654 - categorical_accuracy: 0.8072 - val_loss: 1.3155 - val_categorical_accuracy: 0.5812\n",
      "Epoch 8/12\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.4129 - categorical_accuracy: 0.8695 - val_loss: 1.3661 - val_categorical_accuracy: 0.5825\n",
      "Epoch 9/12\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.2922 - categorical_accuracy: 0.9162 - val_loss: 1.4246 - val_categorical_accuracy: 0.5960\n",
      "Epoch 10/12\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1932 - categorical_accuracy: 0.9551 - val_loss: 1.4775 - val_categorical_accuracy: 0.5933\n",
      "Epoch 11/12\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1112 - categorical_accuracy: 0.9804 - val_loss: 1.5657 - val_categorical_accuracy: 0.5961\n",
      "Epoch 12/12\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0598 - categorical_accuracy: 0.9935 - val_loss: 1.6493 - val_categorical_accuracy: 0.5947\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  1, metrics=<sparse_categorical_accuracy=0.384,loss=1.6947489>\n",
      "test accuracy: 0.3668749928474426\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "test_metrics = evaluation(state.model, federated_test_data)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(test_metrics.sparse_categorical_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  0, metrics=<sparse_categorical_accuracy=0.36533332,loss=1.7711151>\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.528,loss=1.3403175>\n",
      "round  2, metrics=<sparse_categorical_accuracy=0.63815385,loss=1.0338538>\n",
      "round  3, metrics=<sparse_categorical_accuracy=0.7309333,loss=0.8049824>\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "state = iterative_process.initialize()\n",
    "fd_test_accuracy = []\n",
    "fd_train_loss = []\n",
    "for round_num in range(12):\n",
    "    selected = np.random.choice(5, 2, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "    test_metrics = evaluation(state.model, federated_test_data)\n",
    "    fd_train_loss.append(metrics[1])\n",
    "    fd_test_accuracy.append(test_metrics.sparse_categorical_accuracy)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5948632"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1190 - acc: 0.2440 - val_loss: 1.9590 - val_acc: 0.2846\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7290 - acc: 0.3980 - val_loss: 1.8585 - val_acc: 0.3410\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5974 - acc: 0.4520 - val_loss: 1.7614 - val_acc: 0.3788\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4253 - acc: 0.5290 - val_loss: 1.6777 - val_acc: 0.3944\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3012 - acc: 0.5550 - val_loss: 1.7050 - val_acc: 0.3966\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1126 - acc: 0.6500 - val_loss: 1.7451 - val_acc: 0.3902\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0079 - acc: 0.6770 - val_loss: 1.7554 - val_acc: 0.4156\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8570 - acc: 0.7340 - val_loss: 1.7414 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6845 - acc: 0.7880 - val_loss: 1.7564 - val_acc: 0.4180\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5715 - acc: 0.8260 - val_loss: 1.8183 - val_acc: 0.4128\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4442 - acc: 0.8840 - val_loss: 1.8667 - val_acc: 0.4148\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3486 - acc: 0.9150 - val_loss: 1.9364 - val_acc: 0.4170\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1904 - acc: 0.2270 - val_loss: 1.8969 - val_acc: 0.3414\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7393 - acc: 0.3940 - val_loss: 1.7504 - val_acc: 0.3786\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5385 - acc: 0.4590 - val_loss: 1.7045 - val_acc: 0.3994\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4037 - acc: 0.5270 - val_loss: 1.7093 - val_acc: 0.3978\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2525 - acc: 0.5940 - val_loss: 1.6606 - val_acc: 0.4252\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1121 - acc: 0.6220 - val_loss: 1.7862 - val_acc: 0.3834\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9881 - acc: 0.6790 - val_loss: 1.7424 - val_acc: 0.4152\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8295 - acc: 0.7400 - val_loss: 1.7525 - val_acc: 0.4222\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6734 - acc: 0.7910 - val_loss: 1.8202 - val_acc: 0.4188\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5470 - acc: 0.8420 - val_loss: 1.9658 - val_acc: 0.4050\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4330 - acc: 0.8860 - val_loss: 1.9533 - val_acc: 0.4152\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3282 - acc: 0.9350 - val_loss: 2.2226 - val_acc: 0.3824\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1216 - acc: 0.2290 - val_loss: 1.8406 - val_acc: 0.3444\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.7141 - acc: 0.3630 - val_loss: 1.7927 - val_acc: 0.3686\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5256 - acc: 0.4630 - val_loss: 1.7070 - val_acc: 0.3992\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3413 - acc: 0.5300 - val_loss: 1.7106 - val_acc: 0.3958\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2029 - acc: 0.5840 - val_loss: 1.7923 - val_acc: 0.3894\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0358 - acc: 0.6670 - val_loss: 1.7024 - val_acc: 0.4154\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.8298 - acc: 0.7350 - val_loss: 1.7616 - val_acc: 0.4126\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.6970 - acc: 0.7840 - val_loss: 1.8506 - val_acc: 0.4070\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5977 - acc: 0.8230 - val_loss: 1.8947 - val_acc: 0.4082\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4659 - acc: 0.8740 - val_loss: 1.9994 - val_acc: 0.4114\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3580 - acc: 0.9080 - val_loss: 1.9893 - val_acc: 0.4254\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.2505 - acc: 0.9530 - val_loss: 2.0713 - val_acc: 0.4284\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1673 - acc: 0.2270 - val_loss: 1.9708 - val_acc: 0.2932\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7302 - acc: 0.4100 - val_loss: 1.8838 - val_acc: 0.3152\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5661 - acc: 0.4640 - val_loss: 1.7030 - val_acc: 0.3990\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4182 - acc: 0.5130 - val_loss: 1.6961 - val_acc: 0.4008\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2230 - acc: 0.5880 - val_loss: 1.7060 - val_acc: 0.4108\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0768 - acc: 0.6410 - val_loss: 1.7573 - val_acc: 0.3950\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9539 - acc: 0.6810 - val_loss: 1.7299 - val_acc: 0.4208\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7782 - acc: 0.7540 - val_loss: 1.8079 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6448 - acc: 0.8150 - val_loss: 1.8731 - val_acc: 0.4050\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4922 - acc: 0.8830 - val_loss: 1.8941 - val_acc: 0.4170\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3899 - acc: 0.9130 - val_loss: 1.9983 - val_acc: 0.4066\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.2902 - acc: 0.9440 - val_loss: 2.1477 - val_acc: 0.3990\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1843 - acc: 0.2000 - val_loss: 1.8460 - val_acc: 0.3418\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7032 - acc: 0.4020 - val_loss: 1.7764 - val_acc: 0.3624\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5247 - acc: 0.4560 - val_loss: 1.6656 - val_acc: 0.4194\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3137 - acc: 0.5440 - val_loss: 1.6540 - val_acc: 0.4248\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1637 - acc: 0.5950 - val_loss: 1.7113 - val_acc: 0.4162\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9898 - acc: 0.6770 - val_loss: 1.7052 - val_acc: 0.4208\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8052 - acc: 0.7650 - val_loss: 1.7580 - val_acc: 0.4252\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7001 - acc: 0.7750 - val_loss: 1.8109 - val_acc: 0.4266\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5303 - acc: 0.8550 - val_loss: 2.1200 - val_acc: 0.3770\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4638 - acc: 0.8610 - val_loss: 2.0237 - val_acc: 0.4152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3019 - acc: 0.9360 - val_loss: 2.0376 - val_acc: 0.4356\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2049 - acc: 0.9710 - val_loss: 2.1072 - val_acc: 0.4270\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1833 - acc: 0.2120 - val_loss: 1.8398 - val_acc: 0.3330\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7961 - acc: 0.3650 - val_loss: 1.7796 - val_acc: 0.3564\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5951 - acc: 0.4550 - val_loss: 1.7328 - val_acc: 0.3836\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4091 - acc: 0.5300 - val_loss: 1.6611 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2312 - acc: 0.5690 - val_loss: 1.6687 - val_acc: 0.4140\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1084 - acc: 0.6350 - val_loss: 1.7513 - val_acc: 0.3966\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9635 - acc: 0.6980 - val_loss: 1.8730 - val_acc: 0.3842\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8351 - acc: 0.7250 - val_loss: 1.7396 - val_acc: 0.4218\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6489 - acc: 0.8040 - val_loss: 1.8236 - val_acc: 0.4134\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5207 - acc: 0.8610 - val_loss: 1.8635 - val_acc: 0.4214\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3937 - acc: 0.9020 - val_loss: 1.9437 - val_acc: 0.4190\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2924 - acc: 0.9390 - val_loss: 2.0512 - val_acc: 0.4084\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2154 - acc: 0.2210 - val_loss: 1.8781 - val_acc: 0.3338\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7271 - acc: 0.3790 - val_loss: 1.7542 - val_acc: 0.3642\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5133 - acc: 0.4900 - val_loss: 1.6979 - val_acc: 0.4008\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3777 - acc: 0.5350 - val_loss: 1.6525 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2020 - acc: 0.5940 - val_loss: 1.6840 - val_acc: 0.4078\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0491 - acc: 0.6510 - val_loss: 1.7220 - val_acc: 0.4080\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8811 - acc: 0.7210 - val_loss: 1.7670 - val_acc: 0.4066\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7533 - acc: 0.7690 - val_loss: 1.7549 - val_acc: 0.4316\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5863 - acc: 0.8300 - val_loss: 1.7999 - val_acc: 0.4266\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4708 - acc: 0.8730 - val_loss: 1.8575 - val_acc: 0.4328\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3447 - acc: 0.9260 - val_loss: 2.0079 - val_acc: 0.4236\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2577 - acc: 0.9590 - val_loss: 2.0882 - val_acc: 0.4254\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2528 - acc: 0.2120 - val_loss: 1.9859 - val_acc: 0.2762\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7971 - acc: 0.3790 - val_loss: 1.8326 - val_acc: 0.3454\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6003 - acc: 0.4540 - val_loss: 1.7923 - val_acc: 0.3544\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4322 - acc: 0.5120 - val_loss: 1.8342 - val_acc: 0.3598\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3258 - acc: 0.5510 - val_loss: 1.8195 - val_acc: 0.3658\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1475 - acc: 0.6180 - val_loss: 1.7666 - val_acc: 0.3860\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9750 - acc: 0.6900 - val_loss: 1.7335 - val_acc: 0.4164\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8189 - acc: 0.7460 - val_loss: 1.7597 - val_acc: 0.4226\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6622 - acc: 0.8130 - val_loss: 1.8394 - val_acc: 0.4212\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5298 - acc: 0.8460 - val_loss: 1.8898 - val_acc: 0.4306\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4155 - acc: 0.8960 - val_loss: 1.9948 - val_acc: 0.4172\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3297 - acc: 0.9180 - val_loss: 2.0476 - val_acc: 0.4172\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.3169 - acc: 0.1940 - val_loss: 1.9121 - val_acc: 0.3182\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7722 - acc: 0.3770 - val_loss: 1.8246 - val_acc: 0.3472\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6169 - acc: 0.4280 - val_loss: 1.7267 - val_acc: 0.3822\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4300 - acc: 0.5150 - val_loss: 1.7907 - val_acc: 0.3678\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3245 - acc: 0.5590 - val_loss: 1.7715 - val_acc: 0.3790\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1911 - acc: 0.5910 - val_loss: 1.7130 - val_acc: 0.3974\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0504 - acc: 0.6500 - val_loss: 1.7685 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8932 - acc: 0.7200 - val_loss: 1.7532 - val_acc: 0.4208\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7473 - acc: 0.7800 - val_loss: 1.8472 - val_acc: 0.4020\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6315 - acc: 0.8110 - val_loss: 1.8814 - val_acc: 0.4142\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5268 - acc: 0.8560 - val_loss: 1.9072 - val_acc: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4145 - acc: 0.9020 - val_loss: 1.9429 - val_acc: 0.4086\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2005 - acc: 0.2420 - val_loss: 1.9162 - val_acc: 0.3120\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7812 - acc: 0.3770 - val_loss: 1.7474 - val_acc: 0.3860\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6426 - acc: 0.4410 - val_loss: 1.7490 - val_acc: 0.3772\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4347 - acc: 0.5160 - val_loss: 1.7361 - val_acc: 0.3962\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2730 - acc: 0.5850 - val_loss: 1.6644 - val_acc: 0.4104\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0919 - acc: 0.6420 - val_loss: 1.7139 - val_acc: 0.4158\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9022 - acc: 0.7270 - val_loss: 1.7802 - val_acc: 0.3982\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7752 - acc: 0.7680 - val_loss: 1.7667 - val_acc: 0.4264\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5892 - acc: 0.8260 - val_loss: 1.8790 - val_acc: 0.4088\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4256 - acc: 0.8900 - val_loss: 1.9095 - val_acc: 0.4212\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3454 - acc: 0.9270 - val_loss: 2.0838 - val_acc: 0.4060\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2237 - acc: 0.9700 - val_loss: 2.1110 - val_acc: 0.4256\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Epoch 1/12\n",
      "1930/1930 [==============================] - 2s 973us/sample - loss: 0.5913 - acc: 0.7249\n",
      "Epoch 2/12\n",
      "1930/1930 [==============================] - 0s 153us/sample - loss: 0.4753 - acc: 0.7684\n",
      "Epoch 3/12\n",
      "1930/1930 [==============================] - 0s 83us/sample - loss: 0.4713 - acc: 0.7689\n",
      "Epoch 4/12\n",
      "1930/1930 [==============================] - 0s 108us/sample - loss: 0.4658 - acc: 0.7756\n",
      "Epoch 5/12\n",
      "1930/1930 [==============================] - 0s 120us/sample - loss: 0.4657 - acc: 0.7746\n",
      "Epoch 6/12\n",
      "1930/1930 [==============================] - 0s 88us/sample - loss: 0.4632 - acc: 0.7715\n",
      "Epoch 7/12\n",
      "1930/1930 [==============================] - 0s 78us/sample - loss: 0.4624 - acc: 0.7777\n",
      "Epoch 8/12\n",
      "1930/1930 [==============================] - 0s 84us/sample - loss: 0.4619 - acc: 0.7782\n",
      "Epoch 9/12\n",
      "1930/1930 [==============================] - 0s 197us/sample - loss: 0.4599 - acc: 0.7782\n",
      "Epoch 10/12\n",
      "1930/1930 [==============================] - 0s 100us/sample - loss: 0.4581 - acc: 0.7777\n",
      "Epoch 11/12\n",
      "1930/1930 [==============================] - 0s 76us/sample - loss: 0.4539 - acc: 0.7824\n",
      "Epoch 12/12\n",
      "1930/1930 [==============================] - 0s 105us/sample - loss: 0.4547 - acc: 0.7798\n",
      "Epoch 1/12\n",
      "1872/1872 [==============================] - 1s 589us/sample - loss: 0.5423 - acc: 0.7687\n",
      "Epoch 2/12\n",
      "1872/1872 [==============================] - 0s 76us/sample - loss: 0.4069 - acc: 0.8323\n",
      "Epoch 3/12\n",
      "1872/1872 [==============================] - 0s 80us/sample - loss: 0.3987 - acc: 0.8264\n",
      "Epoch 4/12\n",
      "1872/1872 [==============================] - 0s 82us/sample - loss: 0.3906 - acc: 0.8333\n",
      "Epoch 5/12\n",
      "1872/1872 [==============================] - 0s 75us/sample - loss: 0.3947 - acc: 0.8349\n",
      "Epoch 6/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3909 - acc: 0.8291\n",
      "Epoch 7/12\n",
      "1872/1872 [==============================] - 0s 78us/sample - loss: 0.3970 - acc: 0.8280\n",
      "Epoch 8/12\n",
      "1872/1872 [==============================] - 0s 118us/sample - loss: 0.3933 - acc: 0.8355\n",
      "Epoch 9/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3862 - acc: 0.8328\n",
      "Epoch 10/12\n",
      "1872/1872 [==============================] - 0s 101us/sample - loss: 0.3921 - acc: 0.8312\n",
      "Epoch 11/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3882 - acc: 0.8376\n",
      "Epoch 12/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3886 - acc: 0.8323\n",
      "Epoch 1/12\n",
      "2011/2011 [==============================] - 1s 616us/sample - loss: 0.5403 - acc: 0.7653\n",
      "Epoch 2/12\n",
      "2011/2011 [==============================] - 0s 75us/sample - loss: 0.3943 - acc: 0.8344\n",
      "Epoch 3/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3889 - acc: 0.8349\n",
      "Epoch 4/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3796 - acc: 0.8379\n",
      "Epoch 5/12\n",
      "2011/2011 [==============================] - 0s 73us/sample - loss: 0.3781 - acc: 0.8369\n",
      "Epoch 6/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3783 - acc: 0.8414\n",
      "Epoch 7/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3789 - acc: 0.8329\n",
      "Epoch 8/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3768 - acc: 0.8389\n",
      "Epoch 9/12\n",
      "2011/2011 [==============================] - 0s 85us/sample - loss: 0.3774 - acc: 0.8359\n",
      "Epoch 10/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3731 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "2011/2011 [==============================] - 0s 103us/sample - loss: 0.3727 - acc: 0.8339\n",
      "Epoch 12/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3747 - acc: 0.8364\n",
      "Epoch 1/12\n",
      "2104/2104 [==============================] - 1s 508us/sample - loss: 0.4769 - acc: 0.8427\n",
      "Epoch 2/12\n",
      "2104/2104 [==============================] - 0s 82us/sample - loss: 0.3295 - acc: 0.8736\n",
      "Epoch 3/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3237 - acc: 0.8707\n",
      "Epoch 4/12\n",
      "2104/2104 [==============================] - 0s 68us/sample - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 5/12\n",
      "2104/2104 [==============================] - 0s 71us/sample - loss: 0.3204 - acc: 0.8802\n",
      "Epoch 6/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3153 - acc: 0.8755\n",
      "Epoch 7/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3146 - acc: 0.8760\n",
      "Epoch 8/12\n",
      "2104/2104 [==============================] - 0s 69us/sample - loss: 0.3151 - acc: 0.8769\n",
      "Epoch 9/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3174 - acc: 0.8755\n",
      "Epoch 10/12\n",
      "2104/2104 [==============================] - 0s 113us/sample - loss: 0.3185 - acc: 0.8721\n",
      "Epoch 11/12\n",
      "2104/2104 [==============================] - 0s 110us/sample - loss: 0.3121 - acc: 0.8755\n",
      "Epoch 12/12\n",
      "2104/2104 [==============================] - 0s 101us/sample - loss: 0.3133 - acc: 0.8764\n",
      "Epoch 1/12\n",
      "1961/1961 [==============================] - 1s 624us/sample - loss: 0.5332 - acc: 0.8006\n",
      "Epoch 2/12\n",
      "1961/1961 [==============================] - 0s 71us/sample - loss: 0.3938 - acc: 0.8338\n",
      "Epoch 3/12\n",
      "1961/1961 [==============================] - 0s 64us/sample - loss: 0.3862 - acc: 0.8332\n",
      "Epoch 4/12\n",
      "1961/1961 [==============================] - 0s 76us/sample - loss: 0.3847 - acc: 0.8414\n",
      "Epoch 5/12\n",
      "1961/1961 [==============================] - 0s 73us/sample - loss: 0.3840 - acc: 0.8368\n",
      "Epoch 6/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3818 - acc: 0.8409\n",
      "Epoch 7/12\n",
      "1961/1961 [==============================] - 0s 65us/sample - loss: 0.3765 - acc: 0.8409\n",
      "Epoch 8/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3794 - acc: 0.8399\n",
      "Epoch 9/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3764 - acc: 0.8353\n",
      "Epoch 10/12\n",
      "1961/1961 [==============================] - 0s 66us/sample - loss: 0.3705 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "1961/1961 [==============================] - 0s 62us/sample - loss: 0.3706 - acc: 0.8404\n",
      "Epoch 12/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3750 - acc: 0.8404\n",
      "Epoch 1/12\n",
      "1922/1922 [==============================] - 1s 777us/sample - loss: 0.5168 - acc: 0.8293\n",
      "Epoch 2/12\n",
      "1922/1922 [==============================] - 0s 109us/sample - loss: 0.3586 - acc: 0.8574\n",
      "Epoch 3/12\n",
      "1922/1922 [==============================] - 0s 149us/sample - loss: 0.3523 - acc: 0.8585\n",
      "Epoch 4/12\n",
      "1922/1922 [==============================] - 0s 119us/sample - loss: 0.3535 - acc: 0.8611\n",
      "Epoch 5/12\n",
      "1922/1922 [==============================] - 0s 72us/sample - loss: 0.3469 - acc: 0.8580\n",
      "Epoch 6/12\n",
      "1922/1922 [==============================] - 0s 99us/sample - loss: 0.3490 - acc: 0.8595\n",
      "Epoch 7/12\n",
      "1922/1922 [==============================] - 0s 96us/sample - loss: 0.3515 - acc: 0.8600\n",
      "Epoch 8/12\n",
      "1922/1922 [==============================] - 0s 123us/sample - loss: 0.3522 - acc: 0.8590\n",
      "Epoch 9/12\n",
      "1922/1922 [==============================] - 0s 94us/sample - loss: 0.3459 - acc: 0.8626\n",
      "Epoch 10/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3446 - acc: 0.8585\n",
      "Epoch 11/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3440 - acc: 0.8658\n",
      "Epoch 12/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 1/12\n",
      "2073/2073 [==============================] - 1s 712us/sample - loss: 0.6026 - acc: 0.7159\n",
      "Epoch 2/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.5090 - acc: 0.7501\n",
      "Epoch 3/12\n",
      "2073/2073 [==============================] - 0s 90us/sample - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 4/12\n",
      "2073/2073 [==============================] - 0s 88us/sample - loss: 0.4939 - acc: 0.7574\n",
      "Epoch 5/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.4907 - acc: 0.7569\n",
      "Epoch 6/12\n",
      "2073/2073 [==============================] - 0s 86us/sample - loss: 0.4939 - acc: 0.7511\n",
      "Epoch 7/12\n",
      "2073/2073 [==============================] - 0s 120us/sample - loss: 0.4897 - acc: 0.7545\n",
      "Epoch 8/12\n",
      "2073/2073 [==============================] - 0s 112us/sample - loss: 0.4851 - acc: 0.7525\n",
      "Epoch 9/12\n",
      "2073/2073 [==============================] - 0s 89us/sample - loss: 0.4836 - acc: 0.7574\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 0s 73us/sample - loss: 0.4815 - acc: 0.7593\n",
      "Epoch 11/12\n",
      "2073/2073 [==============================] - 0s 67us/sample - loss: 0.4809 - acc: 0.7583\n",
      "Epoch 12/12\n",
      "2073/2073 [==============================] - 0s 74us/sample - loss: 0.4839 - acc: 0.7612\n",
      "Epoch 1/12\n",
      "2060/2060 [==============================] - 1s 666us/sample - loss: 0.5656 - acc: 0.7524\n",
      "Epoch 2/12\n",
      "2060/2060 [==============================] - 0s 96us/sample - loss: 0.4461 - acc: 0.8034\n",
      "Epoch 3/12\n",
      "2060/2060 [==============================] - 0s 115us/sample - loss: 0.4400 - acc: 0.8049\n",
      "Epoch 4/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4378 - acc: 0.8068\n",
      "Epoch 5/12\n",
      "2060/2060 [==============================] - 0s 82us/sample - loss: 0.4332 - acc: 0.8063\n",
      "Epoch 6/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4285 - acc: 0.8112\n",
      "Epoch 7/12\n",
      "2060/2060 [==============================] - 0s 99us/sample - loss: 0.4340 - acc: 0.8083\n",
      "Epoch 8/12\n",
      "2060/2060 [==============================] - 0s 95us/sample - loss: 0.4242 - acc: 0.8107\n",
      "Epoch 9/12\n",
      "2060/2060 [==============================] - 0s 69us/sample - loss: 0.4254 - acc: 0.8087\n",
      "Epoch 10/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4294 - acc: 0.8053\n",
      "Epoch 11/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4233 - acc: 0.8126\n",
      "Epoch 12/12\n",
      "2060/2060 [==============================] - 0s 73us/sample - loss: 0.4243 - acc: 0.8092\n",
      "Epoch 1/12\n",
      "2068/2068 [==============================] - 1s 625us/sample - loss: 0.6028 - acc: 0.7191\n",
      "Epoch 2/12\n",
      "2068/2068 [==============================] - 0s 111us/sample - loss: 0.5009 - acc: 0.7606\n",
      "Epoch 3/12\n",
      "2068/2068 [==============================] - 0s 100us/sample - loss: 0.4894 - acc: 0.7611\n",
      "Epoch 4/12\n",
      "2068/2068 [==============================] - 0s 105us/sample - loss: 0.4856 - acc: 0.7602\n",
      "Epoch 5/12\n",
      "2068/2068 [==============================] - 0s 99us/sample - loss: 0.4867 - acc: 0.7703\n",
      "Epoch 6/12\n",
      "2068/2068 [==============================] - 0s 113us/sample - loss: 0.4813 - acc: 0.7655\n",
      "Epoch 7/12\n",
      "2068/2068 [==============================] - 0s 85us/sample - loss: 0.4828 - acc: 0.7655\n",
      "Epoch 8/12\n",
      "2068/2068 [==============================] - 0s 63us/sample - loss: 0.4779 - acc: 0.7722\n",
      "Epoch 9/12\n",
      "2068/2068 [==============================] - 0s 65us/sample - loss: 0.4774 - acc: 0.7747\n",
      "Epoch 10/12\n",
      "2068/2068 [==============================] - 0s 64us/sample - loss: 0.4781 - acc: 0.7713\n",
      "Epoch 11/12\n",
      "2068/2068 [==============================] - 0s 77us/sample - loss: 0.4806 - acc: 0.7737\n",
      "Epoch 12/12\n",
      "2068/2068 [==============================] - 0s 71us/sample - loss: 0.4771 - acc: 0.7703\n",
      "Epoch 1/12\n",
      "1999/1999 [==============================] - 1s 609us/sample - loss: 0.5430 - acc: 0.7764\n",
      "Epoch 2/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.3979 - acc: 0.8339\n",
      "Epoch 3/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.4011 - acc: 0.8279\n",
      "Epoch 4/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3924 - acc: 0.8349\n",
      "Epoch 5/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3964 - acc: 0.8309\n",
      "Epoch 6/12\n",
      "1999/1999 [==============================] - 0s 75us/sample - loss: 0.3945 - acc: 0.8304\n",
      "Epoch 7/12\n",
      "1999/1999 [==============================] - 0s 72us/sample - loss: 0.3871 - acc: 0.8404\n",
      "Epoch 8/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3892 - acc: 0.8324\n",
      "Epoch 9/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3878 - acc: 0.8339\n",
      "Epoch 10/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3863 - acc: 0.8329\n",
      "Epoch 11/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3829 - acc: 0.8329\n",
      "Epoch 12/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3859 - acc: 0.8369\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948]\n",
      "[0.742772424017791, 0.7492307692307693, 0.8375510204081633, 0.8439597315436241, 0.8300764655904843, 0.8011996572407883, 0.7648809523809523, 0.7786438035853468, 0.7712121212121212, 0.7638347622759158]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Test the success of the attack.\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "mia_accuracy = []\n",
    "mia_precision = []\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(np.argmax(attacker_y_test, axis=1)) if d == c]\n",
    "    data_in = [X[target_indices], y[target_indices]]\n",
    "    data_out = [[attacker_X_test[test_indices]], attacker_y_test[test_indices]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    mia_accuracy.append(attack_accuracy)\n",
    "    mia_precision.append(precision_score(real_membership_labels, attack_guesses))\n",
    "print(mia_accuracy)\n",
    "print(mia_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/MIA/rE1C5.txt', 'w') as log:\n",
    "        print(\"rE1C5acc = {}\".format(mia_accuracy), file=log)\n",
    "        print(\"rE1C5pre = {}\".format(mia_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
