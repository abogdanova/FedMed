{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 4\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_realistic(y, u):\n",
    "    # split dataset into arrays of each class label\n",
    "    all_indices = [i for i, d in enumerate(y)]\n",
    "    shares_arr = [5000, 3000, 1000, 750, 250]\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append([all_indices.pop(0) for i in range(shares_arr[u])]) \n",
    "    return user_indices\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = np.array(source[1][indices], dtype=np.int32)\n",
    "    X = np.array(source[0][indices], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'i':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    elif distribution == 'n':\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    elif distribution == 'r':\n",
    "        indices = get_indices_realistic(source[1][:10000], u)[u]\n",
    "    else:\n",
    "        indices = np.array(get_indices_unbalanced_completely(source[1])[u])\n",
    "    \n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': np.array([source[1][b] for b in batch_samples], dtype=np.int32)})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'i') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'i') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f30429c224a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=loss_fn,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 1.6453 - categorical_accuracy: 0.4105 - val_loss: 1.4564 - val_categorical_accuracy: 0.4887\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.2948 - categorical_accuracy: 0.5413 - val_loss: 1.3070 - val_categorical_accuracy: 0.5377\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.0989 - categorical_accuracy: 0.6127 - val_loss: 1.2251 - val_categorical_accuracy: 0.5706\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.9609 - categorical_accuracy: 0.6592 - val_loss: 1.2384 - val_categorical_accuracy: 0.5791\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.8286 - categorical_accuracy: 0.7077 - val_loss: 1.2404 - val_categorical_accuracy: 0.5842\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.6916 - categorical_accuracy: 0.7620 - val_loss: 1.2497 - val_categorical_accuracy: 0.5907\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.5374 - categorical_accuracy: 0.8224 - val_loss: 1.3085 - val_categorical_accuracy: 0.5914\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4065 - categorical_accuracy: 0.8718 - val_loss: 1.3898 - val_categorical_accuracy: 0.5857\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.2734 - categorical_accuracy: 0.9245 - val_loss: 1.4569 - val_categorical_accuracy: 0.5969\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1703 - categorical_accuracy: 0.9636 - val_loss: 1.5415 - val_categorical_accuracy: 0.5924\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.1021 - categorical_accuracy: 0.9831 - val_loss: 1.6110 - val_categorical_accuracy: 0.5882\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.0585 - categorical_accuracy: 0.9945 - val_loss: 1.6453 - val_categorical_accuracy: 0.5936\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0314 - categorical_accuracy: 0.9982 - val_loss: 1.7216 - val_categorical_accuracy: 0.5952\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0179 - categorical_accuracy: 0.9996 - val_loss: 1.7587 - val_categorical_accuracy: 0.6038\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0107 - categorical_accuracy: 0.9999 - val_loss: 1.8129 - val_categorical_accuracy: 0.6051\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8519 - val_categorical_accuracy: 0.6043\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0058 - categorical_accuracy: 1.0000 - val_loss: 1.9056 - val_categorical_accuracy: 0.6017\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0044 - categorical_accuracy: 1.0000 - val_loss: 1.9266 - val_categorical_accuracy: 0.6014\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0046 - categorical_accuracy: 0.9999 - val_loss: 1.9657 - val_categorical_accuracy: 0.6035\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 1.9959 - val_categorical_accuracy: 0.6053\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 2.0312 - val_categorical_accuracy: 0.6022\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 2.0627 - val_categorical_accuracy: 0.6027\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 2.0921 - val_categorical_accuracy: 0.6026\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 2.1258 - val_categorical_accuracy: 0.6026\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 9.9374e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1454 - val_categorical_accuracy: 0.6023\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 8.2107e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1764 - val_categorical_accuracy: 0.6021\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 6.8230e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2064 - val_categorical_accuracy: 0.6013\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 5.7202e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2441 - val_categorical_accuracy: 0.6027\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7293e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2656 - val_categorical_accuracy: 0.6025\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 4.0281e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2940 - val_categorical_accuracy: 0.6022\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.2552e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3168 - val_categorical_accuracy: 0.6021\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.7323e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3425 - val_categorical_accuracy: 0.6016\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 2.2841e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3709 - val_categorical_accuracy: 0.6003\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.9136e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3944 - val_categorical_accuracy: 0.6014\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.6010e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4202 - val_categorical_accuracy: 0.6012\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3392e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4532 - val_categorical_accuracy: 0.6016\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1291e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4735 - val_categorical_accuracy: 0.6004\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 9.5869e-05 - categorical_accuracy: 1.0000 - val_loss: 2.4977 - val_categorical_accuracy: 0.6009\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 7.9403e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5172 - val_categorical_accuracy: 0.6002\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 6.7607e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5467 - val_categorical_accuracy: 0.6008\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 5.8769e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5714 - val_categorical_accuracy: 0.6007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7460e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5931 - val_categorical_accuracy: 0.6008\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 3.9733e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6141 - val_categorical_accuracy: 0.5998\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.3674e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6374 - val_categorical_accuracy: 0.6006\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 2.8456e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6608 - val_categorical_accuracy: 0.6018\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 2.3901e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6810 - val_categorical_accuracy: 0.6009\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 2.0279e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7009 - val_categorical_accuracy: 0.6002\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.7068e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7247 - val_categorical_accuracy: 0.5999\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 1.4409e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7476 - val_categorical_accuracy: 0.5995\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.2274e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7663 - val_categorical_accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 442us/sample - loss: 1.6801 - sparse_categorical_accuracy: 0.4095\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.3275,loss=1.8733339>\n",
      "test accuracy: 0.40950000286102295\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "# test_metrics = evaluation(state.model, federated_test_data)\n",
    "non_federated_model.set_weights(state.model.trainable)\n",
    "(loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "epochs = 12\n",
    "state = iterative_process.initialize()\n",
    "fd_test_loss = []\n",
    "fd_train_loss = []\n",
    "fd_train_accuracy = []\n",
    "for round_num in range(12):\n",
    "    selected = np.random.choice(5, 5, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "#     non_federated_model.set_weights(state.model.trainable)\n",
    "#     fd_train_loss.append(metrics[1])\n",
    "#     fd_test_accuracy.append(accuracy)\n",
    "#     fd_test_loss.append(loss)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4527,\n",
       " 0.4398,\n",
       " 0.4791,\n",
       " 0.5053,\n",
       " 0.5295,\n",
       " 0.5456,\n",
       " 0.5595,\n",
       " 0.5747,\n",
       " 0.5807,\n",
       " 0.5853,\n",
       " 0.5906,\n",
       " 0.5918,\n",
       " 0.5935,\n",
       " 0.4413,\n",
       " 0.4805,\n",
       " 0.5017,\n",
       " 0.5206,\n",
       " 0.5365,\n",
       " 0.5511,\n",
       " 0.5626,\n",
       " 0.5729,\n",
       " 0.5803,\n",
       " 0.5867,\n",
       " 0.5858,\n",
       " 0.5863]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "y = tf.keras.utils.to_categorical(y)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1514 - acc: 0.2390 - val_loss: 1.8460 - val_acc: 0.3520\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7084 - acc: 0.3960 - val_loss: 1.7144 - val_acc: 0.4000\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5218 - acc: 0.4870 - val_loss: 1.8279 - val_acc: 0.3628\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3486 - acc: 0.5540 - val_loss: 1.6742 - val_acc: 0.4198\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.1761 - acc: 0.6220 - val_loss: 1.6894 - val_acc: 0.4090\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0216 - acc: 0.6720 - val_loss: 1.7723 - val_acc: 0.3964\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.8635 - acc: 0.7360 - val_loss: 1.7523 - val_acc: 0.4194\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7117 - acc: 0.7790 - val_loss: 1.7706 - val_acc: 0.4254\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5368 - acc: 0.8610 - val_loss: 1.9036 - val_acc: 0.4040\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4216 - acc: 0.8940 - val_loss: 2.0062 - val_acc: 0.4022\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3341 - acc: 0.9330 - val_loss: 2.0000 - val_acc: 0.4314\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.2266 - acc: 0.9530 - val_loss: 2.0727 - val_acc: 0.4232\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.0827 - acc: 0.2580 - val_loss: 1.8686 - val_acc: 0.3228\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6957 - acc: 0.4250 - val_loss: 1.8081 - val_acc: 0.3722\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5077 - acc: 0.4880 - val_loss: 1.7518 - val_acc: 0.3806\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3231 - acc: 0.5320 - val_loss: 1.8030 - val_acc: 0.3652\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1726 - acc: 0.5950 - val_loss: 1.7528 - val_acc: 0.3910\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0306 - acc: 0.6640 - val_loss: 1.6796 - val_acc: 0.4190\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8681 - acc: 0.7170 - val_loss: 1.7695 - val_acc: 0.4090\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7299 - acc: 0.7690 - val_loss: 1.7779 - val_acc: 0.4162\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5671 - acc: 0.8470 - val_loss: 1.8619 - val_acc: 0.4152\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5024 - acc: 0.8520 - val_loss: 1.8900 - val_acc: 0.4188\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3452 - acc: 0.9230 - val_loss: 2.0426 - val_acc: 0.4016\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2664 - acc: 0.9470 - val_loss: 2.0627 - val_acc: 0.4174\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.2520 - acc: 0.2160 - val_loss: 2.0721 - val_acc: 0.2290\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7689 - acc: 0.3700 - val_loss: 1.7494 - val_acc: 0.3634\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5254 - acc: 0.4740 - val_loss: 1.6880 - val_acc: 0.3934\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3709 - acc: 0.5200 - val_loss: 1.7263 - val_acc: 0.3854\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2294 - acc: 0.5790 - val_loss: 1.7106 - val_acc: 0.3976\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.0584 - acc: 0.6570 - val_loss: 1.6738 - val_acc: 0.4206\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9107 - acc: 0.7080 - val_loss: 1.7850 - val_acc: 0.3990\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7459 - acc: 0.7730 - val_loss: 1.8289 - val_acc: 0.4104\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6297 - acc: 0.8130 - val_loss: 1.9517 - val_acc: 0.3964\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5131 - acc: 0.8520 - val_loss: 1.9560 - val_acc: 0.4068\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3747 - acc: 0.9100 - val_loss: 1.9855 - val_acc: 0.4268\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2455 - acc: 0.9490 - val_loss: 2.0356 - val_acc: 0.4308\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.2229 - acc: 0.2090 - val_loss: 1.8834 - val_acc: 0.3414\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7496 - acc: 0.4020 - val_loss: 1.7752 - val_acc: 0.3712\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5823 - acc: 0.4650 - val_loss: 1.7838 - val_acc: 0.3716\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4101 - acc: 0.5350 - val_loss: 1.7201 - val_acc: 0.3840\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2195 - acc: 0.6060 - val_loss: 1.7667 - val_acc: 0.3840\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1225 - acc: 0.6170 - val_loss: 1.7496 - val_acc: 0.3908\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9489 - acc: 0.7050 - val_loss: 1.7538 - val_acc: 0.4070\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7702 - acc: 0.7790 - val_loss: 1.7727 - val_acc: 0.4114\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6756 - acc: 0.8050 - val_loss: 1.9124 - val_acc: 0.3858\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5362 - acc: 0.8480 - val_loss: 1.9737 - val_acc: 0.4112\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4016 - acc: 0.9090 - val_loss: 1.9995 - val_acc: 0.4120\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3030 - acc: 0.9410 - val_loss: 2.0651 - val_acc: 0.4082\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1483 - acc: 0.2570 - val_loss: 1.8631 - val_acc: 0.3318\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6701 - acc: 0.4210 - val_loss: 1.7882 - val_acc: 0.3674\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4759 - acc: 0.4960 - val_loss: 1.7918 - val_acc: 0.3754\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3645 - acc: 0.5440 - val_loss: 1.7275 - val_acc: 0.3928\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1871 - acc: 0.6020 - val_loss: 1.7753 - val_acc: 0.3960\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0329 - acc: 0.6590 - val_loss: 1.7708 - val_acc: 0.3984\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8648 - acc: 0.7230 - val_loss: 1.7949 - val_acc: 0.4064\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7276 - acc: 0.7690 - val_loss: 1.9002 - val_acc: 0.3966\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6063 - acc: 0.8220 - val_loss: 2.0011 - val_acc: 0.3820\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4818 - acc: 0.8580 - val_loss: 1.9523 - val_acc: 0.4064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3578 - acc: 0.9250 - val_loss: 2.0819 - val_acc: 0.3988\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2724 - acc: 0.9430 - val_loss: 2.1703 - val_acc: 0.3900\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1744 - acc: 0.2320 - val_loss: 1.8765 - val_acc: 0.3460\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6935 - acc: 0.4120 - val_loss: 1.7020 - val_acc: 0.3980\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5055 - acc: 0.4820 - val_loss: 1.6957 - val_acc: 0.3896\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3498 - acc: 0.5350 - val_loss: 1.7554 - val_acc: 0.3886\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2239 - acc: 0.5790 - val_loss: 1.6528 - val_acc: 0.4194\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0620 - acc: 0.6510 - val_loss: 1.7356 - val_acc: 0.3968\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9507 - acc: 0.6720 - val_loss: 1.7267 - val_acc: 0.4232\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7999 - acc: 0.7490 - val_loss: 1.7824 - val_acc: 0.4162\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6876 - acc: 0.7970 - val_loss: 1.8014 - val_acc: 0.4276\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5254 - acc: 0.8400 - val_loss: 1.8539 - val_acc: 0.4304\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4169 - acc: 0.8900 - val_loss: 1.9182 - val_acc: 0.4242\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3250 - acc: 0.9220 - val_loss: 1.9752 - val_acc: 0.4224\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.2105 - acc: 0.2240 - val_loss: 1.8724 - val_acc: 0.3262\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7404 - acc: 0.4060 - val_loss: 1.9236 - val_acc: 0.3292\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5679 - acc: 0.4760 - val_loss: 1.7223 - val_acc: 0.3986\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4278 - acc: 0.5230 - val_loss: 1.7164 - val_acc: 0.3948\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2624 - acc: 0.5670 - val_loss: 1.7315 - val_acc: 0.4054\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0695 - acc: 0.6560 - val_loss: 1.6888 - val_acc: 0.4214\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9440 - acc: 0.6940 - val_loss: 1.8215 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8127 - acc: 0.7580 - val_loss: 1.7607 - val_acc: 0.4186\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6531 - acc: 0.8040 - val_loss: 1.8889 - val_acc: 0.4134\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5695 - acc: 0.8320 - val_loss: 1.8682 - val_acc: 0.4270\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4131 - acc: 0.8990 - val_loss: 1.9319 - val_acc: 0.4282\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3196 - acc: 0.9390 - val_loss: 2.0481 - val_acc: 0.4160\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1143 - acc: 0.2280 - val_loss: 1.9185 - val_acc: 0.2976\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6389 - acc: 0.4040 - val_loss: 1.7703 - val_acc: 0.3700\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4571 - acc: 0.4790 - val_loss: 1.7400 - val_acc: 0.3810\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2624 - acc: 0.5680 - val_loss: 1.7401 - val_acc: 0.3834\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0776 - acc: 0.6430 - val_loss: 1.6499 - val_acc: 0.4168\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8764 - acc: 0.7210 - val_loss: 1.6856 - val_acc: 0.4262\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7285 - acc: 0.7850 - val_loss: 1.8671 - val_acc: 0.3872\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5656 - acc: 0.8360 - val_loss: 1.8655 - val_acc: 0.4156\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3938 - acc: 0.9050 - val_loss: 1.9982 - val_acc: 0.3996\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3137 - acc: 0.9280 - val_loss: 2.0106 - val_acc: 0.4156\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2026 - acc: 0.9680 - val_loss: 2.1395 - val_acc: 0.4090\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1390 - acc: 0.9840 - val_loss: 2.1655 - val_acc: 0.4168\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1885 - acc: 0.2280 - val_loss: 1.8490 - val_acc: 0.3446\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7227 - acc: 0.3800 - val_loss: 1.7420 - val_acc: 0.3924\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4950 - acc: 0.4820 - val_loss: 1.7049 - val_acc: 0.3850\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3724 - acc: 0.5040 - val_loss: 1.7305 - val_acc: 0.3804\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2103 - acc: 0.5890 - val_loss: 1.7108 - val_acc: 0.3888\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0888 - acc: 0.6280 - val_loss: 1.7534 - val_acc: 0.3814\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9638 - acc: 0.6900 - val_loss: 1.7569 - val_acc: 0.4134\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8390 - acc: 0.7250 - val_loss: 1.7663 - val_acc: 0.4066\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7001 - acc: 0.7880 - val_loss: 1.8438 - val_acc: 0.4102\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6024 - acc: 0.8250 - val_loss: 1.8613 - val_acc: 0.4082\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4501 - acc: 0.8960 - val_loss: 1.9855 - val_acc: 0.4110\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3603 - acc: 0.9160 - val_loss: 2.0364 - val_acc: 0.4174\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 2.1883 - acc: 0.2160 - val_loss: 1.9125 - val_acc: 0.3008\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7343 - acc: 0.3750 - val_loss: 1.7620 - val_acc: 0.3656\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5279 - acc: 0.4820 - val_loss: 1.6508 - val_acc: 0.4208\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3326 - acc: 0.5640 - val_loss: 1.6908 - val_acc: 0.4036\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1816 - acc: 0.6120 - val_loss: 1.7871 - val_acc: 0.3850\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0603 - acc: 0.6440 - val_loss: 1.6662 - val_acc: 0.4288\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8916 - acc: 0.7040 - val_loss: 1.7094 - val_acc: 0.4292\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7479 - acc: 0.7770 - val_loss: 1.8170 - val_acc: 0.4132\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5742 - acc: 0.8550 - val_loss: 1.8565 - val_acc: 0.4186\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4391 - acc: 0.8940 - val_loss: 1.8557 - val_acc: 0.4244\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3258 - acc: 0.9390 - val_loss: 2.0200 - val_acc: 0.4128\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2361 - acc: 0.9640 - val_loss: 2.0403 - val_acc: 0.4264\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 10:00:19.899251 4588352960 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "1978/1978 [==============================] - 1s 329us/sample - loss: 0.5788 - acc: 0.7361\n",
      "Epoch 2/12\n",
      "1978/1978 [==============================] - 0s 99us/sample - loss: 0.4824 - acc: 0.7735\n",
      "Epoch 3/12\n",
      "1978/1978 [==============================] - 0s 90us/sample - loss: 0.4785 - acc: 0.7730\n",
      "Epoch 4/12\n",
      "1978/1978 [==============================] - 0s 70us/sample - loss: 0.4723 - acc: 0.7801\n",
      "Epoch 5/12\n",
      "1978/1978 [==============================] - 0s 52us/sample - loss: 0.4683 - acc: 0.7705\n",
      "Epoch 6/12\n",
      "1978/1978 [==============================] - 0s 49us/sample - loss: 0.4735 - acc: 0.7786\n",
      "Epoch 7/12\n",
      "1978/1978 [==============================] - 0s 48us/sample - loss: 0.4643 - acc: 0.7816\n",
      "Epoch 8/12\n",
      "1978/1978 [==============================] - 0s 49us/sample - loss: 0.4659 - acc: 0.7725\n",
      "Epoch 9/12\n",
      "1978/1978 [==============================] - 0s 57us/sample - loss: 0.4645 - acc: 0.7710\n",
      "Epoch 10/12\n",
      "1978/1978 [==============================] - 0s 70us/sample - loss: 0.4628 - acc: 0.7760\n",
      "Epoch 11/12\n",
      "1978/1978 [==============================] - 0s 57us/sample - loss: 0.4643 - acc: 0.7806\n",
      "Epoch 12/12\n",
      "1978/1978 [==============================] - 0s 69us/sample - loss: 0.4596 - acc: 0.7781\n",
      "Epoch 1/12\n",
      "2025/2025 [==============================] - 1s 249us/sample - loss: 0.5825 - acc: 0.7067\n",
      "Epoch 2/12\n",
      "2025/2025 [==============================] - 0s 49us/sample - loss: 0.4335 - acc: 0.8094\n",
      "Epoch 3/12\n",
      "2025/2025 [==============================] - 0s 73us/sample - loss: 0.4267 - acc: 0.8114\n",
      "Epoch 4/12\n",
      "2025/2025 [==============================] - 0s 74us/sample - loss: 0.4179 - acc: 0.8128\n",
      "Epoch 5/12\n",
      "2025/2025 [==============================] - 0s 58us/sample - loss: 0.4147 - acc: 0.8114\n",
      "Epoch 6/12\n",
      "2025/2025 [==============================] - 0s 50us/sample - loss: 0.4153 - acc: 0.8123\n",
      "Epoch 7/12\n",
      "2025/2025 [==============================] - 0s 50us/sample - loss: 0.4192 - acc: 0.8079\n",
      "Epoch 8/12\n",
      "2025/2025 [==============================] - 0s 54us/sample - loss: 0.4172 - acc: 0.8114\n",
      "Epoch 9/12\n",
      "2025/2025 [==============================] - 0s 52us/sample - loss: 0.4171 - acc: 0.8114\n",
      "Epoch 10/12\n",
      "2025/2025 [==============================] - 0s 66us/sample - loss: 0.4156 - acc: 0.8104\n",
      "Epoch 11/12\n",
      "2025/2025 [==============================] - 0s 61us/sample - loss: 0.4157 - acc: 0.8094\n",
      "Epoch 12/12\n",
      "2025/2025 [==============================] - 0s 78us/sample - loss: 0.4174 - acc: 0.8094\n",
      "Epoch 1/12\n",
      "1905/1905 [==============================] - 0s 246us/sample - loss: 0.5034 - acc: 0.8425\n",
      "Epoch 2/12\n",
      "1905/1905 [==============================] - 0s 57us/sample - loss: 0.3617 - acc: 0.8572\n",
      "Epoch 3/12\n",
      "1905/1905 [==============================] - 0s 55us/sample - loss: 0.3508 - acc: 0.8640\n",
      "Epoch 4/12\n",
      "1905/1905 [==============================] - 0s 52us/sample - loss: 0.3464 - acc: 0.8651\n",
      "Epoch 5/12\n",
      "1905/1905 [==============================] - 0s 52us/sample - loss: 0.3489 - acc: 0.8625\n",
      "Epoch 6/12\n",
      "1905/1905 [==============================] - 0s 59us/sample - loss: 0.3494 - acc: 0.8656\n",
      "Epoch 7/12\n",
      "1905/1905 [==============================] - 0s 169us/sample - loss: 0.3473 - acc: 0.8609\n",
      "Epoch 8/12\n",
      "1905/1905 [==============================] - 0s 67us/sample - loss: 0.3452 - acc: 0.8609\n",
      "Epoch 9/12\n",
      "1905/1905 [==============================] - 0s 59us/sample - loss: 0.3391 - acc: 0.8656\n",
      "Epoch 10/12\n",
      "1905/1905 [==============================] - 0s 92us/sample - loss: 0.3455 - acc: 0.8651\n",
      "Epoch 11/12\n",
      "1905/1905 [==============================] - 0s 71us/sample - loss: 0.3409 - acc: 0.8640\n",
      "Epoch 12/12\n",
      "1905/1905 [==============================] - 0s 59us/sample - loss: 0.3395 - acc: 0.8635\n",
      "Epoch 1/12\n",
      "2096/2096 [==============================] - 1s 426us/sample - loss: 0.4987 - acc: 0.8225\n",
      "Epoch 2/12\n",
      "2096/2096 [==============================] - 0s 184us/sample - loss: 0.3554 - acc: 0.8593\n",
      "Epoch 3/12\n",
      "2096/2096 [==============================] - 0s 174us/sample - loss: 0.3463 - acc: 0.8569\n",
      "Epoch 4/12\n",
      "2096/2096 [==============================] - 0s 122us/sample - loss: 0.3402 - acc: 0.8602\n",
      "Epoch 5/12\n",
      "2096/2096 [==============================] - 0s 70us/sample - loss: 0.3381 - acc: 0.8597\n",
      "Epoch 6/12\n",
      "2096/2096 [==============================] - 0s 88us/sample - loss: 0.3363 - acc: 0.8564\n",
      "Epoch 7/12\n",
      "2096/2096 [==============================] - 0s 107us/sample - loss: 0.3332 - acc: 0.8607\n",
      "Epoch 8/12\n",
      "2096/2096 [==============================] - 0s 112us/sample - loss: 0.3353 - acc: 0.8640\n",
      "Epoch 9/12\n",
      "2096/2096 [==============================] - 1s 254us/sample - loss: 0.3334 - acc: 0.8602\n",
      "Epoch 10/12\n",
      "2096/2096 [==============================] - 0s 139us/sample - loss: 0.3299 - acc: 0.8655\n",
      "Epoch 11/12\n",
      "2096/2096 [==============================] - 0s 131us/sample - loss: 0.3295 - acc: 0.8635\n",
      "Epoch 12/12\n",
      "2096/2096 [==============================] - 0s 90us/sample - loss: 0.3251 - acc: 0.8635\n",
      "Epoch 1/12\n",
      "2007/2007 [==============================] - 1s 272us/sample - loss: 0.5564 - acc: 0.7504\n",
      "Epoch 2/12\n",
      "2007/2007 [==============================] - 0s 72us/sample - loss: 0.4092 - acc: 0.8291\n",
      "Epoch 3/12\n",
      "2007/2007 [==============================] - 0s 73us/sample - loss: 0.4082 - acc: 0.8261\n",
      "Epoch 4/12\n",
      "2007/2007 [==============================] - 0s 65us/sample - loss: 0.4034 - acc: 0.8281\n",
      "Epoch 5/12\n",
      "2007/2007 [==============================] - 0s 70us/sample - loss: 0.4019 - acc: 0.8251\n",
      "Epoch 6/12\n",
      "2007/2007 [==============================] - 0s 96us/sample - loss: 0.3962 - acc: 0.8296\n",
      "Epoch 7/12\n",
      "2007/2007 [==============================] - 0s 93us/sample - loss: 0.3996 - acc: 0.8351\n",
      "Epoch 8/12\n",
      "2007/2007 [==============================] - 0s 130us/sample - loss: 0.3937 - acc: 0.8311\n",
      "Epoch 9/12\n",
      "2007/2007 [==============================] - 0s 151us/sample - loss: 0.3920 - acc: 0.8286\n",
      "Epoch 10/12\n",
      "2007/2007 [==============================] - 0s 184us/sample - loss: 0.3912 - acc: 0.8326\n",
      "Epoch 11/12\n",
      "2007/2007 [==============================] - 0s 122us/sample - loss: 0.3916 - acc: 0.8306\n",
      "Epoch 12/12\n",
      "2007/2007 [==============================] - 0s 99us/sample - loss: 0.3992 - acc: 0.8261\n",
      "Epoch 1/12\n",
      "1898/1898 [==============================] - 1s 375us/sample - loss: 0.5214 - acc: 0.8051\n",
      "Epoch 2/12\n",
      "1898/1898 [==============================] - 0s 96us/sample - loss: 0.3709 - acc: 0.8483\n",
      "Epoch 3/12\n",
      "1898/1898 [==============================] - 0s 76us/sample - loss: 0.3571 - acc: 0.8525\n",
      "Epoch 4/12\n",
      "1898/1898 [==============================] - 0s 81us/sample - loss: 0.3608 - acc: 0.8567\n",
      "Epoch 5/12\n",
      "1898/1898 [==============================] - 0s 73us/sample - loss: 0.3521 - acc: 0.8562\n",
      "Epoch 6/12\n",
      "1898/1898 [==============================] - 0s 74us/sample - loss: 0.3536 - acc: 0.8546\n",
      "Epoch 7/12\n",
      "1898/1898 [==============================] - 0s 76us/sample - loss: 0.3511 - acc: 0.8498\n",
      "Epoch 8/12\n",
      "1898/1898 [==============================] - 0s 71us/sample - loss: 0.3473 - acc: 0.8567\n",
      "Epoch 9/12\n",
      "1898/1898 [==============================] - 0s 72us/sample - loss: 0.3437 - acc: 0.8572\n",
      "Epoch 10/12\n",
      "1898/1898 [==============================] - 0s 73us/sample - loss: 0.3465 - acc: 0.8556\n",
      "Epoch 11/12\n",
      "1898/1898 [==============================] - 0s 66us/sample - loss: 0.3437 - acc: 0.8599\n",
      "Epoch 12/12\n",
      "1898/1898 [==============================] - 0s 65us/sample - loss: 0.3424 - acc: 0.8577\n",
      "Epoch 1/12\n",
      "1936/1936 [==============================] - 1s 301us/sample - loss: 0.6002 - acc: 0.6911\n",
      "Epoch 2/12\n",
      "1936/1936 [==============================] - 0s 61us/sample - loss: 0.4566 - acc: 0.7898\n",
      "Epoch 3/12\n",
      "1936/1936 [==============================] - 0s 86us/sample - loss: 0.4499 - acc: 0.7939\n",
      "Epoch 4/12\n",
      "1936/1936 [==============================] - 0s 113us/sample - loss: 0.4516 - acc: 0.7965\n",
      "Epoch 5/12\n",
      "1936/1936 [==============================] - 0s 99us/sample - loss: 0.4423 - acc: 0.7944\n",
      "Epoch 6/12\n",
      "1936/1936 [==============================] - 0s 114us/sample - loss: 0.4436 - acc: 0.7918\n",
      "Epoch 7/12\n",
      "1936/1936 [==============================] - 0s 124us/sample - loss: 0.4487 - acc: 0.7949\n",
      "Epoch 8/12\n",
      "1936/1936 [==============================] - 0s 97us/sample - loss: 0.4429 - acc: 0.7939\n",
      "Epoch 9/12\n",
      "1936/1936 [==============================] - 0s 98us/sample - loss: 0.4399 - acc: 0.7970\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936/1936 [==============================] - 0s 86us/sample - loss: 0.4420 - acc: 0.8017\n",
      "Epoch 11/12\n",
      "1936/1936 [==============================] - 0s 66us/sample - loss: 0.4388 - acc: 0.7986\n",
      "Epoch 12/12\n",
      "1936/1936 [==============================] - 0s 70us/sample - loss: 0.4389 - acc: 0.7949\n",
      "Epoch 1/12\n",
      "2130/2130 [==============================] - 1s 483us/sample - loss: 0.5932 - acc: 0.7000\n",
      "Epoch 2/12\n",
      "2130/2130 [==============================] - 0s 146us/sample - loss: 0.4590 - acc: 0.7845\n",
      "Epoch 3/12\n",
      "2130/2130 [==============================] - 1s 273us/sample - loss: 0.4484 - acc: 0.7915\n",
      "Epoch 4/12\n",
      "2130/2130 [==============================] - 0s 134us/sample - loss: 0.4417 - acc: 0.7930\n",
      "Epoch 5/12\n",
      "2130/2130 [==============================] - 0s 121us/sample - loss: 0.4459 - acc: 0.7897\n",
      "Epoch 6/12\n",
      "2130/2130 [==============================] - 0s 107us/sample - loss: 0.4417 - acc: 0.7920\n",
      "Epoch 7/12\n",
      "2130/2130 [==============================] - 0s 116us/sample - loss: 0.4430 - acc: 0.7892\n",
      "Epoch 8/12\n",
      "2130/2130 [==============================] - 0s 169us/sample - loss: 0.4461 - acc: 0.7906\n",
      "Epoch 9/12\n",
      "2130/2130 [==============================] - 0s 125us/sample - loss: 0.4417 - acc: 0.7944\n",
      "Epoch 10/12\n",
      "2130/2130 [==============================] - 0s 219us/sample - loss: 0.4404 - acc: 0.7944\n",
      "Epoch 11/12\n",
      "2130/2130 [==============================] - 1s 241us/sample - loss: 0.4421 - acc: 0.7873\n",
      "Epoch 12/12\n",
      "2130/2130 [==============================] - 0s 142us/sample - loss: 0.4413 - acc: 0.7930\n",
      "Epoch 1/12\n",
      "2014/2014 [==============================] - 1s 417us/sample - loss: 0.5889 - acc: 0.7304\n",
      "Epoch 2/12\n",
      "2014/2014 [==============================] - 0s 78us/sample - loss: 0.4921 - acc: 0.7632\n",
      "Epoch 3/12\n",
      "2014/2014 [==============================] - 0s 122us/sample - loss: 0.4823 - acc: 0.7696\n",
      "Epoch 4/12\n",
      "2014/2014 [==============================] - 0s 66us/sample - loss: 0.4863 - acc: 0.7582\n",
      "Epoch 5/12\n",
      "2014/2014 [==============================] - 0s 55us/sample - loss: 0.4793 - acc: 0.7686\n",
      "Epoch 6/12\n",
      "2014/2014 [==============================] - 0s 89us/sample - loss: 0.4776 - acc: 0.7646\n",
      "Epoch 7/12\n",
      "2014/2014 [==============================] - 0s 89us/sample - loss: 0.4763 - acc: 0.7736\n",
      "Epoch 8/12\n",
      "2014/2014 [==============================] - 0s 79us/sample - loss: 0.4752 - acc: 0.7661\n",
      "Epoch 9/12\n",
      "2014/2014 [==============================] - 0s 78us/sample - loss: 0.4738 - acc: 0.7686\n",
      "Epoch 10/12\n",
      "2014/2014 [==============================] - 0s 75us/sample - loss: 0.4700 - acc: 0.7751\n",
      "Epoch 11/12\n",
      "2014/2014 [==============================] - 0s 86us/sample - loss: 0.4757 - acc: 0.7676\n",
      "Epoch 12/12\n",
      "2014/2014 [==============================] - 0s 69us/sample - loss: 0.4683 - acc: 0.7691\n",
      "Epoch 1/12\n",
      "2011/2011 [==============================] - 1s 337us/sample - loss: 0.5471 - acc: 0.7857\n",
      "Epoch 2/12\n",
      "2011/2011 [==============================] - 1s 324us/sample - loss: 0.4159 - acc: 0.8115\n",
      "Epoch 3/12\n",
      "2011/2011 [==============================] - 0s 184us/sample - loss: 0.4089 - acc: 0.8230\n",
      "Epoch 4/12\n",
      "2011/2011 [==============================] - 0s 207us/sample - loss: 0.4084 - acc: 0.8210\n",
      "Epoch 5/12\n",
      "2011/2011 [==============================] - 0s 187us/sample - loss: 0.4079 - acc: 0.8200\n",
      "Epoch 6/12\n",
      "2011/2011 [==============================] - 0s 143us/sample - loss: 0.4064 - acc: 0.8175\n",
      "Epoch 7/12\n",
      "2011/2011 [==============================] - 0s 109us/sample - loss: 0.4083 - acc: 0.8200\n",
      "Epoch 8/12\n",
      "2011/2011 [==============================] - ETA: 0s - loss: 0.4044 - acc: 0.822 - 0s 154us/sample - loss: 0.4071 - acc: 0.8210\n",
      "Epoch 9/12\n",
      "2011/2011 [==============================] - 0s 105us/sample - loss: 0.4000 - acc: 0.8250\n",
      "Epoch 10/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.4008 - acc: 0.8210\n",
      "Epoch 11/12\n",
      "2011/2011 [==============================] - 0s 107us/sample - loss: 0.4022 - acc: 0.8235\n",
      "Epoch 12/12\n",
      "2011/2011 [==============================] - 0s 91us/sample - loss: 0.4046 - acc: 0.8225\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_model.set_weights(state.model.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.567003367003367, 0.566622691292876, 0.5532194480946123, 0.4868073878627968, 0.542225201072386, 0.5072751322751323, 0.5129427792915532, 0.5383064516129032, 0.5513333333333333, 0.5749500333111259]\n",
      "[0.7007874015748031, 0.7162673392181589, 0.7325581395348837, 0.7803030303030303, 0.7536, 0.7272727272727273, 0.7209302325581395, 0.7194950911640954, 0.7036114570361146, 0.760806916426513]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Test the success of the attack.\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "mia_accuracy = []\n",
    "mia_precision = []\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(np.argmax(attacker_y_test, axis=1)) if d == c]\n",
    "    data_in = [X[target_indices], y[target_indices]]\n",
    "    data_out = [[attacker_X_test[test_indices]], attacker_y_test[test_indices]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    mia_accuracy.append(attack_accuracy)\n",
    "    mia_precision.append(precision_score(real_membership_labels, attack_guesses))\n",
    "print(mia_accuracy)\n",
    "print(mia_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/MIA/rE2C2.txt', 'w') as log:\n",
    "        print(\"rE2C2acc = {}\".format(mia_accuracy), file=log)\n",
    "        print(\"rE2C2pre = {}\".format(mia_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167537343955065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7296367659521652"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7021517553793885, 0.7156398104265402, 0.7280701754385965, 0.7718832891246684, 0.741307371349096, 0.7302504816955684, 0.7214885954381752, 0.7171428571428572, 0.7144535840188014, 0.7539797395079595])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<conv2d/kernel=[[[[-5.42145409e-02 -9.29415375e-02  9.59848799e-03 -1.24651883\n",
    "    -4.74597374e-03  6.39463514e-02  7.72277191e-02 -6.19306117e-02\n",
    "     1.12236537e-01 -9.36510563e-02  9.26551670e-02  8.08892250e-02\n",
    "    -1.36332000e-02 -7.95079619e-02 -6.24745712e-02  1.12114482e-01\n",
    "     7.13181347e-02 -5.21638282e-02  1.27013907e-01 -1.14578225e-01\n",
    "     6.35279119e-02  1.11849017e-01 -7.88050294e-02  2.95140538e-02\n",
    "    -2.79310048e-02 -1.47477508e-01 -8.70073959e-02  5.86430654e-02\n",
    "     3.71561968e-04  7.01781064e-02  1.18049368e-01  8.97085816e-02]\n",
    "   [ 1.30100861e-01 -3.27671580e-02  1.19756632e-01 -1.42253444e-01\n",
    "    -1.02145381e-01  8.69416911e-03  3.48152928e-02  1.01850023e-02\n",
    "    -9.14086178e-02 -3.09834778e-02 -6.95866793e-02 -2.65930369e-02\n",
    "     2.75183469e-03 -9.73303542e-02  1.05238795e-01 -5.39357290e-02\n",
    "    -8.16846713e-02  7.36190155e-02 -4.24497686e-02  6.97964653e-02\n",
    "     3.57606560e-02 -1.14507638e-01 -9.91197024e-03  3.90822105e-02\n",
    "    -1.09545747e-02 -2.22205427e-02  4.85142171e-02 -2.42610071e-02\n",
    "    -9.69229490e-02 -3.09411883e-02 -9.12560746e-02 -8.95848572e-02]\n",
    "   [ 8.69994611e-03 -6.94726855e-02 -1.32288700e-02 -5.20821698e-02\n",
    "     9.10899714e-02 -1.21620931e-01  7.07676709e-02 -9.30615235e-04\n",
    "     7.55560547e-02  8.09496716e-02  3.51783223e-02 -3.14740092e-02\n",
    "    7.05207363e-02 -4.19138856e-02 -7.14744478e-02  2.24497169e-02\n",
    "     4.06181552e-02 -1.40542030e-01 -1.37560964e-01  1.03260078e-01\n",
    "     5.35405315e-02  2.68804077e-02 -5.61432987e-02  9.36878845e-02\n",
    "    -1.06399521e-01 -5.91180474e-02 -1.88456737e-02 -3.25000919e-02\n",
    "     8.84265006e-02  1.40605614e-01 -1.19070061e-01 -3.89880836e-02]]\n",
    "\n",
    "  [[ 9.04263332e-02  7.62202963e-02  5.33219166e-02 -6.59058453e-04\n",
    "    -4.14211676e-02 -6.37268350e-02 -1.00701481e-01 -5.83394282e-02\n",
    "    -2.31314227e-02  2.32471675e-02 -1.01180360e-01  7.85506517e-02\n",
    "    -9.31655094e-02  1.10249102e-01  7.13690519e-02  8.32763091e-02\n",
    "    -9.46618468e-02  2.16780808e-02 -4.85283621e-02  2.55225040e-02\n",
    "     3.09757367e-02 -8.94018561e-02 -1.51236150e-02  9.61311162e-02\n",
    "    -7.54762068e-02 -5.42784818e-02 -2.63635311e-02  1.56526789e-02\n",
    "    -4.46861126e-02 -1.02023780e-01  1.16602696e-01 -4.11380455e-02]\n",
    "   [ 3.15265842e-02  1.18916936e-01 -7.81858712e-02 -8.60707834e-02\n",
    "    -9.52330232e-02 -5.93867563e-02  1.63705945e-02  1.08087242e-01\n",
    "     2.07749587e-02  3.70924966e-03  1.82045344e-02 -7.39347115e-02\n",
    "    -3.37157436e-02  1.39418036e-01 -3.82396206e-02 -5.40040396e-02\n",
    "     2.98683136e-03  1.03068948e-01 -1.04710817e-01 -1.93764158e-02\n",
    "    -1.32776611e-02 -3.73043008e-02 -6.90225735e-02  3.32656354e-02\n",
    "    -2.53778649e-03 -5.25377169e-02 -8.16403404e-02 -5.89925870e-02\n",
    "     1.53077155e-01  3.53813022e-02 -5.37460223e-02  7.87424296e-02]\n",
    "   [ 1.45501345e-02 -3.77694666e-02 -7.15423422e-03 -1.03167430e-01\n",
    "    -1.09158896e-01 -3.68729122e-02  1.14286393e-01 -1.36856660e-01\n",
    "    -4.26050508e-03  9.36951563e-02 -1.01519845e-01  1.52138695e-01\n",
    "     8.83981436e-02  5.20503595e-02 -1.18898429e-01  1.15645960e-01\n",
    "    -1.18085600e-01 -1.52510509e-01 -7.79650360e-02 -5.74325072e-03\n",
    "     4.25417349e-02 -4.51641008e-02 -4.90775257e-02 -3.62045281e-02\n",
    "    -5.01497015e-02 -1.51180755e-02  1.27451643e-01  3.49950306e-02\n",
    "    -2.29743831e-02  5.81850111e-03 -8.78469869e-02 -1.35514066e-01]]\n",
    "                 [[ 2.43675094e-02 -1.78505890e-02  1.10490210e-01 -1.15350552e-01\n",
    "    -7.09867850e-02  8.19782764e-02  9.13404822e-02  6.22468181e-02\n",
    "    -1.27217427e-01  5.40355332e-02  2.51856614e-02 -1.03047401e-01\n",
    "    -1.04841396e-01 -1.02888949e-01  1.77751761e-02  2.37558000e-02\n",
    "    -6.10768311e-02  1.27097949e-01 -9.89521816e-02 -1.05360396e-01\n",
    "     8.35644305e-02 -2.36028749e-02  7.65823424e-02 -4.81431447e-02\n",
    "     3.48126516e-02 -1.00858532e-01 -6.14999942e-02 -3.01653203e-02\n",
    "     3.02533917e-02  3.82566974e-02  2.90715750e-02 -1.15484178e-01]\n",
    "   [-1.17214173e-01 -1.11008277e-02 -4.84149754e-02 -1.04862235e-01"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
