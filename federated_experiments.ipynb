{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqrD7Yzlmlsk"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2k8X1C1nmpKv"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32xflLc4NTx-"
   },
   "source": [
    "# Custom Federated Algorithms, Part 2: Implementing Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtATV6DlqPs0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/federated/tutorials/custom_federated_algorithms_2\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/federated/blob/v0.4.0/docs/tutorials/custom_federated_algorithms_2.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_igJ2sfaNWS8"
   },
   "source": [
    "This tutorial is the second part of a two-part series that demonstrates how to\n",
    "implement custom types of federated algorithms in TFF using the\n",
    "[Federated Core (FC)](../federated_core.md), which serves as a foundation for\n",
    "the [Federated Learning (FL)](../federated_learning.md) layer (`tff.learning`).\n",
    "\n",
    "We encourage you to first read the\n",
    "[first part of this series](custom_federated_algorithms_1.ipynb), which\n",
    "introduce some of the key concepts and programming abstractions used here.\n",
    "\n",
    "This second part of the series uses the mechanisms introduced in the first part\n",
    "to implement a simple version of federated training and evaluation algorithms.\n",
    "\n",
    "We encourage you to review the\n",
    "[image classification](federated_learning_for_image_classification.ipynb) and\n",
    "[text generation](federated_learning_for_text_generation.ipynb) tutorials for a\n",
    "higher-level and more gentle introduction to TFF's Federated Learning APIs, as\n",
    "they will help you put the concepts we describe here in context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuJuLEh2TfZG"
   },
   "source": [
    "## Before we start\n",
    "\n",
    "Before we start, try to run the following \"Hello World\" example to make sure\n",
    "your environment is correctly setup. If it doesn't work, please refer to the\n",
    "[Installation](../install.md) guide for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB1ovcX1mBxQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_federated in /anaconda3/lib/python3.7/site-packages (0.4.0)\n",
      "Requirement already satisfied: enum34~=1.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.1.6)\n",
      "Requirement already satisfied: numpy~=1.14 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.16.4)\n",
      "Requirement already satisfied: h5py~=2.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (2.8.0)\n",
      "Requirement already satisfied: six~=1.10 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: tensorflow~=1.13 in /anaconda3/lib/python3.7/site-packages (from tensorflow_federated) (1.13.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.9)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.19.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.13.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.7.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.31.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow~=1.13->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow~=1.13->tensorflow_federated) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow~=1.13->tensorflow_federated) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "# NOTE: If you are running a Jupyter notebook, and installing a locally built\n",
    "# pip package, you may need to edit the following to point to the '.whl' file\n",
    "# on your local filesystem.\n",
    "\n",
    "#!pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-skNC6aovM46"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data\n",
    "from tensorflow_federated import python as tff\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1550886524193,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "zzXwGnZamIMM",
    "outputId": "9febf2e4-6cb9-44c5-b665-629acef0f2f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World!'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tff.federated_computation\n",
    "def hello_world():\n",
    "  return 'Hello, World!'\n",
    "\n",
    "\n",
    "hello_world()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iu5Gd8D6W33s"
   },
   "source": [
    "## Implementing Federated Averaging\n",
    "\n",
    "As in\n",
    "[Federated Learning for Image Classification](federated_learning_for_image_classification.md),\n",
    "we are going to use the MNIST example, but since this is intended as a low-level\n",
    "tutorial, we are going to bypass the Keras API and `tff.simulation`, write raw\n",
    "model code, and construct a federated data set from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6qCjef350c_"
   },
   "source": [
    "\n",
    "### Preparing federated data sets\n",
    "\n",
    "For the sake of a demonstration, we're going to simulate a scenario in which we\n",
    "have data from 10 users, and each of the users contributes knowledge how to\n",
    "recognize a different digit. This is about as\n",
    "non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "as it gets.\n",
    "\n",
    "First, let's load the standard MNIST data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uThZM4Ds-KDQ"
   },
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "cifar_train, cifar_test = tf.keras.datasets.cifar10.load_data()\n",
    "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
    "cifar_class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cifar_test = (cifar_test[0], tf.keras.utils.to_categorical(cifar_test[1]))\n",
    "# cifar_train = (cifar_train[0], tf.keras.utils.to_categorical(cifar_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "colab": {
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1550886524725,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "PkJc5rHA2no_",
    "outputId": "baa6de95-5e62-4f4f-a5ad-5a82358a2d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 32, 32, 3), (50000, 1)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10000, 32, 32, 3), (10000, 1)]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in cifar_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFET4BKJFbkP"
   },
   "source": [
    "The data comes as Numpy arrays, one with images and another with digit labels, both\n",
    "with the first dimension going over the individual examples. Let's write a\n",
    "helper function that formats it in a way compatible with how we feed federated\n",
    "sequences into TFF computations, i.e., as a list of lists - the outer list\n",
    "ranging over the users (digits), the inner ones ranging over batches of data in\n",
    "each client's sequence. As is customary, we will structure each batch as a pair\n",
    "of tensors named `x` and `y`, each with the leading batch dimension. While at\n",
    "it, we'll also flatten each image into a 784-element vector and rescale the\n",
    "pixels in it into the `0..1` range, so that we don't have to clutter the model\n",
    "logic with data conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTaTLiq5GNqy"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES_PER_USER = 2000\n",
    "BATCH_SIZE = 32\n",
    "USERS = 5\n",
    "NUM_EPOCHS = 1\n",
    "CLASSES = 10\n",
    "\n",
    "\n",
    "def get_indices_unbalanced_completely(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    class_shares = CLASSES // min(CLASSES, USERS)\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append(\n",
    "            np.array(\n",
    "                [indices_array.pop(0)[:NUM_EXAMPLES_PER_USER//class_shares] for j in range(class_shares)])\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_unbalanced(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    # each user will have 2 classes excluded from their data sets, thus 250 examples * remaining 8 classes\n",
    "    class_shares = 250\n",
    "    # store indices for future use\n",
    "    user_indices = []\n",
    "    # auxilary index array to pop out pairs of classes missing at each user\n",
    "    class_index = list(range(CLASSES))\n",
    "    for u in range(USERS):\n",
    "        columns_out = [class_index.pop(0) for i in range(2)]\n",
    "        selected_columns = set(range(CLASSES)) - set(columns_out)\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(\n",
    "            np.array(indices_array)[list(selected_columns)].T[starting_index:starting_index + class_shares]\n",
    "            .flatten())\n",
    "    return user_indices\n",
    "\n",
    "def get_indices_realistic(y, u):\n",
    "    # split dataset into arrays of each class label\n",
    "    all_indices = [i for i, d in enumerate(y)]\n",
    "    shares_arr = [5000, 3000, 1000, 750, 250]\n",
    "    user_indices = []\n",
    "    for u in range(USERS):\n",
    "        user_indices.append([all_indices.pop(0) for i in range(shares_arr[u])]) \n",
    "    return user_indices\n",
    "\n",
    "def get_indices_even(y):\n",
    "    # split dataset into arrays of each class label\n",
    "    indices_array = []\n",
    "    for c in range(CLASSES):\n",
    "        indices_array.append([i for i, d in enumerate(y) if d == c])\n",
    "    user_indices = []\n",
    "    class_shares = NUM_EXAMPLES_PER_USER // CLASSES\n",
    "    \n",
    "    # take even shares of each class for every user\n",
    "    for u in range(USERS):\n",
    "        starting_index = u*class_shares\n",
    "        user_indices.append(np.array(indices_array).T[starting_index:starting_index + class_shares].flatten())   \n",
    "    return user_indices\n",
    "\n",
    "def get_non_distributed(source):\n",
    "    #indices = np.concatenate(get_indices_even(source[1]))\n",
    "    y = np.array(source[1][:10000], dtype=np.int32)\n",
    "    X = np.array(source[0][:10000], dtype=np.float32) / 255.0\n",
    "    return X, y\n",
    "    \n",
    "def get_distributed(source, u, distribution):\n",
    "    if distribution == 'i':\n",
    "        indices = get_indices_even(source[1])[u]\n",
    "    elif distribution == 'n':\n",
    "        indices = get_indices_unbalanced(source[1])[u]\n",
    "    elif distribution == 'r':\n",
    "        indices = get_indices_realistic(source[1][:10000], u)[u]\n",
    "    else:\n",
    "        indices = np.array(get_indices_unbalanced_completely(source[1])[u])\n",
    "    \n",
    "    output_sequence = []\n",
    "    for repeat in range(NUM_EPOCHS):\n",
    "        for i in range(0, len(indices), BATCH_SIZE):\n",
    "            batch_samples = indices[i:i + BATCH_SIZE]\n",
    "            output_sequence.append({\n",
    "                'x': np.array([source[0][b] / 255.0 for b in batch_samples], dtype=np.float32),\n",
    "                'y': np.array([source[1][b] for b in batch_samples], dtype=np.int32)})\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "federated_train_data = [get_distributed(cifar_train, u, 'i') for u in range(USERS)]\n",
    "federated_test_data = [get_distributed(cifar_test, u, 'i') for u in range(USERS)]\n",
    "\n",
    "(X, y) = get_non_distributed(cifar_train)\n",
    "(X_test, y_test) = get_non_distributed(cifar_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(federated_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 32, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batch = federated_train_data[1][-2]\n",
    "sample_batch['x'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpNdBimWaMHD"
   },
   "source": [
    "As a quick sanity check, let's look at the `Y` tensor in the last batch of data\n",
    "contributed by the fifth client (the one corresponding to the digit `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xgvcwv7Obhat"
   },
   "source": [
    "Just to be sure, let's also look at the image corresponding to the last element of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "height": 275
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1550886527273,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "cI4aat1za525",
    "outputId": "e516287a-fbee-49c9-f861-4691e5caf283"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#print(cifar_class_labels[federated_train_data[4][5]['y'][-5][0]])\n",
    "plt.imshow(federated_train_data[4][5]['x'][-5].reshape(32, 32, 3))\n",
    "#plt.imshow(cifar_train[0][4])\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "\n",
    "from tensorflow.python.keras.optimizer_v2 import gradient_descent\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_compiled_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS)),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(128, activation=\"tanh\"),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "    \n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, y_pred))\n",
    "    \n",
    "    model.compile(\n",
    "      loss=loss_fn,\n",
    "      optimizer=\"adam\",\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-federated keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model = create_compiled_keras_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 23s 2ms/sample - loss: 1.6453 - categorical_accuracy: 0.4105 - val_loss: 1.4564 - val_categorical_accuracy: 0.4887\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.2948 - categorical_accuracy: 0.5413 - val_loss: 1.3070 - val_categorical_accuracy: 0.5377\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.0989 - categorical_accuracy: 0.6127 - val_loss: 1.2251 - val_categorical_accuracy: 0.5706\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.9609 - categorical_accuracy: 0.6592 - val_loss: 1.2384 - val_categorical_accuracy: 0.5791\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.8286 - categorical_accuracy: 0.7077 - val_loss: 1.2404 - val_categorical_accuracy: 0.5842\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.6916 - categorical_accuracy: 0.7620 - val_loss: 1.2497 - val_categorical_accuracy: 0.5907\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.5374 - categorical_accuracy: 0.8224 - val_loss: 1.3085 - val_categorical_accuracy: 0.5914\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4065 - categorical_accuracy: 0.8718 - val_loss: 1.3898 - val_categorical_accuracy: 0.5857\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 0.2734 - categorical_accuracy: 0.9245 - val_loss: 1.4569 - val_categorical_accuracy: 0.5969\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.1703 - categorical_accuracy: 0.9636 - val_loss: 1.5415 - val_categorical_accuracy: 0.5924\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.1021 - categorical_accuracy: 0.9831 - val_loss: 1.6110 - val_categorical_accuracy: 0.5882\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.0585 - categorical_accuracy: 0.9945 - val_loss: 1.6453 - val_categorical_accuracy: 0.5936\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0314 - categorical_accuracy: 0.9982 - val_loss: 1.7216 - val_categorical_accuracy: 0.5952\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0179 - categorical_accuracy: 0.9996 - val_loss: 1.7587 - val_categorical_accuracy: 0.6038\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0107 - categorical_accuracy: 0.9999 - val_loss: 1.8129 - val_categorical_accuracy: 0.6051\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0081 - categorical_accuracy: 1.0000 - val_loss: 1.8519 - val_categorical_accuracy: 0.6043\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0058 - categorical_accuracy: 1.0000 - val_loss: 1.9056 - val_categorical_accuracy: 0.6017\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 0.0044 - categorical_accuracy: 1.0000 - val_loss: 1.9266 - val_categorical_accuracy: 0.6014\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 0.0046 - categorical_accuracy: 0.9999 - val_loss: 1.9657 - val_categorical_accuracy: 0.6035\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.0029 - categorical_accuracy: 1.0000 - val_loss: 1.9959 - val_categorical_accuracy: 0.6053\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.0022 - categorical_accuracy: 1.0000 - val_loss: 2.0312 - val_categorical_accuracy: 0.6022\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0018 - categorical_accuracy: 1.0000 - val_loss: 2.0627 - val_categorical_accuracy: 0.6027\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 2.0921 - val_categorical_accuracy: 0.6026\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 2.1258 - val_categorical_accuracy: 0.6026\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 9.9374e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1454 - val_categorical_accuracy: 0.6023\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 8.2107e-04 - categorical_accuracy: 1.0000 - val_loss: 2.1764 - val_categorical_accuracy: 0.6021\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 6.8230e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2064 - val_categorical_accuracy: 0.6013\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 5.7202e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2441 - val_categorical_accuracy: 0.6027\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7293e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2656 - val_categorical_accuracy: 0.6025\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 17s 2ms/sample - loss: 4.0281e-04 - categorical_accuracy: 1.0000 - val_loss: 2.2940 - val_categorical_accuracy: 0.6022\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.2552e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3168 - val_categorical_accuracy: 0.6021\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 2.7323e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3425 - val_categorical_accuracy: 0.6016\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 2.2841e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3709 - val_categorical_accuracy: 0.6003\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.9136e-04 - categorical_accuracy: 1.0000 - val_loss: 2.3944 - val_categorical_accuracy: 0.6014\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.6010e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4202 - val_categorical_accuracy: 0.6012\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.3392e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4532 - val_categorical_accuracy: 0.6016\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1291e-04 - categorical_accuracy: 1.0000 - val_loss: 2.4735 - val_categorical_accuracy: 0.6004\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 9.5869e-05 - categorical_accuracy: 1.0000 - val_loss: 2.4977 - val_categorical_accuracy: 0.6009\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 7.9403e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5172 - val_categorical_accuracy: 0.6002\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 6.7607e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5467 - val_categorical_accuracy: 0.6008\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 5.8769e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5714 - val_categorical_accuracy: 0.6007\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 4.7460e-05 - categorical_accuracy: 1.0000 - val_loss: 2.5931 - val_categorical_accuracy: 0.6008\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 15s 1ms/sample - loss: 3.9733e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6141 - val_categorical_accuracy: 0.5998\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 3.3674e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6374 - val_categorical_accuracy: 0.6006\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 2.8456e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6608 - val_categorical_accuracy: 0.6018\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 2.3901e-05 - categorical_accuracy: 1.0000 - val_loss: 2.6810 - val_categorical_accuracy: 0.6009\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 15s 2ms/sample - loss: 2.0279e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7009 - val_categorical_accuracy: 0.6002\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 1.7068e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7247 - val_categorical_accuracy: 0.5999\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 1.4409e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7476 - val_categorical_accuracy: 0.5995\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.2274e-05 - categorical_accuracy: 1.0000 - val_loss: 2.7663 - val_categorical_accuracy: 0.5997\n"
     ]
    }
   ],
   "source": [
    "history_callback = non_federated_model.fit(X, y, validation_data=(X_test, y_test), batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6563694650650025,\n",
       " 1.3007290199279786,\n",
       " 1.1194022260665895,\n",
       " 0.9801722335815429,\n",
       " 0.8453258259773254,\n",
       " 0.7207920631408692,\n",
       " 0.569933446264267,\n",
       " 0.42374285163879394,\n",
       " 0.30032064225673677,\n",
       " 0.19598041729927063,\n",
       " 0.11359974697828293,\n",
       " 0.0627660088956356]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = history_callback.history['loss']\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_federated_model.save(\"non_federated_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/'+ datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")+'.txt', 'w') as log:\n",
    "        print(\"Cifar10, Non-federated, IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"NFTrain = {}\".format(history_callback.history[\"loss\"]), file=log)\n",
    "        print(\"NFTest = {}\".format(history_callback.history[\"val_loss\"]), file=log)\n",
    "        print(\"NFAccuracy = {}\".format(history_callback.history[\"val_categorical_accuracy\"]), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    keras_model = create_compiled_keras_model()\n",
    "    return tff.learning.from_compiled_keras_model(keras_model, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "iterative_process = tff.learning.build_federated_averaging_process(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 442us/sample - loss: 1.6801 - sparse_categorical_accuracy: 0.4095\n",
      "round  1, metrics=<sparse_categorical_accuracy=0.3275,loss=1.8733339>\n",
      "test accuracy: 0.40950000286102295\n"
     ]
    }
   ],
   "source": [
    "# One round / one user test\n",
    "state = iterative_process.initialize()\n",
    "state, metrics = iterative_process.next(state, federated_train_data[0:1])\n",
    "# test_metrics = evaluation(state.model, federated_test_data)\n",
    "non_federated_model.set_weights(state.model.trainable)\n",
    "(loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "print('round  1, metrics={}'.format(metrics))\n",
    "print('test accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "epochs = 12\n",
    "state = iterative_process.initialize()\n",
    "fd_test_loss = []\n",
    "fd_train_loss = []\n",
    "fd_train_accuracy = []\n",
    "for round_num in range(12):\n",
    "    selected = np.random.choice(5, 5, replace=False)\n",
    "    state, metrics = iterative_process.next(state, list(np.array(federated_train_data)[selected]))\n",
    "    non_federated_model.set_weights(state.model.trainable)\n",
    "    (loss, accuracy) = non_federated_model.evaluate(X_test, y_test)\n",
    "    fd_train_loss.append(metrics[1])\n",
    "    fd_test_accuracy.append(accuracy)\n",
    "    fd_test_loss.append(loss)\n",
    "    print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4527,\n",
       " 0.4398,\n",
       " 0.4791,\n",
       " 0.5053,\n",
       " 0.5295,\n",
       " 0.5456,\n",
       " 0.5595,\n",
       " 0.5747,\n",
       " 0.5807,\n",
       " 0.5853,\n",
       " 0.5906,\n",
       " 0.5918,\n",
       " 0.5935]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd_test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 504us/sample - loss: 2.0773 - sparse_categorical_accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0773194374084474, 0.6001]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 360us/sample - loss: 1.2888 - sparse_categorical_accuracy: 0.5730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2887664936065675, 0.573]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_federated_model.set_weights(tff.learning.keras_weights_from_tff_weights(state.model))\n",
    "non_federated_model.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = np.random.choice(5, 4, replace=False)\n",
    "len(list(np.array(federated_train_data)[selected]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/Exp5/niiE2C2.txt', 'w') as log:\n",
    "        print(\"Cifar10, Federated E2C2, non-IDD, minibatch_size: 32\", file=log)\n",
    "        print(\"Train Loss: {}\".format(fd_train_loss), file=log)\n",
    "        print(\"Test Accuracy: {}\".format(fd_test_accuracy), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership Inference Attach (MIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1190 - acc: 0.2440 - val_loss: 1.9590 - val_acc: 0.2846\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7290 - acc: 0.3980 - val_loss: 1.8585 - val_acc: 0.3410\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5974 - acc: 0.4520 - val_loss: 1.7614 - val_acc: 0.3788\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4253 - acc: 0.5290 - val_loss: 1.6777 - val_acc: 0.3944\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3012 - acc: 0.5550 - val_loss: 1.7050 - val_acc: 0.3966\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1126 - acc: 0.6500 - val_loss: 1.7451 - val_acc: 0.3902\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0079 - acc: 0.6770 - val_loss: 1.7554 - val_acc: 0.4156\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8570 - acc: 0.7340 - val_loss: 1.7414 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6845 - acc: 0.7880 - val_loss: 1.7564 - val_acc: 0.4180\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5715 - acc: 0.8260 - val_loss: 1.8183 - val_acc: 0.4128\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4442 - acc: 0.8840 - val_loss: 1.8667 - val_acc: 0.4148\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3486 - acc: 0.9150 - val_loss: 1.9364 - val_acc: 0.4170\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1904 - acc: 0.2270 - val_loss: 1.8969 - val_acc: 0.3414\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7393 - acc: 0.3940 - val_loss: 1.7504 - val_acc: 0.3786\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5385 - acc: 0.4590 - val_loss: 1.7045 - val_acc: 0.3994\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4037 - acc: 0.5270 - val_loss: 1.7093 - val_acc: 0.3978\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2525 - acc: 0.5940 - val_loss: 1.6606 - val_acc: 0.4252\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1121 - acc: 0.6220 - val_loss: 1.7862 - val_acc: 0.3834\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9881 - acc: 0.6790 - val_loss: 1.7424 - val_acc: 0.4152\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8295 - acc: 0.7400 - val_loss: 1.7525 - val_acc: 0.4222\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6734 - acc: 0.7910 - val_loss: 1.8202 - val_acc: 0.4188\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5470 - acc: 0.8420 - val_loss: 1.9658 - val_acc: 0.4050\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4330 - acc: 0.8860 - val_loss: 1.9533 - val_acc: 0.4152\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3282 - acc: 0.9350 - val_loss: 2.2226 - val_acc: 0.3824\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1216 - acc: 0.2290 - val_loss: 1.8406 - val_acc: 0.3444\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.7141 - acc: 0.3630 - val_loss: 1.7927 - val_acc: 0.3686\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.5256 - acc: 0.4630 - val_loss: 1.7070 - val_acc: 0.3992\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3413 - acc: 0.5300 - val_loss: 1.7106 - val_acc: 0.3958\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2029 - acc: 0.5840 - val_loss: 1.7923 - val_acc: 0.3894\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0358 - acc: 0.6670 - val_loss: 1.7024 - val_acc: 0.4154\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 0.8298 - acc: 0.7350 - val_loss: 1.7616 - val_acc: 0.4126\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.6970 - acc: 0.7840 - val_loss: 1.8506 - val_acc: 0.4070\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5977 - acc: 0.8230 - val_loss: 1.8947 - val_acc: 0.4082\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4659 - acc: 0.8740 - val_loss: 1.9994 - val_acc: 0.4114\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3580 - acc: 0.9080 - val_loss: 1.9893 - val_acc: 0.4254\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.2505 - acc: 0.9530 - val_loss: 2.0713 - val_acc: 0.4284\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1673 - acc: 0.2270 - val_loss: 1.9708 - val_acc: 0.2932\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7302 - acc: 0.4100 - val_loss: 1.8838 - val_acc: 0.3152\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5661 - acc: 0.4640 - val_loss: 1.7030 - val_acc: 0.3990\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.4182 - acc: 0.5130 - val_loss: 1.6961 - val_acc: 0.4008\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.2230 - acc: 0.5880 - val_loss: 1.7060 - val_acc: 0.4108\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0768 - acc: 0.6410 - val_loss: 1.7573 - val_acc: 0.3950\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9539 - acc: 0.6810 - val_loss: 1.7299 - val_acc: 0.4208\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.7782 - acc: 0.7540 - val_loss: 1.8079 - val_acc: 0.4126\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6448 - acc: 0.8150 - val_loss: 1.8731 - val_acc: 0.4050\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4922 - acc: 0.8830 - val_loss: 1.8941 - val_acc: 0.4170\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3899 - acc: 0.9130 - val_loss: 1.9983 - val_acc: 0.4066\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.2902 - acc: 0.9440 - val_loss: 2.1477 - val_acc: 0.3990\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.1843 - acc: 0.2000 - val_loss: 1.8460 - val_acc: 0.3418\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7032 - acc: 0.4020 - val_loss: 1.7764 - val_acc: 0.3624\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5247 - acc: 0.4560 - val_loss: 1.6656 - val_acc: 0.4194\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3137 - acc: 0.5440 - val_loss: 1.6540 - val_acc: 0.4248\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1637 - acc: 0.5950 - val_loss: 1.7113 - val_acc: 0.4162\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9898 - acc: 0.6770 - val_loss: 1.7052 - val_acc: 0.4208\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8052 - acc: 0.7650 - val_loss: 1.7580 - val_acc: 0.4252\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7001 - acc: 0.7750 - val_loss: 1.8109 - val_acc: 0.4266\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5303 - acc: 0.8550 - val_loss: 2.1200 - val_acc: 0.3770\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4638 - acc: 0.8610 - val_loss: 2.0237 - val_acc: 0.4152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3019 - acc: 0.9360 - val_loss: 2.0376 - val_acc: 0.4356\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2049 - acc: 0.9710 - val_loss: 2.1072 - val_acc: 0.4270\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.1833 - acc: 0.2120 - val_loss: 1.8398 - val_acc: 0.3330\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7961 - acc: 0.3650 - val_loss: 1.7796 - val_acc: 0.3564\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5951 - acc: 0.4550 - val_loss: 1.7328 - val_acc: 0.3836\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4091 - acc: 0.5300 - val_loss: 1.6611 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2312 - acc: 0.5690 - val_loss: 1.6687 - val_acc: 0.4140\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.1084 - acc: 0.6350 - val_loss: 1.7513 - val_acc: 0.3966\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9635 - acc: 0.6980 - val_loss: 1.8730 - val_acc: 0.3842\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8351 - acc: 0.7250 - val_loss: 1.7396 - val_acc: 0.4218\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6489 - acc: 0.8040 - val_loss: 1.8236 - val_acc: 0.4134\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.5207 - acc: 0.8610 - val_loss: 1.8635 - val_acc: 0.4214\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.3937 - acc: 0.9020 - val_loss: 1.9437 - val_acc: 0.4190\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2924 - acc: 0.9390 - val_loss: 2.0512 - val_acc: 0.4084\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2154 - acc: 0.2210 - val_loss: 1.8781 - val_acc: 0.3338\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7271 - acc: 0.3790 - val_loss: 1.7542 - val_acc: 0.3642\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5133 - acc: 0.4900 - val_loss: 1.6979 - val_acc: 0.4008\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3777 - acc: 0.5350 - val_loss: 1.6525 - val_acc: 0.4064\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2020 - acc: 0.5940 - val_loss: 1.6840 - val_acc: 0.4078\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0491 - acc: 0.6510 - val_loss: 1.7220 - val_acc: 0.4080\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8811 - acc: 0.7210 - val_loss: 1.7670 - val_acc: 0.4066\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7533 - acc: 0.7690 - val_loss: 1.7549 - val_acc: 0.4316\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5863 - acc: 0.8300 - val_loss: 1.7999 - val_acc: 0.4266\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4708 - acc: 0.8730 - val_loss: 1.8575 - val_acc: 0.4328\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3447 - acc: 0.9260 - val_loss: 2.0079 - val_acc: 0.4236\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2577 - acc: 0.9590 - val_loss: 2.0882 - val_acc: 0.4254\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 2.2528 - acc: 0.2120 - val_loss: 1.9859 - val_acc: 0.2762\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7971 - acc: 0.3790 - val_loss: 1.8326 - val_acc: 0.3454\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6003 - acc: 0.4540 - val_loss: 1.7923 - val_acc: 0.3544\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4322 - acc: 0.5120 - val_loss: 1.8342 - val_acc: 0.3598\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3258 - acc: 0.5510 - val_loss: 1.8195 - val_acc: 0.3658\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1475 - acc: 0.6180 - val_loss: 1.7666 - val_acc: 0.3860\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9750 - acc: 0.6900 - val_loss: 1.7335 - val_acc: 0.4164\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8189 - acc: 0.7460 - val_loss: 1.7597 - val_acc: 0.4226\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6622 - acc: 0.8130 - val_loss: 1.8394 - val_acc: 0.4212\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5298 - acc: 0.8460 - val_loss: 1.8898 - val_acc: 0.4306\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4155 - acc: 0.8960 - val_loss: 1.9948 - val_acc: 0.4172\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3297 - acc: 0.9180 - val_loss: 2.0476 - val_acc: 0.4172\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.3169 - acc: 0.1940 - val_loss: 1.9121 - val_acc: 0.3182\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7722 - acc: 0.3770 - val_loss: 1.8246 - val_acc: 0.3472\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6169 - acc: 0.4280 - val_loss: 1.7267 - val_acc: 0.3822\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4300 - acc: 0.5150 - val_loss: 1.7907 - val_acc: 0.3678\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3245 - acc: 0.5590 - val_loss: 1.7715 - val_acc: 0.3790\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1911 - acc: 0.5910 - val_loss: 1.7130 - val_acc: 0.3974\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0504 - acc: 0.6500 - val_loss: 1.7685 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8932 - acc: 0.7200 - val_loss: 1.7532 - val_acc: 0.4208\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7473 - acc: 0.7800 - val_loss: 1.8472 - val_acc: 0.4020\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6315 - acc: 0.8110 - val_loss: 1.8814 - val_acc: 0.4142\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5268 - acc: 0.8560 - val_loss: 1.9072 - val_acc: 0.4192\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4145 - acc: 0.9020 - val_loss: 1.9429 - val_acc: 0.4086\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 2.2005 - acc: 0.2420 - val_loss: 1.9162 - val_acc: 0.3120\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7812 - acc: 0.3770 - val_loss: 1.7474 - val_acc: 0.3860\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6426 - acc: 0.4410 - val_loss: 1.7490 - val_acc: 0.3772\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4347 - acc: 0.5160 - val_loss: 1.7361 - val_acc: 0.3962\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2730 - acc: 0.5850 - val_loss: 1.6644 - val_acc: 0.4104\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0919 - acc: 0.6420 - val_loss: 1.7139 - val_acc: 0.4158\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9022 - acc: 0.7270 - val_loss: 1.7802 - val_acc: 0.3982\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7752 - acc: 0.7680 - val_loss: 1.7667 - val_acc: 0.4264\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5892 - acc: 0.8260 - val_loss: 1.8790 - val_acc: 0.4088\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4256 - acc: 0.8900 - val_loss: 1.9095 - val_acc: 0.4212\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3454 - acc: 0.9270 - val_loss: 2.0838 - val_acc: 0.4060\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2237 - acc: 0.9700 - val_loss: 2.1110 - val_acc: 0.4256\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# Using cifar10 test set to train shadow models\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Epoch 1/12\n",
      "1930/1930 [==============================] - 2s 973us/sample - loss: 0.5913 - acc: 0.7249\n",
      "Epoch 2/12\n",
      "1930/1930 [==============================] - 0s 153us/sample - loss: 0.4753 - acc: 0.7684\n",
      "Epoch 3/12\n",
      "1930/1930 [==============================] - 0s 83us/sample - loss: 0.4713 - acc: 0.7689\n",
      "Epoch 4/12\n",
      "1930/1930 [==============================] - 0s 108us/sample - loss: 0.4658 - acc: 0.7756\n",
      "Epoch 5/12\n",
      "1930/1930 [==============================] - 0s 120us/sample - loss: 0.4657 - acc: 0.7746\n",
      "Epoch 6/12\n",
      "1930/1930 [==============================] - 0s 88us/sample - loss: 0.4632 - acc: 0.7715\n",
      "Epoch 7/12\n",
      "1930/1930 [==============================] - 0s 78us/sample - loss: 0.4624 - acc: 0.7777\n",
      "Epoch 8/12\n",
      "1930/1930 [==============================] - 0s 84us/sample - loss: 0.4619 - acc: 0.7782\n",
      "Epoch 9/12\n",
      "1930/1930 [==============================] - 0s 197us/sample - loss: 0.4599 - acc: 0.7782\n",
      "Epoch 10/12\n",
      "1930/1930 [==============================] - 0s 100us/sample - loss: 0.4581 - acc: 0.7777\n",
      "Epoch 11/12\n",
      "1930/1930 [==============================] - 0s 76us/sample - loss: 0.4539 - acc: 0.7824\n",
      "Epoch 12/12\n",
      "1930/1930 [==============================] - 0s 105us/sample - loss: 0.4547 - acc: 0.7798\n",
      "Epoch 1/12\n",
      "1872/1872 [==============================] - 1s 589us/sample - loss: 0.5423 - acc: 0.7687\n",
      "Epoch 2/12\n",
      "1872/1872 [==============================] - 0s 76us/sample - loss: 0.4069 - acc: 0.8323\n",
      "Epoch 3/12\n",
      "1872/1872 [==============================] - 0s 80us/sample - loss: 0.3987 - acc: 0.8264\n",
      "Epoch 4/12\n",
      "1872/1872 [==============================] - 0s 82us/sample - loss: 0.3906 - acc: 0.8333\n",
      "Epoch 5/12\n",
      "1872/1872 [==============================] - 0s 75us/sample - loss: 0.3947 - acc: 0.8349\n",
      "Epoch 6/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3909 - acc: 0.8291\n",
      "Epoch 7/12\n",
      "1872/1872 [==============================] - 0s 78us/sample - loss: 0.3970 - acc: 0.8280\n",
      "Epoch 8/12\n",
      "1872/1872 [==============================] - 0s 118us/sample - loss: 0.3933 - acc: 0.8355\n",
      "Epoch 9/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3862 - acc: 0.8328\n",
      "Epoch 10/12\n",
      "1872/1872 [==============================] - 0s 101us/sample - loss: 0.3921 - acc: 0.8312\n",
      "Epoch 11/12\n",
      "1872/1872 [==============================] - 0s 105us/sample - loss: 0.3882 - acc: 0.8376\n",
      "Epoch 12/12\n",
      "1872/1872 [==============================] - 0s 74us/sample - loss: 0.3886 - acc: 0.8323\n",
      "Epoch 1/12\n",
      "2011/2011 [==============================] - 1s 616us/sample - loss: 0.5403 - acc: 0.7653\n",
      "Epoch 2/12\n",
      "2011/2011 [==============================] - 0s 75us/sample - loss: 0.3943 - acc: 0.8344\n",
      "Epoch 3/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3889 - acc: 0.8349\n",
      "Epoch 4/12\n",
      "2011/2011 [==============================] - 0s 77us/sample - loss: 0.3796 - acc: 0.8379\n",
      "Epoch 5/12\n",
      "2011/2011 [==============================] - 0s 73us/sample - loss: 0.3781 - acc: 0.8369\n",
      "Epoch 6/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3783 - acc: 0.8414\n",
      "Epoch 7/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3789 - acc: 0.8329\n",
      "Epoch 8/12\n",
      "2011/2011 [==============================] - 0s 70us/sample - loss: 0.3768 - acc: 0.8389\n",
      "Epoch 9/12\n",
      "2011/2011 [==============================] - 0s 85us/sample - loss: 0.3774 - acc: 0.8359\n",
      "Epoch 10/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3731 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "2011/2011 [==============================] - 0s 103us/sample - loss: 0.3727 - acc: 0.8339\n",
      "Epoch 12/12\n",
      "2011/2011 [==============================] - 0s 110us/sample - loss: 0.3747 - acc: 0.8364\n",
      "Epoch 1/12\n",
      "2104/2104 [==============================] - 1s 508us/sample - loss: 0.4769 - acc: 0.8427\n",
      "Epoch 2/12\n",
      "2104/2104 [==============================] - 0s 82us/sample - loss: 0.3295 - acc: 0.8736\n",
      "Epoch 3/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3237 - acc: 0.8707\n",
      "Epoch 4/12\n",
      "2104/2104 [==============================] - 0s 68us/sample - loss: 0.3248 - acc: 0.8769\n",
      "Epoch 5/12\n",
      "2104/2104 [==============================] - 0s 71us/sample - loss: 0.3204 - acc: 0.8802\n",
      "Epoch 6/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3153 - acc: 0.8755\n",
      "Epoch 7/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3146 - acc: 0.8760\n",
      "Epoch 8/12\n",
      "2104/2104 [==============================] - 0s 69us/sample - loss: 0.3151 - acc: 0.8769\n",
      "Epoch 9/12\n",
      "2104/2104 [==============================] - 0s 70us/sample - loss: 0.3174 - acc: 0.8755\n",
      "Epoch 10/12\n",
      "2104/2104 [==============================] - 0s 113us/sample - loss: 0.3185 - acc: 0.8721\n",
      "Epoch 11/12\n",
      "2104/2104 [==============================] - 0s 110us/sample - loss: 0.3121 - acc: 0.8755\n",
      "Epoch 12/12\n",
      "2104/2104 [==============================] - 0s 101us/sample - loss: 0.3133 - acc: 0.8764\n",
      "Epoch 1/12\n",
      "1961/1961 [==============================] - 1s 624us/sample - loss: 0.5332 - acc: 0.8006\n",
      "Epoch 2/12\n",
      "1961/1961 [==============================] - 0s 71us/sample - loss: 0.3938 - acc: 0.8338\n",
      "Epoch 3/12\n",
      "1961/1961 [==============================] - 0s 64us/sample - loss: 0.3862 - acc: 0.8332\n",
      "Epoch 4/12\n",
      "1961/1961 [==============================] - 0s 76us/sample - loss: 0.3847 - acc: 0.8414\n",
      "Epoch 5/12\n",
      "1961/1961 [==============================] - 0s 73us/sample - loss: 0.3840 - acc: 0.8368\n",
      "Epoch 6/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3818 - acc: 0.8409\n",
      "Epoch 7/12\n",
      "1961/1961 [==============================] - 0s 65us/sample - loss: 0.3765 - acc: 0.8409\n",
      "Epoch 8/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3794 - acc: 0.8399\n",
      "Epoch 9/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3764 - acc: 0.8353\n",
      "Epoch 10/12\n",
      "1961/1961 [==============================] - 0s 66us/sample - loss: 0.3705 - acc: 0.8394\n",
      "Epoch 11/12\n",
      "1961/1961 [==============================] - 0s 62us/sample - loss: 0.3706 - acc: 0.8404\n",
      "Epoch 12/12\n",
      "1961/1961 [==============================] - 0s 63us/sample - loss: 0.3750 - acc: 0.8404\n",
      "Epoch 1/12\n",
      "1922/1922 [==============================] - 1s 777us/sample - loss: 0.5168 - acc: 0.8293\n",
      "Epoch 2/12\n",
      "1922/1922 [==============================] - 0s 109us/sample - loss: 0.3586 - acc: 0.8574\n",
      "Epoch 3/12\n",
      "1922/1922 [==============================] - 0s 149us/sample - loss: 0.3523 - acc: 0.8585\n",
      "Epoch 4/12\n",
      "1922/1922 [==============================] - 0s 119us/sample - loss: 0.3535 - acc: 0.8611\n",
      "Epoch 5/12\n",
      "1922/1922 [==============================] - 0s 72us/sample - loss: 0.3469 - acc: 0.8580\n",
      "Epoch 6/12\n",
      "1922/1922 [==============================] - 0s 99us/sample - loss: 0.3490 - acc: 0.8595\n",
      "Epoch 7/12\n",
      "1922/1922 [==============================] - 0s 96us/sample - loss: 0.3515 - acc: 0.8600\n",
      "Epoch 8/12\n",
      "1922/1922 [==============================] - 0s 123us/sample - loss: 0.3522 - acc: 0.8590\n",
      "Epoch 9/12\n",
      "1922/1922 [==============================] - 0s 94us/sample - loss: 0.3459 - acc: 0.8626\n",
      "Epoch 10/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3446 - acc: 0.8585\n",
      "Epoch 11/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3440 - acc: 0.8658\n",
      "Epoch 12/12\n",
      "1922/1922 [==============================] - 0s 63us/sample - loss: 0.3455 - acc: 0.8606\n",
      "Epoch 1/12\n",
      "2073/2073 [==============================] - 1s 712us/sample - loss: 0.6026 - acc: 0.7159\n",
      "Epoch 2/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.5090 - acc: 0.7501\n",
      "Epoch 3/12\n",
      "2073/2073 [==============================] - 0s 90us/sample - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 4/12\n",
      "2073/2073 [==============================] - 0s 88us/sample - loss: 0.4939 - acc: 0.7574\n",
      "Epoch 5/12\n",
      "2073/2073 [==============================] - 0s 105us/sample - loss: 0.4907 - acc: 0.7569\n",
      "Epoch 6/12\n",
      "2073/2073 [==============================] - 0s 86us/sample - loss: 0.4939 - acc: 0.7511\n",
      "Epoch 7/12\n",
      "2073/2073 [==============================] - 0s 120us/sample - loss: 0.4897 - acc: 0.7545\n",
      "Epoch 8/12\n",
      "2073/2073 [==============================] - 0s 112us/sample - loss: 0.4851 - acc: 0.7525\n",
      "Epoch 9/12\n",
      "2073/2073 [==============================] - 0s 89us/sample - loss: 0.4836 - acc: 0.7574\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2073/2073 [==============================] - 0s 73us/sample - loss: 0.4815 - acc: 0.7593\n",
      "Epoch 11/12\n",
      "2073/2073 [==============================] - 0s 67us/sample - loss: 0.4809 - acc: 0.7583\n",
      "Epoch 12/12\n",
      "2073/2073 [==============================] - 0s 74us/sample - loss: 0.4839 - acc: 0.7612\n",
      "Epoch 1/12\n",
      "2060/2060 [==============================] - 1s 666us/sample - loss: 0.5656 - acc: 0.7524\n",
      "Epoch 2/12\n",
      "2060/2060 [==============================] - 0s 96us/sample - loss: 0.4461 - acc: 0.8034\n",
      "Epoch 3/12\n",
      "2060/2060 [==============================] - 0s 115us/sample - loss: 0.4400 - acc: 0.8049\n",
      "Epoch 4/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4378 - acc: 0.8068\n",
      "Epoch 5/12\n",
      "2060/2060 [==============================] - 0s 82us/sample - loss: 0.4332 - acc: 0.8063\n",
      "Epoch 6/12\n",
      "2060/2060 [==============================] - 0s 113us/sample - loss: 0.4285 - acc: 0.8112\n",
      "Epoch 7/12\n",
      "2060/2060 [==============================] - 0s 99us/sample - loss: 0.4340 - acc: 0.8083\n",
      "Epoch 8/12\n",
      "2060/2060 [==============================] - 0s 95us/sample - loss: 0.4242 - acc: 0.8107\n",
      "Epoch 9/12\n",
      "2060/2060 [==============================] - 0s 69us/sample - loss: 0.4254 - acc: 0.8087\n",
      "Epoch 10/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4294 - acc: 0.8053\n",
      "Epoch 11/12\n",
      "2060/2060 [==============================] - 0s 66us/sample - loss: 0.4233 - acc: 0.8126\n",
      "Epoch 12/12\n",
      "2060/2060 [==============================] - 0s 73us/sample - loss: 0.4243 - acc: 0.8092\n",
      "Epoch 1/12\n",
      "2068/2068 [==============================] - 1s 625us/sample - loss: 0.6028 - acc: 0.7191\n",
      "Epoch 2/12\n",
      "2068/2068 [==============================] - 0s 111us/sample - loss: 0.5009 - acc: 0.7606\n",
      "Epoch 3/12\n",
      "2068/2068 [==============================] - 0s 100us/sample - loss: 0.4894 - acc: 0.7611\n",
      "Epoch 4/12\n",
      "2068/2068 [==============================] - 0s 105us/sample - loss: 0.4856 - acc: 0.7602\n",
      "Epoch 5/12\n",
      "2068/2068 [==============================] - 0s 99us/sample - loss: 0.4867 - acc: 0.7703\n",
      "Epoch 6/12\n",
      "2068/2068 [==============================] - 0s 113us/sample - loss: 0.4813 - acc: 0.7655\n",
      "Epoch 7/12\n",
      "2068/2068 [==============================] - 0s 85us/sample - loss: 0.4828 - acc: 0.7655\n",
      "Epoch 8/12\n",
      "2068/2068 [==============================] - 0s 63us/sample - loss: 0.4779 - acc: 0.7722\n",
      "Epoch 9/12\n",
      "2068/2068 [==============================] - 0s 65us/sample - loss: 0.4774 - acc: 0.7747\n",
      "Epoch 10/12\n",
      "2068/2068 [==============================] - 0s 64us/sample - loss: 0.4781 - acc: 0.7713\n",
      "Epoch 11/12\n",
      "2068/2068 [==============================] - 0s 77us/sample - loss: 0.4806 - acc: 0.7737\n",
      "Epoch 12/12\n",
      "2068/2068 [==============================] - 0s 71us/sample - loss: 0.4771 - acc: 0.7703\n",
      "Epoch 1/12\n",
      "1999/1999 [==============================] - 1s 609us/sample - loss: 0.5430 - acc: 0.7764\n",
      "Epoch 2/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.3979 - acc: 0.8339\n",
      "Epoch 3/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.4011 - acc: 0.8279\n",
      "Epoch 4/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3924 - acc: 0.8349\n",
      "Epoch 5/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3964 - acc: 0.8309\n",
      "Epoch 6/12\n",
      "1999/1999 [==============================] - 0s 75us/sample - loss: 0.3945 - acc: 0.8304\n",
      "Epoch 7/12\n",
      "1999/1999 [==============================] - 0s 72us/sample - loss: 0.3871 - acc: 0.8404\n",
      "Epoch 8/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3892 - acc: 0.8324\n",
      "Epoch 9/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3878 - acc: 0.8339\n",
      "Epoch 10/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3863 - acc: 0.8329\n",
      "Epoch 11/12\n",
      "1999/1999 [==============================] - 0s 69us/sample - loss: 0.3829 - acc: 0.8329\n",
      "Epoch 12/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.3859 - acc: 0.8369\n"
     ]
    }
   ],
   "source": [
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnonymousTuple([(conv2d/kernel, array([[[[ 1.06490128e-01, -4.87016588e-02, -6.85656965e-02,\n",
       "          -3.34899910e-02, -1.29685551e-01,  4.60619703e-02,\n",
       "          -7.16587976e-02, -8.52979869e-02, -8.97415876e-02,\n",
       "           1.17851011e-01,  1.13360420e-01,  1.50884083e-02,\n",
       "           1.22043245e-01,  4.09488901e-02, -1.12375773e-01,\n",
       "           3.34218144e-02,  1.11435927e-01,  3.85068357e-02,\n",
       "           4.91258428e-02, -1.03038169e-01,  2.58933716e-02,\n",
       "          -5.99962361e-02,  9.04363990e-02, -2.97869649e-02,\n",
       "          -9.27260295e-02, -9.19310227e-02,  7.80995786e-02,\n",
       "           1.03498466e-01, -6.25450909e-02, -9.60691199e-02,\n",
       "           2.82946154e-02,  1.33131042e-01],\n",
       "         [-9.26792994e-02, -1.58183157e-01,  3.16159278e-02,\n",
       "           1.06977910e-01, -3.55711812e-03,  9.78906155e-02,\n",
       "          -4.51781414e-02,  1.09493561e-01,  1.70011535e-01,\n",
       "          -7.23329633e-02,  1.56633779e-01,  1.42113730e-01,\n",
       "          -5.58223873e-02,  1.40036851e-01, -1.04734935e-01,\n",
       "           5.67419417e-02, -5.67961670e-02,  1.78843290e-01,\n",
       "          -1.97310280e-02,  3.65473740e-02,  1.41543522e-01,\n",
       "          -9.75631773e-02,  2.38953531e-03, -1.25410587e-01,\n",
       "           3.28855515e-02,  1.16471387e-01, -1.19362324e-01,\n",
       "           2.91323029e-02,  1.00438491e-01, -5.58059663e-04,\n",
       "          -1.56958774e-01, -8.94572437e-02],\n",
       "         [ 5.51510826e-02, -1.47621870e-01, -3.01040895e-02,\n",
       "           3.69285494e-02,  1.01483814e-01,  1.33640960e-01,\n",
       "          -1.55667618e-01,  1.22673318e-01,  1.30018920e-01,\n",
       "          -4.43460681e-02,  9.52384099e-02, -1.01100579e-01,\n",
       "           1.33731246e-01,  2.62024179e-02, -6.39518425e-02,\n",
       "          -5.42472750e-02,  3.71446535e-02,  1.30959570e-01,\n",
       "           4.53424361e-03,  1.22390106e-01,  9.00588278e-03,\n",
       "          -6.82163909e-02,  1.21874101e-01,  1.11803986e-01,\n",
       "           1.01639323e-01, -1.87100265e-02, -1.02297321e-03,\n",
       "           4.98357825e-02,  4.50041033e-02, -5.99361360e-02,\n",
       "          -2.14911494e-02,  4.87208441e-02]],\n",
       "\n",
       "        [[-3.34591009e-02, -3.09745707e-02,  6.22146111e-03,\n",
       "           6.79621249e-02, -1.10325485e-01,  3.93547490e-02,\n",
       "          -5.05487621e-02,  1.39863551e-01,  1.58064365e-02,\n",
       "           8.30182899e-03, -1.98528934e-02,  1.64755225e-01,\n",
       "          -8.06700066e-03,  2.38758512e-02, -5.28553650e-02,\n",
       "           5.58563583e-02, -5.45599833e-02,  1.37607604e-01,\n",
       "          -1.35664493e-01, -3.68988365e-02,  1.05513215e-01,\n",
       "           1.02359131e-01, -1.19282521e-01,  1.31573364e-01,\n",
       "          -8.81266370e-02,  7.19482526e-02,  5.86366951e-02,\n",
       "           2.61635613e-02, -4.41425368e-02, -8.07066411e-02,\n",
       "          -1.46076337e-01, -5.01267239e-02],\n",
       "         [ 5.68920448e-02, -1.25904292e-01,  1.54995680e-01,\n",
       "           3.87829393e-02, -5.76653779e-02,  6.95789382e-02,\n",
       "          -1.12172529e-01,  3.32845971e-02,  1.54281124e-01,\n",
       "          -1.02868438e-01, -6.34713918e-02, -2.48429831e-02,\n",
       "          -7.09634647e-02,  5.00809252e-02,  1.35047868e-01,\n",
       "           5.44185303e-02,  6.24836944e-02, -5.26304357e-02,\n",
       "           3.52846123e-02,  7.85266981e-02,  2.38594227e-02,\n",
       "           4.47187498e-02, -7.43855387e-02, -3.96534093e-02,\n",
       "           9.54218023e-03,  6.21044217e-03, -1.31401606e-02,\n",
       "           1.41690224e-01, -2.29403079e-02, -5.96954450e-02,\n",
       "          -2.69486010e-03, -8.15337524e-02],\n",
       "         [-1.41506970e-01,  1.09892033e-01,  1.18067861e-01,\n",
       "           1.15517780e-01, -1.43871754e-01, -1.24750681e-01,\n",
       "          -5.79819679e-02, -9.33679119e-02,  5.29625043e-02,\n",
       "           2.27698330e-02,  4.58210856e-02,  1.12824090e-01,\n",
       "           2.77318750e-02,  8.18045065e-02,  1.20784692e-01,\n",
       "           8.63247737e-02, -1.32586941e-01,  1.52964696e-01,\n",
       "           1.08295120e-01, -6.96892152e-03, -1.10033877e-01,\n",
       "          -5.33336997e-02, -9.93011966e-02, -1.09620981e-01,\n",
       "          -5.91600984e-02,  9.80283767e-02,  9.25713554e-02,\n",
       "           6.83808699e-02, -7.40438774e-02,  8.99896324e-02,\n",
       "          -9.75174233e-02,  1.33911937e-01]],\n",
       "\n",
       "        [[-3.10996082e-02,  8.49848315e-02,  1.36891484e-01,\n",
       "           3.26184668e-02,  3.32857482e-02,  7.73140118e-02,\n",
       "           1.13024682e-01,  4.05435152e-02,  3.04426271e-02,\n",
       "           9.78921074e-03, -1.17748789e-01, -3.16847786e-02,\n",
       "          -7.84001574e-02,  2.65967157e-02,  1.11699000e-01,\n",
       "          -7.08167702e-02,  8.46164376e-02, -7.05781356e-02,\n",
       "          -9.25439522e-02, -8.40613246e-02, -4.58811224e-02,\n",
       "          -6.08764030e-03,  1.06232166e-01,  4.10081074e-02,\n",
       "           5.63952653e-03, -1.25940219e-01,  5.80487475e-02,\n",
       "          -3.39355804e-02, -1.47272111e-03,  1.00603871e-01,\n",
       "           2.47486662e-02, -9.56901312e-02],\n",
       "         [-5.11057209e-03,  2.22087130e-02, -1.20021500e-01,\n",
       "           1.18448295e-01, -3.33565027e-02, -1.73074335e-01,\n",
       "          -1.16940022e-01, -4.01437394e-02,  3.64797451e-02,\n",
       "          -7.13780448e-02, -9.75033715e-02,  9.61558670e-02,\n",
       "           1.69294477e-02,  3.22903655e-02,  1.94474608e-02,\n",
       "          -1.18157774e-01,  9.68430117e-02,  7.79355597e-03,\n",
       "           6.11120835e-02, -4.40809578e-02,  1.09306797e-01,\n",
       "          -3.17682065e-02, -8.61990824e-02,  7.92296752e-02,\n",
       "          -9.01842266e-02,  1.94790065e-02, -8.49856734e-02,\n",
       "           1.35284469e-01, -3.55626307e-02, -8.13332647e-02,\n",
       "           3.00534330e-02,  1.19813204e-01],\n",
       "         [-9.21209995e-03, -1.33574396e-01, -6.78886175e-02,\n",
       "          -1.03776827e-01, -1.05229743e-01, -1.37709886e-01,\n",
       "           1.31332338e-01,  1.10103875e-01, -6.90999627e-02,\n",
       "           1.11236490e-01,  3.61511298e-02, -4.72854674e-02,\n",
       "          -1.37412757e-01, -3.07341609e-02, -1.55318826e-02,\n",
       "          -2.52165981e-02, -1.49646685e-01,  1.13988146e-01,\n",
       "           7.57923573e-02, -1.24911010e-01, -8.52483138e-02,\n",
       "           1.69540271e-01, -1.47219241e-01,  1.05117835e-01,\n",
       "          -3.98641229e-02, -1.10329375e-01, -7.65220299e-02,\n",
       "          -8.73047933e-02, -3.85248177e-02,  4.53836806e-02,\n",
       "           9.79384705e-02, -3.55799682e-02]]],\n",
       "\n",
       "\n",
       "       [[[-1.52266808e-02, -1.67295057e-02,  5.43209948e-02,\n",
       "          -1.52241886e-01, -1.20607793e-01, -1.21212453e-01,\n",
       "           1.41753152e-01,  1.22303320e-02, -1.17822640e-01,\n",
       "          -8.67244825e-02, -1.00332320e-01,  4.16920073e-02,\n",
       "           4.57657650e-02,  6.91801533e-02,  8.72951597e-02,\n",
       "           8.68024155e-02, -1.37088507e-01, -7.55543411e-02,\n",
       "           2.16813013e-02, -1.74780756e-01,  1.55288607e-01,\n",
       "          -1.07655168e-01,  5.92554137e-02, -6.09586686e-02,\n",
       "          -1.18576847e-01,  1.12491935e-01,  7.64532536e-02,\n",
       "          -3.62580009e-02, -6.98017096e-03,  9.23043862e-02,\n",
       "          -1.51262596e-01, -7.88537040e-02],\n",
       "         [ 1.11089066e-01, -7.49370158e-02,  1.98614784e-04,\n",
       "          -1.52203098e-01, -1.54703949e-02,  5.29362783e-02,\n",
       "           7.20193908e-02, -8.48359317e-02, -6.85835183e-02,\n",
       "          -5.60467169e-02,  1.02921473e-02, -9.99438465e-02,\n",
       "          -4.94404100e-02,  1.58630926e-02, -1.34040013e-01,\n",
       "           1.23476043e-01,  1.13665387e-01, -5.16239144e-02,\n",
       "           6.54974859e-03, -1.66303262e-01, -7.82178044e-02,\n",
       "          -1.02027692e-01,  5.01571633e-02, -1.71783075e-01,\n",
       "           6.60030320e-02, -3.30664180e-02,  5.85410520e-02,\n",
       "          -1.14874460e-01,  1.42955974e-01, -1.46874320e-02,\n",
       "          -9.01093632e-02, -1.51288658e-01],\n",
       "         [-1.23658203e-01,  1.78819278e-03, -3.73617397e-04,\n",
       "          -1.09836683e-01,  3.03768925e-02,  9.69743952e-02,\n",
       "           9.14539844e-02, -1.27890006e-01,  7.63667300e-02,\n",
       "          -9.27246958e-02, -1.09586395e-01,  2.62762383e-02,\n",
       "           1.23445965e-01,  4.34383266e-02, -7.93793052e-02,\n",
       "           4.64982493e-03, -4.75940071e-02,  1.29088253e-01,\n",
       "           4.67881747e-02, -4.05989736e-02,  1.33194119e-01,\n",
       "          -1.40284253e-02, -1.07337959e-01, -1.27320185e-01,\n",
       "          -1.02152996e-01, -1.03771292e-01,  8.67911950e-02,\n",
       "           1.56608611e-01, -3.48045081e-02, -1.00884520e-01,\n",
       "           8.96559805e-02,  1.17176976e-02]],\n",
       "\n",
       "        [[ 7.16793584e-03,  1.49479777e-01, -4.05025631e-02,\n",
       "          -1.04310997e-01,  1.22796632e-01, -5.33661665e-03,\n",
       "           1.12803303e-01,  2.95523163e-02,  7.86144882e-02,\n",
       "          -9.14989412e-02, -1.15373388e-01, -4.57094610e-02,\n",
       "           9.82840657e-02, -4.56397831e-02,  5.05898781e-02,\n",
       "           9.51476172e-02,  5.17982170e-02, -1.41627461e-01,\n",
       "           1.84867308e-02,  1.27563000e-01, -1.17350347e-01,\n",
       "           3.81052531e-02,  4.62479033e-02, -6.28931727e-03,\n",
       "          -8.91435295e-02, -1.35390013e-01, -2.81569045e-02,\n",
       "           7.95859937e-03, -8.23595971e-02, -1.36246171e-03,\n",
       "          -7.06645623e-02,  1.11613631e-01],\n",
       "         [ 1.11441039e-01,  3.76530830e-03, -1.22646034e-01,\n",
       "          -9.70279574e-02, -1.28550842e-01, -1.17176138e-01,\n",
       "          -1.17497094e-01,  6.61319420e-02, -8.83296132e-02,\n",
       "          -1.13592640e-01, -5.59585402e-04, -1.69775821e-02,\n",
       "          -1.11137778e-01, -7.18751624e-02,  9.59657729e-02,\n",
       "           3.77121679e-02,  9.24804509e-02,  5.88613115e-02,\n",
       "          -3.69407958e-03,  8.07897598e-02, -2.89932564e-02,\n",
       "          -9.34198201e-02,  1.08417399e-01, -7.64280930e-02,\n",
       "           1.35387689e-01, -2.82895323e-02, -5.64915710e-04,\n",
       "           1.13405092e-02, -1.10463545e-01,  6.33847490e-02,\n",
       "           1.11124896e-01, -8.89988765e-02],\n",
       "         [-4.11376283e-02, -1.11904055e-01, -3.76943573e-02,\n",
       "           8.57611820e-02,  9.44698304e-02,  1.12164669e-01,\n",
       "           3.91086787e-02,  2.08176766e-03, -5.15873283e-02,\n",
       "           5.18476367e-02, -2.05307845e-02, -5.87407164e-02,\n",
       "           2.06865445e-02, -3.90735976e-02,  2.10224949e-02,\n",
       "          -8.30508862e-03,  2.75749173e-02, -9.67736393e-02,\n",
       "          -9.11150947e-02,  1.41631261e-01, -1.03764467e-01,\n",
       "          -3.35805379e-02, -5.41284643e-02, -7.76794627e-02,\n",
       "          -4.66675535e-02,  2.21852474e-02, -1.35445997e-01,\n",
       "           1.63193103e-02,  2.37388890e-02,  4.60260212e-02,\n",
       "           3.00548077e-02,  7.69646233e-03]],\n",
       "\n",
       "        [[-9.49807744e-03, -4.31577535e-03, -8.83831754e-02,\n",
       "           6.72874600e-02, -1.05320893e-01,  7.34524196e-03,\n",
       "          -1.26935869e-01,  3.38832885e-02, -8.85864496e-02,\n",
       "          -1.22943617e-01, -1.05282273e-02, -1.26336187e-01,\n",
       "          -1.50545880e-01, -3.39855179e-02,  4.61598709e-02,\n",
       "          -1.61445826e-01,  1.16171546e-01, -1.50209025e-01,\n",
       "          -4.75766249e-02,  9.67842564e-02,  1.25975072e-01,\n",
       "           5.76831400e-02,  1.29293904e-01,  3.78610156e-02,\n",
       "           8.99809077e-02,  2.27556122e-03, -1.18507579e-01,\n",
       "          -1.15965761e-01,  5.16685508e-02,  1.32063180e-01,\n",
       "           8.50991011e-02, -1.37866601e-01],\n",
       "         [ 6.33785408e-03,  9.42219868e-02,  8.67802054e-02,\n",
       "          -4.93399352e-02,  2.31974404e-02, -1.85273290e-01,\n",
       "          -1.29910335e-01, -9.05324817e-02,  1.37599796e-01,\n",
       "          -7.58215934e-02,  1.34169996e-01, -1.13457166e-01,\n",
       "           2.46443376e-02,  7.00740591e-02,  5.74005730e-02,\n",
       "          -3.83083560e-02,  4.75388058e-02, -1.55166015e-01,\n",
       "          -1.29349381e-01, -1.34233490e-01, -8.97974148e-02,\n",
       "          -1.05607305e-02, -4.74074222e-02, -4.21841480e-02,\n",
       "          -1.07882202e-01, -9.76941511e-02, -6.89950511e-02,\n",
       "           8.47652406e-02, -1.69210017e-01, -4.23382223e-03,\n",
       "           6.01036027e-02, -6.34487569e-02],\n",
       "         [ 4.97066006e-02, -6.70011640e-02, -6.82312027e-02,\n",
       "           1.00354627e-02,  4.95746247e-02,  1.78063307e-02,\n",
       "          -1.23948708e-01,  7.04261139e-02,  4.97176051e-02,\n",
       "           5.36862761e-04,  5.82790263e-02,  1.47962987e-01,\n",
       "          -4.06328030e-03,  5.79641797e-02, -1.24442443e-01,\n",
       "          -9.51765925e-02, -9.49373245e-02, -1.42826289e-01,\n",
       "           9.01782811e-02, -1.26050860e-01,  6.32191300e-02,\n",
       "           1.45598948e-01, -3.53740342e-02,  3.89828831e-02,\n",
       "           1.01614848e-01,  1.45201311e-02,  1.09342426e-01,\n",
       "          -1.17247716e-01,  6.24068119e-02,  1.19845703e-01,\n",
       "          -3.11638154e-02,  1.27724394e-01]]],\n",
       "\n",
       "\n",
       "       [[[ 5.81656136e-02,  1.03481576e-01, -1.10595994e-01,\n",
       "           3.81543636e-02,  1.55691147e-01,  8.45329612e-02,\n",
       "          -9.31494385e-02,  8.25830698e-02,  1.23287618e-01,\n",
       "           1.00637324e-01, -3.91268823e-03, -1.08141020e-01,\n",
       "           6.10780269e-02, -8.58638734e-02, -1.39653593e-01,\n",
       "           4.34344448e-02, -1.38304353e-01,  1.34120703e-01,\n",
       "           1.84482172e-01, -1.34341165e-01, -7.08079487e-02,\n",
       "          -6.72053322e-02, -1.06133111e-01, -3.76116484e-02,\n",
       "          -7.29129463e-02,  1.39965042e-01, -8.21970925e-02,\n",
       "          -9.76400673e-02,  1.00936338e-01, -5.77745512e-02,\n",
       "           1.03402257e-01,  5.75564913e-02],\n",
       "         [-1.24428429e-01, -4.10743281e-02, -5.54466844e-02,\n",
       "           1.14659354e-01,  4.66203503e-02, -8.32238495e-02,\n",
       "           1.36211142e-01, -5.18195145e-02, -1.10936373e-01,\n",
       "           1.56443700e-01, -2.63903979e-02, -5.78041226e-02,\n",
       "           1.22972941e-02,  5.41460216e-02, -1.20221026e-01,\n",
       "          -1.28053473e-02, -8.09050575e-02,  1.21131800e-01,\n",
       "           5.83906323e-02, -1.78455133e-02, -8.12812150e-02,\n",
       "           1.22304611e-01, -9.97870117e-02,  9.20346305e-02,\n",
       "           9.41844657e-02,  1.53623372e-01, -1.07467897e-01,\n",
       "          -1.62558295e-02,  7.73368403e-02,  1.09347049e-02,\n",
       "          -3.96423750e-02,  1.02591194e-01],\n",
       "         [-9.17880684e-02,  6.91573322e-02,  4.74862121e-02,\n",
       "           1.19237371e-01,  8.83453041e-02,  9.38967839e-02,\n",
       "          -1.80559931e-04,  4.46411930e-02, -1.46372363e-01,\n",
       "           9.58842319e-03, -3.32664922e-02, -1.45399105e-02,\n",
       "          -6.64664758e-03, -4.58533429e-02, -6.50980100e-02,\n",
       "          -1.21311858e-01, -1.12709455e-01, -6.73443973e-02,\n",
       "           7.36716911e-02,  6.43740296e-02, -1.32512659e-01,\n",
       "          -1.20066531e-01, -6.85287565e-02,  1.41725391e-01,\n",
       "           9.57821831e-02, -1.00659719e-03,  4.54522856e-02,\n",
       "           5.13010658e-02,  1.68077290e-01, -9.96465683e-02,\n",
       "           8.51038471e-02,  3.66304778e-02]],\n",
       "\n",
       "        [[ 1.22943148e-01,  1.41507506e-01,  1.63624391e-01,\n",
       "          -7.34992921e-02,  4.40538526e-02,  9.73749682e-02,\n",
       "           1.23883924e-02,  9.76051763e-02, -4.61775549e-02,\n",
       "           1.01015508e-01, -3.85502763e-02, -8.49127769e-02,\n",
       "           8.36786181e-02, -1.06543556e-01,  8.04471150e-02,\n",
       "          -3.27976272e-02,  7.59596704e-03,  5.75700700e-02,\n",
       "          -7.10941404e-02,  1.31616145e-01, -5.16123958e-02,\n",
       "           1.46384522e-01, -6.25890046e-02,  8.18482414e-02,\n",
       "           1.01227768e-01, -5.03806844e-02, -1.44850221e-02,\n",
       "          -1.21722639e-01,  8.46084952e-02, -3.60367633e-02,\n",
       "           5.96164018e-02, -9.00093764e-02],\n",
       "         [ 6.05034120e-02, -6.83298707e-02,  4.76993918e-02,\n",
       "          -9.20791328e-02, -7.85147846e-02, -7.59454966e-02,\n",
       "           3.94626185e-02, -9.18578878e-02, -1.11120604e-01,\n",
       "           1.25047147e-01, -6.33365475e-03,  7.90872425e-02,\n",
       "           8.82457271e-02, -8.75382572e-02,  1.27690181e-01,\n",
       "          -9.72386673e-02,  1.87467653e-02, -1.83611047e-02,\n",
       "          -6.62904829e-02,  1.04158960e-01, -5.29817194e-02,\n",
       "          -1.21642642e-01,  1.80466231e-02, -4.34923619e-02,\n",
       "           1.33451447e-02, -5.89486547e-02,  7.39395386e-04,\n",
       "          -1.37410045e-01,  7.91209713e-02, -1.02345988e-01,\n",
       "          -9.49912220e-02, -1.51053369e-02],\n",
       "         [-3.52287255e-02,  1.17908865e-01,  3.80658396e-02,\n",
       "           9.21073332e-02,  1.35537460e-01,  3.47734801e-02,\n",
       "           1.08223416e-01, -4.17885520e-02,  6.67623803e-02,\n",
       "           1.77614540e-02,  1.85993984e-02, -2.56008133e-02,\n",
       "          -2.80667935e-02, -8.37552845e-02,  7.20149279e-02,\n",
       "           4.92505729e-02, -1.50241420e-01, -1.55028716e-01,\n",
       "          -7.91058838e-02,  4.86415178e-02,  1.33540735e-01,\n",
       "          -6.92358091e-02,  3.36485021e-02, -9.11759585e-03,\n",
       "          -1.11127965e-01, -7.25842044e-02,  1.25175156e-03,\n",
       "          -5.78687061e-03, -1.40027910e-01,  1.28957063e-01,\n",
       "           1.13093890e-01, -2.11040163e-03]],\n",
       "\n",
       "        [[-8.33985303e-03,  2.06665844e-02,  3.58466469e-02,\n",
       "          -1.25345722e-01,  3.93364839e-02,  2.39765309e-02,\n",
       "           3.64356814e-03, -1.50483595e-02,  2.69571748e-02,\n",
       "          -6.35684878e-02, -1.13255661e-02, -2.23742295e-02,\n",
       "          -1.55173820e-02, -4.05923016e-02, -2.73044966e-02,\n",
       "          -6.82888702e-02,  1.05691716e-01, -1.22971147e-01,\n",
       "           5.63814640e-02,  1.02063365e-01,  2.44104466e-03,\n",
       "          -1.00079300e-02,  4.39327694e-02,  1.87874641e-02,\n",
       "           1.63588494e-01,  8.30685869e-02, -9.04716700e-02,\n",
       "           1.41668633e-01,  1.99736487e-02,  1.31665587e-01,\n",
       "           8.46482664e-02,  5.64097948e-02],\n",
       "         [ 3.71972984e-03,  1.19635418e-01, -1.17221229e-01,\n",
       "           4.44640480e-02,  1.42472699e-01,  5.84293678e-02,\n",
       "           1.30637780e-01, -1.45633742e-01, -8.27626362e-02,\n",
       "           9.60976481e-02, -3.61932889e-02, -4.35982682e-02,\n",
       "          -4.74249944e-02,  8.44253451e-02,  7.99104422e-02,\n",
       "           4.91521731e-02,  1.31023869e-01,  5.81921823e-02,\n",
       "          -3.14211547e-02, -1.18190581e-02, -1.31257698e-01,\n",
       "          -3.14852223e-02,  7.52024129e-02,  4.40303497e-02,\n",
       "          -5.18257022e-02, -5.80034265e-03,  1.59209058e-01,\n",
       "          -6.57557100e-02, -9.45275202e-02, -1.19385891e-01,\n",
       "          -5.74400909e-02,  1.30784446e-02],\n",
       "         [-8.72850344e-02, -7.81746656e-02, -5.07358424e-02,\n",
       "           5.53891025e-02, -9.71810371e-02, -7.09751323e-02,\n",
       "           3.82535309e-02, -1.35021418e-01, -7.62337521e-02,\n",
       "          -6.35871897e-03,  1.41337618e-01,  1.62261561e-01,\n",
       "          -9.10155326e-02, -1.13894835e-01, -9.99207720e-02,\n",
       "           1.21681668e-01,  6.42022118e-02, -1.32004082e-01,\n",
       "          -1.57830670e-01,  1.07202634e-01,  1.09680273e-01,\n",
       "           9.74090248e-02,  1.21755995e-01, -2.22005378e-02,\n",
       "           3.51858139e-02,  1.27219083e-02,  6.04354814e-02,\n",
       "          -9.61122960e-02, -5.65178245e-02,  6.49389252e-02,\n",
       "           1.01950809e-01,  7.03303814e-02]]]], dtype=float32)), (conv2d/bias, array([ 0.01040061, -0.02039495,  0.01662251, -0.03210071, -0.00441269,\n",
       "       -0.04428442, -0.01472292, -0.02943253,  0.00314773,  0.00096839,\n",
       "       -0.0095632 ,  0.00976825, -0.00788618, -0.02887983,  0.00139348,\n",
       "       -0.0197586 , -0.00088946, -0.0035785 , -0.0155122 , -0.03850878,\n",
       "       -0.01182549,  0.00715725, -0.00289165, -0.01489225,  0.00523285,\n",
       "        0.02750093,  0.02070633, -0.00606055, -0.03065105, -0.04888826,\n",
       "       -0.05686004, -0.01672955], dtype=float32)), (conv2d_1/kernel, array([[[[ 9.90927219e-02, -7.38704354e-02, -6.34515136e-02, ...,\n",
       "          -6.57228976e-02,  2.11655330e-02, -7.33346641e-02],\n",
       "         [ 2.14500092e-02, -4.16160636e-02, -4.36209366e-02, ...,\n",
       "          -2.97477935e-02,  1.05898932e-01, -6.21222630e-02],\n",
       "         [ 9.97273717e-03, -8.45740661e-02,  4.58597466e-02, ...,\n",
       "          -1.43278074e-02, -4.88230139e-02,  1.07932165e-02],\n",
       "         ...,\n",
       "         [ 1.52517138e-02, -1.04703188e-01,  1.99387856e-02, ...,\n",
       "          -6.47971034e-02, -7.49052092e-02,  3.54880802e-02],\n",
       "         [ 2.25305948e-02, -5.71559593e-02,  4.82814535e-02, ...,\n",
       "          -5.84059432e-02, -4.75035384e-02, -9.63504165e-02],\n",
       "         [-7.40580782e-02, -3.75338309e-02,  2.80642863e-02, ...,\n",
       "          -3.42311338e-02,  7.41706192e-02, -4.52978313e-02]],\n",
       "\n",
       "        [[ 4.76948954e-02, -9.81575027e-02,  1.11646436e-01, ...,\n",
       "          -5.52339591e-02,  5.33507671e-03,  4.05577309e-02],\n",
       "         [ 6.29633442e-02, -6.27732500e-02, -7.01574460e-02, ...,\n",
       "           1.03883054e-02, -6.61617098e-03,  2.40296703e-02],\n",
       "         [-6.75788000e-02, -2.17425805e-02, -5.44487759e-02, ...,\n",
       "          -6.75786287e-02,  7.65090692e-04,  6.78587332e-02],\n",
       "         ...,\n",
       "         [-1.02194794e-01, -5.99313304e-02, -8.49828962e-03, ...,\n",
       "          -1.14659831e-01, -2.51513105e-02, -6.38656020e-02],\n",
       "         [-3.25407833e-02, -4.27341927e-03, -4.19849008e-02, ...,\n",
       "          -8.93425569e-02, -3.03984974e-02,  2.88954750e-02],\n",
       "         [-3.94269451e-02, -4.24737781e-02, -5.51514514e-02, ...,\n",
       "          -2.61134934e-02,  2.03755274e-02,  1.05056457e-01]],\n",
       "\n",
       "        [[ 6.81978278e-03, -8.78427401e-02,  9.51087698e-02, ...,\n",
       "           8.99433866e-02,  4.28576879e-02,  3.20996828e-02],\n",
       "         [-3.29406261e-02, -5.54706044e-02,  1.03540719e-02, ...,\n",
       "          -3.80117968e-02, -1.56238731e-02,  3.96782681e-02],\n",
       "         [-2.63067568e-03, -5.72384782e-02, -6.23780712e-02, ...,\n",
       "           4.91586551e-02,  6.30469322e-02,  3.37243378e-02],\n",
       "         ...,\n",
       "         [-7.34632313e-02,  6.90929741e-02, -1.80577748e-02, ...,\n",
       "          -5.87742068e-02, -1.46671519e-01,  2.92475857e-02],\n",
       "         [ 4.51055765e-02,  6.01168349e-02,  7.56343156e-02, ...,\n",
       "           2.83050295e-02,  5.63841313e-05,  1.69953555e-02],\n",
       "         [ 2.31342781e-02, -3.58173475e-02,  2.80027334e-02, ...,\n",
       "          -9.93935093e-02,  1.99884195e-02,  7.83136040e-02]]],\n",
       "\n",
       "\n",
       "       [[[-4.16505486e-02, -2.27216128e-02,  3.04863974e-02, ...,\n",
       "          -2.45113354e-02,  3.33179869e-02,  5.84873706e-02],\n",
       "         [-6.73200265e-02,  7.13369846e-02,  3.91728468e-02, ...,\n",
       "          -1.11070946e-01,  2.75582951e-02,  1.96033120e-02],\n",
       "         [ 4.38773781e-02, -3.42122614e-02,  7.57018775e-02, ...,\n",
       "          -1.26959179e-02, -3.62692997e-02, -1.97495017e-02],\n",
       "         ...,\n",
       "         [-2.85759978e-02,  1.06289089e-02, -2.74961367e-02, ...,\n",
       "           4.14566062e-02,  7.96628445e-02, -6.31512776e-02],\n",
       "         [-3.63207385e-02,  9.37959738e-03,  4.07634266e-02, ...,\n",
       "          -3.20717953e-02, -2.26395745e-02, -8.82690549e-02],\n",
       "         [-8.62886235e-02,  2.68907081e-02, -2.61452924e-02, ...,\n",
       "          -1.67196300e-02,  2.12721322e-02,  6.57446533e-02]],\n",
       "\n",
       "        [[-4.61788960e-02,  1.59777645e-02,  7.21261054e-02, ...,\n",
       "          -7.77143687e-02,  2.92688180e-02,  3.17834392e-02],\n",
       "         [-8.44698101e-02, -5.56204356e-02,  2.55882423e-02, ...,\n",
       "          -7.43502602e-02, -4.49678041e-02, -2.71758344e-02],\n",
       "         [-1.59558188e-02, -5.21031544e-02,  4.85114492e-02, ...,\n",
       "           2.21180283e-02,  3.64199951e-02, -4.25081588e-02],\n",
       "         ...,\n",
       "         [ 2.85611954e-04,  7.87612796e-02, -4.75500375e-02, ...,\n",
       "          -9.70606133e-02, -1.44047700e-02,  9.15030614e-02],\n",
       "         [-1.87137127e-02,  2.86163408e-02,  2.02928521e-02, ...,\n",
       "           3.13545354e-02,  2.84127519e-02,  3.20718326e-02],\n",
       "         [ 2.81203687e-02,  1.33245498e-01, -7.82978013e-02, ...,\n",
       "           1.31504401e-03,  4.01014425e-02,  1.32888317e-01]],\n",
       "\n",
       "        [[ 1.21306842e-02, -6.34108037e-02,  2.31827032e-02, ...,\n",
       "           7.16339126e-02,  5.75141236e-02,  6.29225522e-02],\n",
       "         [ 9.11149308e-02,  2.11142178e-04, -4.96876016e-02, ...,\n",
       "           8.40237290e-02, -6.08439781e-02,  7.74909137e-03],\n",
       "         [-6.22026511e-02, -6.83046281e-02,  2.85660047e-02, ...,\n",
       "           3.52835096e-02, -8.63978490e-02, -5.83414435e-02],\n",
       "         ...,\n",
       "         [ 1.35376472e-02,  7.86476061e-02, -9.48132500e-02, ...,\n",
       "          -9.60133746e-02, -1.25256062e-01, -5.87375052e-02],\n",
       "         [ 5.53115271e-02,  5.94688058e-02,  1.76347867e-02, ...,\n",
       "          -1.59390848e-02, -9.86418650e-02,  5.88842295e-02],\n",
       "         [ 5.51819876e-02,  7.01256469e-02, -5.53125553e-02, ...,\n",
       "          -8.28322768e-02,  1.26866307e-02,  5.96787371e-02]]],\n",
       "\n",
       "\n",
       "       [[[-3.35874483e-02,  1.17746964e-01, -1.63279474e-02, ...,\n",
       "          -4.68607396e-02, -3.72885875e-02,  5.15011176e-02],\n",
       "         [-6.07167594e-02,  1.40374154e-01, -4.85797301e-02, ...,\n",
       "          -9.88178998e-02, -4.34165914e-03,  5.67588545e-02],\n",
       "         [-1.00822188e-02, -2.89845094e-02,  1.59793552e-02, ...,\n",
       "          -7.99909607e-02, -8.49804357e-02,  6.33826945e-03],\n",
       "         ...,\n",
       "         [-8.14705044e-02, -5.37054390e-02,  6.61246255e-02, ...,\n",
       "          -1.92466713e-02,  7.29781464e-02, -9.63346958e-02],\n",
       "         [ 7.59978443e-02,  3.51251997e-02, -3.13686803e-02, ...,\n",
       "           7.00849965e-02, -5.35477069e-04, -8.90010744e-02],\n",
       "         [ 3.22174877e-02, -5.74145243e-02,  4.71587144e-02, ...,\n",
       "          -1.09396297e-02,  4.12689038e-02,  3.25908214e-02]],\n",
       "\n",
       "        [[-1.47583904e-02,  4.12973464e-02,  1.04106180e-01, ...,\n",
       "           6.04604855e-02,  3.95332761e-02,  8.87247082e-03],\n",
       "         [-6.13685958e-02,  9.82314907e-03,  2.09132452e-02, ...,\n",
       "          -4.11919765e-02, -2.19256002e-02, -5.43179028e-02],\n",
       "         [-1.00941928e-02, -3.53303216e-02, -4.24165800e-02, ...,\n",
       "           2.15305062e-03,  2.83887517e-02,  4.93339039e-02],\n",
       "         ...,\n",
       "         [-1.04254745e-01,  6.33335561e-02,  5.50607219e-02, ...,\n",
       "           7.42341857e-03, -9.70950723e-02, -7.13807996e-04],\n",
       "         [ 2.40950938e-02,  3.62282954e-02,  1.61006693e-02, ...,\n",
       "          -9.27821100e-02, -2.15105861e-02,  6.25142455e-03],\n",
       "         [-3.55216488e-02,  1.43741146e-02, -5.54236546e-02, ...,\n",
       "          -3.42032574e-02, -2.15699077e-02,  3.74175534e-02]],\n",
       "\n",
       "        [[ 2.84921303e-02,  8.78696814e-02, -7.32353032e-02, ...,\n",
       "           8.52562040e-02, -2.30015423e-02,  1.57888345e-02],\n",
       "         [ 9.07233208e-02,  1.86509490e-02, -5.46634756e-03, ...,\n",
       "           2.85952222e-02,  4.46688160e-02,  1.17115937e-02],\n",
       "         [-4.46515344e-02, -2.74342336e-02, -2.77315034e-03, ...,\n",
       "          -7.05609098e-02,  4.18216512e-02,  6.23272806e-02],\n",
       "         ...,\n",
       "         [ 7.52054974e-02, -4.57747281e-02,  1.26518775e-02, ...,\n",
       "           4.32607383e-02, -2.07281243e-02,  3.15230712e-02],\n",
       "         [ 7.11101815e-02, -5.07609081e-03, -1.52530838e-02, ...,\n",
       "          -5.68426549e-02, -5.10450937e-02, -7.90823475e-02],\n",
       "         [ 3.53009440e-02,  1.40748196e-03, -7.06495941e-02, ...,\n",
       "          -3.16242352e-02,  2.46935338e-03, -7.95605928e-02]]]],\n",
       "      dtype=float32)), (conv2d_1/bias, array([-0.05285292, -0.01456037, -0.04670366, -0.02752017, -0.01796139,\n",
       "       -0.01257203, -0.03739773, -0.03707683, -0.05546997, -0.00134997,\n",
       "       -0.01515625,  0.00944658, -0.04037773, -0.04559018, -0.01220725,\n",
       "       -0.02213695, -0.04043098, -0.03064253, -0.03058228, -0.05010867,\n",
       "       -0.01323192, -0.03616897, -0.03777353, -0.03810808, -0.0218527 ,\n",
       "       -0.02999828, -0.03611615,  0.00962629, -0.05846659, -0.05097562,\n",
       "       -0.04682234, -0.01931757, -0.0514984 , -0.02800665, -0.05037342,\n",
       "       -0.01892491, -0.03414574, -0.03311944, -0.05230324, -0.0341572 ,\n",
       "       -0.02072025, -0.03038128, -0.01191312, -0.01468166, -0.03262056,\n",
       "       -0.02620728, -0.0140416 , -0.03086521, -0.05809467, -0.03949608,\n",
       "       -0.04531123, -0.05168335, -0.06364068, -0.02428545, -0.03225978,\n",
       "       -0.06053378, -0.03204698, -0.03035398, -0.0344631 , -0.03467951,\n",
       "        0.0039353 , -0.02787788, -0.03622764, -0.02906063], dtype=float32)), (dense/kernel, array([[-0.02099845,  0.00202158, -0.02560382, ..., -0.0013929 ,\n",
       "        -0.02659819, -0.01539872],\n",
       "       [ 0.02042516,  0.02374067,  0.00779084, ..., -0.01745688,\n",
       "        -0.0280701 , -0.05049959],\n",
       "       [-0.031402  , -0.01827362,  0.02438202, ..., -0.03543929,\n",
       "        -0.03685323, -0.03550771],\n",
       "       ...,\n",
       "       [-0.03614363, -0.01109538,  0.03128086, ..., -0.06321033,\n",
       "        -0.01241758, -0.02269372],\n",
       "       [ 0.02532119, -0.04314683,  0.00084813, ...,  0.0140811 ,\n",
       "         0.02613617,  0.03959678],\n",
       "       [ 0.00604129, -0.04383718,  0.01246234, ...,  0.02274245,\n",
       "        -0.00928755,  0.02629573]], dtype=float32)), (dense/bias, array([-1.19883323e-03, -2.13003904e-02, -2.77100760e-03, -4.01294976e-02,\n",
       "       -2.94454917e-02, -2.47659627e-02, -1.97867863e-02,  3.53334956e-02,\n",
       "        2.39237007e-02,  2.92958971e-02, -3.56812701e-02, -1.32569596e-02,\n",
       "       -2.71007661e-02,  1.16602862e-02, -2.45868433e-02, -5.39978454e-03,\n",
       "        5.83595876e-03,  1.69597305e-02,  1.45877162e-02,  2.96793990e-02,\n",
       "       -1.24754915e-02, -2.26328559e-02,  2.91157840e-03,  5.70350140e-03,\n",
       "        6.59344867e-02,  1.67937540e-02,  3.65041234e-02,  1.11114625e-02,\n",
       "       -1.13729490e-02, -1.23337312e-02, -2.99906936e-02,  1.37114180e-02,\n",
       "        2.05338057e-02, -2.93640904e-02, -4.37132306e-02, -2.75427792e-02,\n",
       "        8.86968337e-05, -2.47145761e-02,  2.28426270e-02, -2.51699369e-02,\n",
       "        2.11187284e-02,  2.30814912e-03, -3.56554613e-03, -2.49862280e-02,\n",
       "        2.30241213e-02, -3.09013706e-02, -6.64691487e-03,  2.91595235e-02,\n",
       "       -3.50523666e-02, -5.06361537e-02, -3.24239917e-02,  2.18177568e-02,\n",
       "       -1.41121109e-03, -1.30910249e-02,  3.68483663e-02,  1.42552992e-02,\n",
       "        1.12749683e-02, -2.31796298e-02, -6.99077919e-03, -9.10782628e-03,\n",
       "       -4.96119587e-03,  2.08246429e-02, -1.47436680e-02,  5.43696433e-02,\n",
       "        1.08833304e-02,  8.48477613e-03,  6.03548763e-03, -7.29719363e-03,\n",
       "       -2.67400611e-02, -9.78598278e-03, -5.52947400e-03,  2.71611232e-02,\n",
       "       -9.73943621e-03, -1.83993336e-02,  2.63393745e-02,  2.06601378e-02,\n",
       "       -1.50501737e-02, -2.94972379e-02,  2.41580652e-03, -1.76522657e-02,\n",
       "       -2.96515562e-02, -1.35022672e-02, -1.47753889e-02, -3.66684422e-02,\n",
       "        1.06560346e-02, -2.64346804e-02, -1.66668259e-02,  1.03055201e-02,\n",
       "       -3.74120325e-02,  2.03623362e-02, -4.52836566e-02, -2.79384237e-02,\n",
       "        4.82537523e-02, -2.51644067e-02,  1.61403082e-02, -2.80146720e-03,\n",
       "        2.21060179e-02,  5.31743132e-02,  1.45031745e-02, -2.93512568e-02,\n",
       "        4.41384427e-02, -1.59038771e-02,  1.42015070e-02,  1.64245479e-02,\n",
       "        1.40410820e-02,  2.44659428e-02, -2.13962402e-02, -2.06975117e-02,\n",
       "       -2.12768419e-03, -6.25896594e-03, -7.93209486e-03, -2.95155365e-02,\n",
       "       -4.37861718e-02, -2.03681421e-02,  4.15963940e-02,  3.27650979e-02,\n",
       "       -1.36727048e-02,  3.62040359e-03,  5.47365546e-02,  5.81069551e-02,\n",
       "       -1.83937652e-03,  2.78549083e-02,  1.44074298e-02, -1.37997139e-02,\n",
       "        6.49788277e-03,  1.03400480e-02,  7.65769975e-03,  1.63895674e-02],\n",
       "      dtype=float32)), (dense_1/kernel, array([[ 0.10632993, -0.05521846, -0.15138169, ...,  0.18220738,\n",
       "        -0.23303658,  0.08162715],\n",
       "       [ 0.06326113,  0.1520256 , -0.02333925, ...,  0.18567652,\n",
       "         0.12483248,  0.03244749],\n",
       "       [ 0.10225661,  0.08712546,  0.15772569, ..., -0.14893088,\n",
       "         0.23994647,  0.0624648 ],\n",
       "       ...,\n",
       "       [-0.19072099, -0.07706731, -0.15062037, ..., -0.06154261,\n",
       "        -0.15578488, -0.09799255],\n",
       "       [ 0.06637164,  0.10466282, -0.04387826, ..., -0.18383834,\n",
       "        -0.11610316,  0.04478766],\n",
       "       [-0.1482015 , -0.19183478, -0.16351864, ...,  0.13500232,\n",
       "        -0.13920769, -0.09762202]], dtype=float32)), (dense_1/bias, array([-2.3743953e-05,  2.7665976e-03,  5.3337816e-02,  3.8536422e-02,\n",
       "        3.8001373e-02, -1.1353919e-02,  6.2541883e-03, -3.2378972e-02,\n",
       "       -1.7136516e-02, -2.5408937e-02], dtype=float32))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0628 18:16:37.998084 4439864768 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')\n",
    "non_federated_model.set_weights(state.model.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948]\n",
      "[0.742772424017791, 0.7492307692307693, 0.8375510204081633, 0.8439597315436241, 0.8300764655904843, 0.8011996572407883, 0.7648809523809523, 0.7786438035853468, 0.7712121212121212, 0.7638347622759158]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# Test the success of the attack.\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "\n",
    "mia_accuracy = []\n",
    "mia_precision = []\n",
    "\n",
    "for c in range(CLASSES):\n",
    "    target_indices = [i for i, d in enumerate(np.argmax(y, axis=1)) if d == c]\n",
    "    test_indices = [i for i, d in enumerate(np.argmax(attacker_y_test, axis=1)) if d == c]\n",
    "    data_in = [X[target_indices], y[target_indices]]\n",
    "    data_out = [[attacker_X_test[test_indices]], attacker_y_test[test_indices]]\n",
    "\n",
    "    # Compile them into the expected format for the AttackModelBundle.\n",
    "    attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "        target_model, data_in, data_out\n",
    "    )\n",
    "\n",
    "    # Compute the attack accuracy.\n",
    "    attack_guesses = amb.predict(attack_test_data)\n",
    "    attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "    mia_accuracy.append(attack_accuracy)\n",
    "    mia_precision.append(precision_score(real_membership_labels, attack_guesses))\n",
    "print(mia_accuracy)\n",
    "print(mia_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('Log/MIA/rE2C2.txt', 'w') as log:\n",
    "        print(\"rE2C2acc = {}\".format(mia_accuracy), file=log)\n",
    "        print(\"rE2C2pre = {}\".format(mia_precision), file=log)\n",
    "                \n",
    "except IOError:\n",
    "    print('File Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167537343955065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.7695852534562212, 0.7829560585885486, 0.8656618610747051, 0.8687206965840589, 0.8522954091816367, 0.8390646492434664, 0.7910643889618922, 0.807277628032345, 0.7960396039603961, 0.7948717948717948])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7883361707485956"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([0.742772424017791, 0.7492307692307693, 0.8375510204081633, 0.8439597315436241, 0.8300764655904843, 0.8011996572407883, 0.7648809523809523, 0.7786438035853468, 0.7712121212121212, 0.7638347622759158])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 7, 8, 9])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Custom Federated Algorithms, Part 2: Implementing Federated Averaging",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
