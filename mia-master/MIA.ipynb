{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mia.estimators import ShadowModelBundle, AttackModelBundle, prepare_attack_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "HEIGHT = 32\n",
    "CHANNELS = 3\n",
    "SHADOW_DATASET_SIZE = 1000\n",
    "ATTACK_TEST_DATASET_SIZE = 4000\n",
    "\n",
    "\n",
    "target_epochs = 12\n",
    "attack_epochs = 12\n",
    "num_shadows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \"\"\"Prepare CIFAR10 data.\"\"\"\n",
    "    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    X_train = X_train.astype(\"float32\")[:10000]\n",
    "    X_test = X_test.astype(\"float32\")\n",
    "    y_train = y_train.astype(\"float32\")[:10000]\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def target_model_fn():\n",
    "    \"\"\"The architecture of the target (victim) model.\n",
    "\n",
    "    The attack is white-box, hence the attacker is assumed to know this architecture too.\"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv2D(\n",
    "            32,\n",
    "            (3, 3),\n",
    "            activation=\"tanh\",\n",
    "            padding=\"same\",\n",
    "            input_shape=(WIDTH, HEIGHT, CHANNELS),\n",
    "        )\n",
    "    )\n",
    "    #model.add(layers.Conv2D(32, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\", padding=\"same\"))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation=\"tanh\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(layers.Dropout(0.25))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"tanh\"))\n",
    "    #model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    model.compile(\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def attack_model_fn():\n",
    "    \"\"\"Attack model that takes target model predictions and predicts membership.\n",
    "\n",
    "    Following the original paper, this attack model is specific to the class of the input.\n",
    "    AttachModelBundle creates multiple instances of this model for each class.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(128, activation=\"relu\", input_shape=(NUM_CLASSES,)))\n",
    "\n",
    "    model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "    model.add(layers.Dense(64, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the target model...\n",
      "Epoch 1/12\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 1.6906 - acc: 0.3976\n",
      "Epoch 2/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 1.3236 - acc: 0.5404\n",
      "Epoch 3/12\n",
      "10000/10000 [==============================] - 14s 1ms/sample - loss: 1.1436 - acc: 0.5978\n",
      "Epoch 4/12\n",
      "10000/10000 [==============================] - 13s 1ms/sample - loss: 0.9770 - acc: 0.6657\n",
      "Epoch 5/12\n",
      "10000/10000 [==============================] - 16s 2ms/sample - loss: 0.8551 - acc: 0.7004\n",
      "Epoch 6/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.7194 - acc: 0.7525\n",
      "Epoch 7/12\n",
      "10000/10000 [==============================] - 12s 1ms/sample - loss: 0.5633 - acc: 0.8112\n",
      "Epoch 8/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.4366 - acc: 0.8611\n",
      "Epoch 9/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.3003 - acc: 0.9140\n",
      "Epoch 10/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1877 - acc: 0.9577\n",
      "Epoch 11/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.1075 - acc: 0.9827\n",
      "Epoch 12/12\n",
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.0644 - acc: 0.9929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a72c4d358>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = get_data()\n",
    "\n",
    "# Train the target model.\n",
    "print(\"Training the target model...\")\n",
    "target_model = target_model_fn()\n",
    "target_model.fit(X_train, y_train, epochs=target_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.save(\"target_cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "target_model = load_model('target_cifar10.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3) (5000, 32, 32, 3)\n",
      "Training the shadow models...\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.0989 - acc: 0.2590 - val_loss: 1.9415 - val_acc: 0.3164\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7077 - acc: 0.4200 - val_loss: 1.7723 - val_acc: 0.3732\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4389 - acc: 0.5340 - val_loss: 1.7382 - val_acc: 0.3788\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2909 - acc: 0.5650 - val_loss: 1.7032 - val_acc: 0.4096\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1076 - acc: 0.6390 - val_loss: 1.7638 - val_acc: 0.3940\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9691 - acc: 0.6810 - val_loss: 1.7947 - val_acc: 0.4008\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8605 - acc: 0.7280 - val_loss: 1.9150 - val_acc: 0.3950\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7473 - acc: 0.7430 - val_loss: 1.9584 - val_acc: 0.3836\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5877 - acc: 0.8220 - val_loss: 1.9785 - val_acc: 0.3970\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4987 - acc: 0.8580 - val_loss: 2.0629 - val_acc: 0.3942\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3975 - acc: 0.9020 - val_loss: 2.0822 - val_acc: 0.4050\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2989 - acc: 0.9280 - val_loss: 2.1634 - val_acc: 0.4108\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1629 - acc: 0.2190 - val_loss: 1.9986 - val_acc: 0.2770\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7043 - acc: 0.4060 - val_loss: 1.7829 - val_acc: 0.3812\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4799 - acc: 0.4940 - val_loss: 1.7990 - val_acc: 0.3652\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3044 - acc: 0.5470 - val_loss: 1.6959 - val_acc: 0.4114\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1371 - acc: 0.6250 - val_loss: 1.7481 - val_acc: 0.3932\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9851 - acc: 0.6760 - val_loss: 1.8955 - val_acc: 0.3862\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8797 - acc: 0.7280 - val_loss: 1.7766 - val_acc: 0.4190\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7161 - acc: 0.7860 - val_loss: 1.8700 - val_acc: 0.3960\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5937 - acc: 0.8230 - val_loss: 1.9139 - val_acc: 0.4080\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4915 - acc: 0.8540 - val_loss: 2.0033 - val_acc: 0.4132\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3784 - acc: 0.9030 - val_loss: 2.0785 - val_acc: 0.4088\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2748 - acc: 0.9400 - val_loss: 2.1296 - val_acc: 0.4126\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1366 - acc: 0.2420 - val_loss: 1.9137 - val_acc: 0.2954\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.6409 - acc: 0.4330 - val_loss: 1.7385 - val_acc: 0.3726\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4292 - acc: 0.5300 - val_loss: 1.7941 - val_acc: 0.3594\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2958 - acc: 0.5550 - val_loss: 1.7328 - val_acc: 0.3962\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0889 - acc: 0.6350 - val_loss: 1.7274 - val_acc: 0.4070\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9299 - acc: 0.6920 - val_loss: 1.7801 - val_acc: 0.3994\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7417 - acc: 0.7790 - val_loss: 1.7785 - val_acc: 0.4086\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5765 - acc: 0.8340 - val_loss: 1.9108 - val_acc: 0.4036\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4602 - acc: 0.8660 - val_loss: 1.9443 - val_acc: 0.4142\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3447 - acc: 0.9180 - val_loss: 2.0995 - val_acc: 0.3996\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2518 - acc: 0.9570 - val_loss: 2.1295 - val_acc: 0.4112\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1826 - acc: 0.9740 - val_loss: 2.1744 - val_acc: 0.4186\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1746 - acc: 0.2420 - val_loss: 1.8953 - val_acc: 0.3050\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.6793 - acc: 0.4080 - val_loss: 1.8310 - val_acc: 0.3330\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4788 - acc: 0.4870 - val_loss: 1.7302 - val_acc: 0.3940\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2904 - acc: 0.5770 - val_loss: 1.7333 - val_acc: 0.3942\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.1141 - acc: 0.6280 - val_loss: 1.7322 - val_acc: 0.3972\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9850 - acc: 0.6820 - val_loss: 1.8259 - val_acc: 0.3896\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8156 - acc: 0.7460 - val_loss: 1.7378 - val_acc: 0.4248\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6132 - acc: 0.8170 - val_loss: 1.8775 - val_acc: 0.4140\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5190 - acc: 0.8560 - val_loss: 2.0011 - val_acc: 0.4032\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3724 - acc: 0.9180 - val_loss: 1.9720 - val_acc: 0.4182\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2631 - acc: 0.9500 - val_loss: 2.0773 - val_acc: 0.4152\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1796 - acc: 0.9730 - val_loss: 2.1563 - val_acc: 0.4294\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1832 - acc: 0.2300 - val_loss: 1.9003 - val_acc: 0.3162\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.7193 - acc: 0.4100 - val_loss: 1.7881 - val_acc: 0.3608\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4227 - acc: 0.5360 - val_loss: 1.7510 - val_acc: 0.3894\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2525 - acc: 0.5860 - val_loss: 1.8889 - val_acc: 0.3554\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1199 - acc: 0.6290 - val_loss: 1.8620 - val_acc: 0.3646\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9641 - acc: 0.6780 - val_loss: 1.8284 - val_acc: 0.3896\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7813 - acc: 0.7630 - val_loss: 1.9329 - val_acc: 0.4068\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6791 - acc: 0.7900 - val_loss: 2.0106 - val_acc: 0.3880\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5542 - acc: 0.8360 - val_loss: 1.9852 - val_acc: 0.4024\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3791 - acc: 0.9110 - val_loss: 2.0678 - val_acc: 0.4038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2846 - acc: 0.9390 - val_loss: 2.0801 - val_acc: 0.4186\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2082 - acc: 0.9600 - val_loss: 2.1954 - val_acc: 0.4102\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.0579 - acc: 0.2760 - val_loss: 1.8805 - val_acc: 0.3350\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5731 - acc: 0.4600 - val_loss: 1.8106 - val_acc: 0.3588\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3722 - acc: 0.5320 - val_loss: 1.7855 - val_acc: 0.3668\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.2248 - acc: 0.5820 - val_loss: 1.7458 - val_acc: 0.3938\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9940 - acc: 0.6760 - val_loss: 1.8404 - val_acc: 0.3884\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8531 - acc: 0.7200 - val_loss: 1.7843 - val_acc: 0.4032\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7112 - acc: 0.7660 - val_loss: 1.8420 - val_acc: 0.4092\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5508 - acc: 0.8330 - val_loss: 1.9246 - val_acc: 0.3938\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4020 - acc: 0.9050 - val_loss: 1.9202 - val_acc: 0.4260\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2930 - acc: 0.9400 - val_loss: 2.0283 - val_acc: 0.4162\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2249 - acc: 0.9520 - val_loss: 2.2237 - val_acc: 0.3992\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.1786 - acc: 0.9730 - val_loss: 2.2215 - val_acc: 0.4102\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.2129 - acc: 0.2200 - val_loss: 1.9219 - val_acc: 0.3264\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7290 - acc: 0.4120 - val_loss: 1.7774 - val_acc: 0.3654\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5021 - acc: 0.4940 - val_loss: 1.7657 - val_acc: 0.3704\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3225 - acc: 0.5640 - val_loss: 1.7255 - val_acc: 0.4006\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1558 - acc: 0.6010 - val_loss: 1.7773 - val_acc: 0.3956\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.9535 - acc: 0.6890 - val_loss: 1.8044 - val_acc: 0.3884\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8308 - acc: 0.7340 - val_loss: 1.8164 - val_acc: 0.4144\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7161 - acc: 0.7730 - val_loss: 1.9376 - val_acc: 0.4036\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5788 - acc: 0.8350 - val_loss: 1.9852 - val_acc: 0.4054\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4554 - acc: 0.8890 - val_loss: 2.0395 - val_acc: 0.3944\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3148 - acc: 0.9300 - val_loss: 2.1743 - val_acc: 0.4010\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2357 - acc: 0.9580 - val_loss: 2.1852 - val_acc: 0.4004\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.1421 - acc: 0.2170 - val_loss: 1.9181 - val_acc: 0.3258\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7233 - acc: 0.3930 - val_loss: 1.7380 - val_acc: 0.3846\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.5105 - acc: 0.4820 - val_loss: 1.6941 - val_acc: 0.4086\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3293 - acc: 0.5410 - val_loss: 1.7053 - val_acc: 0.4058\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1523 - acc: 0.6300 - val_loss: 1.7856 - val_acc: 0.3960\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0420 - acc: 0.6770 - val_loss: 1.8000 - val_acc: 0.3958\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8751 - acc: 0.7310 - val_loss: 1.7972 - val_acc: 0.4106\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7084 - acc: 0.7710 - val_loss: 1.8623 - val_acc: 0.3982\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5649 - acc: 0.8400 - val_loss: 1.8855 - val_acc: 0.4106\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4689 - acc: 0.8780 - val_loss: 2.0160 - val_acc: 0.4074\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3497 - acc: 0.9260 - val_loss: 2.0838 - val_acc: 0.3978\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2816 - acc: 0.9410 - val_loss: 2.2102 - val_acc: 0.3948\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 2.3107 - acc: 0.1950 - val_loss: 2.0021 - val_acc: 0.2778\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.7701 - acc: 0.3880 - val_loss: 1.7611 - val_acc: 0.3794\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4868 - acc: 0.4950 - val_loss: 1.7162 - val_acc: 0.3842\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.3136 - acc: 0.5430 - val_loss: 1.7103 - val_acc: 0.3974\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.1273 - acc: 0.6280 - val_loss: 1.7265 - val_acc: 0.3968\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.0055 - acc: 0.6740 - val_loss: 1.7481 - val_acc: 0.3996\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8351 - acc: 0.7360 - val_loss: 1.7855 - val_acc: 0.4028\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6701 - acc: 0.8050 - val_loss: 1.8453 - val_acc: 0.4090\n",
      "Epoch 9/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5451 - acc: 0.8500 - val_loss: 1.9101 - val_acc: 0.4058\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4151 - acc: 0.8910 - val_loss: 2.0632 - val_acc: 0.3854\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3764 - acc: 0.8890 - val_loss: 2.1156 - val_acc: 0.3938\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2319 - acc: 0.9660 - val_loss: 2.1808 - val_acc: 0.4040\n",
      "Train on 1000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      "1000/1000 [==============================] - 8s 8ms/sample - loss: 2.2566 - acc: 0.2150 - val_loss: 1.9986 - val_acc: 0.2876\n",
      "Epoch 2/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 1.7209 - acc: 0.3700 - val_loss: 1.8117 - val_acc: 0.3380\n",
      "Epoch 3/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 1.4728 - acc: 0.4780 - val_loss: 1.7488 - val_acc: 0.3820\n",
      "Epoch 4/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 1.3065 - acc: 0.5460 - val_loss: 1.8395 - val_acc: 0.3634\n",
      "Epoch 5/12\n",
      "1000/1000 [==============================] - 6s 6ms/sample - loss: 1.1465 - acc: 0.6080 - val_loss: 1.8351 - val_acc: 0.3602\n",
      "Epoch 6/12\n",
      "1000/1000 [==============================] - 4s 4ms/sample - loss: 0.9757 - acc: 0.6880 - val_loss: 1.8086 - val_acc: 0.3950\n",
      "Epoch 7/12\n",
      "1000/1000 [==============================] - 5s 5ms/sample - loss: 0.8307 - acc: 0.7220 - val_loss: 1.8769 - val_acc: 0.3840\n",
      "Epoch 8/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6907 - acc: 0.7830 - val_loss: 1.9333 - val_acc: 0.3778\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.5708 - acc: 0.8230 - val_loss: 1.9500 - val_acc: 0.3976\n",
      "Epoch 10/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.4331 - acc: 0.8910 - val_loss: 2.0101 - val_acc: 0.4030\n",
      "Epoch 11/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.3280 - acc: 0.9260 - val_loss: 2.1256 - val_acc: 0.3990\n",
      "Epoch 12/12\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.2354 - acc: 0.9600 - val_loss: 2.1638 - val_acc: 0.4028\n"
     ]
    }
   ],
   "source": [
    "# Train the shadow models.\n",
    "smb = ShadowModelBundle(\n",
    "    target_model_fn,\n",
    "    shadow_dataset_size=SHADOW_DATASET_SIZE,\n",
    "    num_models=num_shadows\n",
    ")\n",
    "\n",
    "# We assume that attacker's data were not seen in target's training.\n",
    "attacker_X_train, attacker_X_test, attacker_y_train, attacker_y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5)\n",
    "print(attacker_X_train.shape, attacker_X_test.shape)\n",
    "\n",
    "print(\"Training the shadow models...\")\n",
    "X_shadow, y_shadow = smb.fit_transform(\n",
    "    attacker_X_train,\n",
    "    attacker_y_train,\n",
    "    fit_kwargs=dict(\n",
    "        epochs=target_epochs,\n",
    "        verbose=True,\n",
    "        validation_data=(attacker_X_test, attacker_y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 01:14:13.797880 4659361216 deprecation.py:506] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the attack models...\n",
      "Epoch 1/12\n",
      "1999/1999 [==============================] - 1s 502us/sample - loss: 0.6137 - acc: 0.6918\n",
      "Epoch 2/12\n",
      "1999/1999 [==============================] - 0s 116us/sample - loss: 0.5149 - acc: 0.7359\n",
      "Epoch 3/12\n",
      "1999/1999 [==============================] - 0s 71us/sample - loss: 0.5096 - acc: 0.7434\n",
      "Epoch 4/12\n",
      "1999/1999 [==============================] - 0s 73us/sample - loss: 0.5095 - acc: 0.7379\n",
      "Epoch 5/12\n",
      "1999/1999 [==============================] - 0s 75us/sample - loss: 0.5028 - acc: 0.7444\n",
      "Epoch 6/12\n",
      "1999/1999 [==============================] - 0s 92us/sample - loss: 0.5034 - acc: 0.7404\n",
      "Epoch 7/12\n",
      "1999/1999 [==============================] - 0s 72us/sample - loss: 0.5046 - acc: 0.7399\n",
      "Epoch 8/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.5018 - acc: 0.7404\n",
      "Epoch 9/12\n",
      "1999/1999 [==============================] - 0s 66us/sample - loss: 0.5041 - acc: 0.7414\n",
      "Epoch 10/12\n",
      "1999/1999 [==============================] - 0s 93us/sample - loss: 0.4997 - acc: 0.7404\n",
      "Epoch 11/12\n",
      "1999/1999 [==============================] - 0s 65us/sample - loss: 0.5013 - acc: 0.7454\n",
      "Epoch 12/12\n",
      "1999/1999 [==============================] - 0s 67us/sample - loss: 0.4961 - acc: 0.7394\n",
      "Epoch 1/12\n",
      "2035/2035 [==============================] - 1s 260us/sample - loss: 0.6144 - acc: 0.6747\n",
      "Epoch 2/12\n",
      "2035/2035 [==============================] - 0s 73us/sample - loss: 0.4817 - acc: 0.7617\n",
      "Epoch 3/12\n",
      "2035/2035 [==============================] - 0s 72us/sample - loss: 0.4690 - acc: 0.7666\n",
      "Epoch 4/12\n",
      "2035/2035 [==============================] - 0s 69us/sample - loss: 0.4692 - acc: 0.7705\n",
      "Epoch 5/12\n",
      "2035/2035 [==============================] - 0s 72us/sample - loss: 0.4653 - acc: 0.7740\n",
      "Epoch 6/12\n",
      "2035/2035 [==============================] - 0s 116us/sample - loss: 0.4640 - acc: 0.7725\n",
      "Epoch 7/12\n",
      "2035/2035 [==============================] - 0s 68us/sample - loss: 0.4638 - acc: 0.7686\n",
      "Epoch 8/12\n",
      "2035/2035 [==============================] - 0s 62us/sample - loss: 0.4645 - acc: 0.7740\n",
      "Epoch 9/12\n",
      "2035/2035 [==============================] - 0s 65us/sample - loss: 0.4591 - acc: 0.7744\n",
      "Epoch 10/12\n",
      "2035/2035 [==============================] - 0s 86us/sample - loss: 0.4635 - acc: 0.7735\n",
      "Epoch 11/12\n",
      "2035/2035 [==============================] - 0s 92us/sample - loss: 0.4635 - acc: 0.7705\n",
      "Epoch 12/12\n",
      "2035/2035 [==============================] - 0s 82us/sample - loss: 0.4590 - acc: 0.7749\n",
      "Epoch 1/12\n",
      "1961/1961 [==============================] - 1s 295us/sample - loss: 0.5620 - acc: 0.7613\n",
      "Epoch 2/12\n",
      "1961/1961 [==============================] - 0s 80us/sample - loss: 0.4611 - acc: 0.7950\n",
      "Epoch 3/12\n",
      "1961/1961 [==============================] - 0s 103us/sample - loss: 0.4533 - acc: 0.7970\n",
      "Epoch 4/12\n",
      "1961/1961 [==============================] - 0s 125us/sample - loss: 0.4454 - acc: 0.7996\n",
      "Epoch 5/12\n",
      "1961/1961 [==============================] - 0s 101us/sample - loss: 0.4466 - acc: 0.8011\n",
      "Epoch 6/12\n",
      "1961/1961 [==============================] - 0s 83us/sample - loss: 0.4483 - acc: 0.7996\n",
      "Epoch 7/12\n",
      "1961/1961 [==============================] - 0s 92us/sample - loss: 0.4447 - acc: 0.8006\n",
      "Epoch 8/12\n",
      "1961/1961 [==============================] - 0s 79us/sample - loss: 0.4476 - acc: 0.8057\n",
      "Epoch 9/12\n",
      "1961/1961 [==============================] - 0s 108us/sample - loss: 0.4433 - acc: 0.8021\n",
      "Epoch 10/12\n",
      "1961/1961 [==============================] - 0s 137us/sample - loss: 0.4436 - acc: 0.7996\n",
      "Epoch 11/12\n",
      "1961/1961 [==============================] - 0s 75us/sample - loss: 0.4380 - acc: 0.8021\n",
      "Epoch 12/12\n",
      "1961/1961 [==============================] - 0s 175us/sample - loss: 0.4463 - acc: 0.8021\n",
      "Epoch 1/12\n",
      "2020/2020 [==============================] - 1s 408us/sample - loss: 0.5181 - acc: 0.8163\n",
      "Epoch 2/12\n",
      "2020/2020 [==============================] - 0s 102us/sample - loss: 0.3930 - acc: 0.8347\n",
      "Epoch 3/12\n",
      "2020/2020 [==============================] - 0s 94us/sample - loss: 0.3861 - acc: 0.8287\n",
      "Epoch 4/12\n",
      "2020/2020 [==============================] - 0s 94us/sample - loss: 0.3869 - acc: 0.8347\n",
      "Epoch 5/12\n",
      "2020/2020 [==============================] - 0s 107us/sample - loss: 0.3876 - acc: 0.8337\n",
      "Epoch 6/12\n",
      "2020/2020 [==============================] - 0s 81us/sample - loss: 0.3847 - acc: 0.8342\n",
      "Epoch 7/12\n",
      "2020/2020 [==============================] - 0s 80us/sample - loss: 0.3842 - acc: 0.8332\n",
      "Epoch 8/12\n",
      "2020/2020 [==============================] - 0s 78us/sample - loss: 0.3910 - acc: 0.8342\n",
      "Epoch 9/12\n",
      "2020/2020 [==============================] - 0s 78us/sample - loss: 0.3820 - acc: 0.8356\n",
      "Epoch 10/12\n",
      "2020/2020 [==============================] - 0s 72us/sample - loss: 0.3836 - acc: 0.8371\n",
      "Epoch 11/12\n",
      "2020/2020 [==============================] - 0s 73us/sample - loss: 0.3831 - acc: 0.8376\n",
      "Epoch 12/12\n",
      "2020/2020 [==============================] - 0s 72us/sample - loss: 0.3824 - acc: 0.8351\n",
      "Epoch 1/12\n",
      "1958/1958 [==============================] - 1s 309us/sample - loss: 0.5856 - acc: 0.7252\n",
      "Epoch 2/12\n",
      "1958/1958 [==============================] - 0s 86us/sample - loss: 0.4966 - acc: 0.7630\n",
      "Epoch 3/12\n",
      "1958/1958 [==============================] - 0s 72us/sample - loss: 0.4925 - acc: 0.7625\n",
      "Epoch 4/12\n",
      "1958/1958 [==============================] - 0s 70us/sample - loss: 0.4887 - acc: 0.7594\n",
      "Epoch 5/12\n",
      "1958/1958 [==============================] - 0s 72us/sample - loss: 0.4808 - acc: 0.7635\n",
      "Epoch 6/12\n",
      "1958/1958 [==============================] - 0s 83us/sample - loss: 0.4823 - acc: 0.7605\n",
      "Epoch 7/12\n",
      "1958/1958 [==============================] - 0s 136us/sample - loss: 0.4791 - acc: 0.7625\n",
      "Epoch 8/12\n",
      "1958/1958 [==============================] - 0s 86us/sample - loss: 0.4798 - acc: 0.7589\n",
      "Epoch 9/12\n",
      "1958/1958 [==============================] - 0s 78us/sample - loss: 0.4784 - acc: 0.7564\n",
      "Epoch 10/12\n",
      "1958/1958 [==============================] - 0s 86us/sample - loss: 0.4769 - acc: 0.7584\n",
      "Epoch 11/12\n",
      "1958/1958 [==============================] - 0s 80us/sample - loss: 0.4756 - acc: 0.7646\n",
      "Epoch 12/12\n",
      "1958/1958 [==============================] - 0s 67us/sample - loss: 0.4717 - acc: 0.7625\n",
      "Epoch 1/12\n",
      "2133/2133 [==============================] - 1s 276us/sample - loss: 0.5622 - acc: 0.7459\n",
      "Epoch 2/12\n",
      "2133/2133 [==============================] - 0s 68us/sample - loss: 0.4594 - acc: 0.7900\n",
      "Epoch 3/12\n",
      "2133/2133 [==============================] - 0s 69us/sample - loss: 0.4508 - acc: 0.7876\n",
      "Epoch 4/12\n",
      "2133/2133 [==============================] - 0s 64us/sample - loss: 0.4474 - acc: 0.7909\n",
      "Epoch 5/12\n",
      "2133/2133 [==============================] - 0s 67us/sample - loss: 0.4444 - acc: 0.7937\n",
      "Epoch 6/12\n",
      "2133/2133 [==============================] - 0s 68us/sample - loss: 0.4446 - acc: 0.7890\n",
      "Epoch 7/12\n",
      "2133/2133 [==============================] - 0s 67us/sample - loss: 0.4403 - acc: 0.7942\n",
      "Epoch 8/12\n",
      "2133/2133 [==============================] - 0s 67us/sample - loss: 0.4413 - acc: 0.7932\n",
      "Epoch 9/12\n",
      "2133/2133 [==============================] - 0s 67us/sample - loss: 0.4443 - acc: 0.7937\n",
      "Epoch 10/12\n",
      "2133/2133 [==============================] - 0s 68us/sample - loss: 0.4397 - acc: 0.7947\n",
      "Epoch 11/12\n",
      "2133/2133 [==============================] - 0s 66us/sample - loss: 0.4391 - acc: 0.7951\n",
      "Epoch 12/12\n",
      "2133/2133 [==============================] - 0s 67us/sample - loss: 0.4373 - acc: 0.7979\n",
      "Epoch 1/12\n",
      "1974/1974 [==============================] - 1s 286us/sample - loss: 0.6084 - acc: 0.6960\n",
      "Epoch 2/12\n",
      "1974/1974 [==============================] - 0s 67us/sample - loss: 0.5324 - acc: 0.7234\n",
      "Epoch 3/12\n",
      "1974/1974 [==============================] - 0s 68us/sample - loss: 0.5194 - acc: 0.7335\n",
      "Epoch 4/12\n",
      "1974/1974 [==============================] - 0s 131us/sample - loss: 0.5181 - acc: 0.7391\n",
      "Epoch 5/12\n",
      "1974/1974 [==============================] - 0s 95us/sample - loss: 0.5164 - acc: 0.7371\n",
      "Epoch 6/12\n",
      "1974/1974 [==============================] - 0s 216us/sample - loss: 0.5173 - acc: 0.7330s - loss: 0.5292 - acc:\n",
      "Epoch 7/12\n",
      "1974/1974 [==============================] - 0s 226us/sample - loss: 0.5169 - acc: 0.7335\n",
      "Epoch 8/12\n",
      "1974/1974 [==============================] - 0s 133us/sample - loss: 0.5195 - acc: 0.7315\n",
      "Epoch 9/12\n",
      "1974/1974 [==============================] - 0s 131us/sample - loss: 0.5132 - acc: 0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12\n",
      "1974/1974 [==============================] - 0s 115us/sample - loss: 0.5116 - acc: 0.7366\n",
      "Epoch 11/12\n",
      "1974/1974 [==============================] - 0s 97us/sample - loss: 0.5131 - acc: 0.7406\n",
      "Epoch 12/12\n",
      "1974/1974 [==============================] - 0s 143us/sample - loss: 0.5149 - acc: 0.7366\n",
      "Epoch 1/12\n",
      "2033/2033 [==============================] - 1s 437us/sample - loss: 0.5992 - acc: 0.7226\n",
      "Epoch 2/12\n",
      "2033/2033 [==============================] - 0s 76us/sample - loss: 0.5087 - acc: 0.7467\n",
      "Epoch 3/12\n",
      "2033/2033 [==============================] - 0s 64us/sample - loss: 0.5005 - acc: 0.7521\n",
      "Epoch 4/12\n",
      "2033/2033 [==============================] - 0s 64us/sample - loss: 0.5025 - acc: 0.7526\n",
      "Epoch 5/12\n",
      "2033/2033 [==============================] - 0s 64us/sample - loss: 0.5001 - acc: 0.7541\n",
      "Epoch 6/12\n",
      "2033/2033 [==============================] - 0s 71us/sample - loss: 0.5003 - acc: 0.7501\n",
      "Epoch 7/12\n",
      "2033/2033 [==============================] - 0s 66us/sample - loss: 0.4994 - acc: 0.7531\n",
      "Epoch 8/12\n",
      "2033/2033 [==============================] - 0s 66us/sample - loss: 0.4983 - acc: 0.7511\n",
      "Epoch 9/12\n",
      "2033/2033 [==============================] - 0s 66us/sample - loss: 0.4937 - acc: 0.7501\n",
      "Epoch 10/12\n",
      "2033/2033 [==============================] - 0s 63us/sample - loss: 0.4944 - acc: 0.7506\n",
      "Epoch 11/12\n",
      "2033/2033 [==============================] - 0s 65us/sample - loss: 0.4968 - acc: 0.7536\n",
      "Epoch 12/12\n",
      "2033/2033 [==============================] - 0s 67us/sample - loss: 0.4942 - acc: 0.7516\n",
      "Epoch 1/12\n",
      "1877/1877 [==============================] - 1s 344us/sample - loss: 0.5991 - acc: 0.7086\n",
      "Epoch 2/12\n",
      "1877/1877 [==============================] - 0s 69us/sample - loss: 0.4934 - acc: 0.7656\n",
      "Epoch 3/12\n",
      "1877/1877 [==============================] - 0s 68us/sample - loss: 0.4848 - acc: 0.7704\n",
      "Epoch 4/12\n",
      "1877/1877 [==============================] - 0s 68us/sample - loss: 0.4785 - acc: 0.7720\n",
      "Epoch 5/12\n",
      "1877/1877 [==============================] - 0s 78us/sample - loss: 0.4747 - acc: 0.7704\n",
      "Epoch 6/12\n",
      "1877/1877 [==============================] - 0s 83us/sample - loss: 0.4761 - acc: 0.7752\n",
      "Epoch 7/12\n",
      "1877/1877 [==============================] - 0s 78us/sample - loss: 0.4770 - acc: 0.7720\n",
      "Epoch 8/12\n",
      "1877/1877 [==============================] - 0s 85us/sample - loss: 0.4728 - acc: 0.7677\n",
      "Epoch 9/12\n",
      "1877/1877 [==============================] - 0s 90us/sample - loss: 0.4726 - acc: 0.7688\n",
      "Epoch 10/12\n",
      "1877/1877 [==============================] - 0s 81us/sample - loss: 0.4740 - acc: 0.7666\n",
      "Epoch 11/12\n",
      "1877/1877 [==============================] - 0s 78us/sample - loss: 0.4700 - acc: 0.7725\n",
      "Epoch 12/12\n",
      "1877/1877 [==============================] - 0s 90us/sample - loss: 0.4716 - acc: 0.7698\n",
      "Epoch 1/12\n",
      "2010/2010 [==============================] - 2s 821us/sample - loss: 0.5734 - acc: 0.7383\n",
      "Epoch 2/12\n",
      "2010/2010 [==============================] - 0s 106us/sample - loss: 0.4867 - acc: 0.7647\n",
      "Epoch 3/12\n",
      "2010/2010 [==============================] - 0s 149us/sample - loss: 0.4750 - acc: 0.7751\n",
      "Epoch 4/12\n",
      "2010/2010 [==============================] - 0s 96us/sample - loss: 0.4746 - acc: 0.7716\n",
      "Epoch 5/12\n",
      "2010/2010 [==============================] - 0s 102us/sample - loss: 0.4776 - acc: 0.7736\n",
      "Epoch 6/12\n",
      "2010/2010 [==============================] - 0s 98us/sample - loss: 0.4713 - acc: 0.7746\n",
      "Epoch 7/12\n",
      "2010/2010 [==============================] - 0s 98us/sample - loss: 0.4662 - acc: 0.7776\n",
      "Epoch 8/12\n",
      "2010/2010 [==============================] - 0s 111us/sample - loss: 0.4704 - acc: 0.7741\n",
      "Epoch 9/12\n",
      "2010/2010 [==============================] - 0s 94us/sample - loss: 0.4687 - acc: 0.7746\n",
      "Epoch 10/12\n",
      "2010/2010 [==============================] - 0s 97us/sample - loss: 0.4695 - acc: 0.7761\n",
      "Epoch 11/12\n",
      "2010/2010 [==============================] - 0s 78us/sample - loss: 0.4655 - acc: 0.7756\n",
      "Epoch 12/12\n",
      "2010/2010 [==============================] - 0s 76us/sample - loss: 0.4647 - acc: 0.7786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ShadowModelBundle returns data in the format suitable for the AttackModelBundle.\n",
    "amb = AttackModelBundle(attack_model_fn, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Fit the attack models.\n",
    "print(\"Training the attack models...\")\n",
    "amb.fit(\n",
    "    X_shadow, y_shadow, fit_kwargs=dict(epochs=attack_epochs, verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2c6f1b1bdb97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'non_federated_cifar10.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0moptimizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m       optimizer = optimizers.deserialize(\n\u001b[0;32m--> 249\u001b[0;31m           optimizer_config, custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0;31m# Recover loss functions and metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m       printable_module_name='optimizer')\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 list(custom_objects.items())))\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lr, beta_1, beta_2, epsilon, decay, amsgrad, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m                \u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                **kwargs):\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         raise TypeError('Unexpected keyword argument '\n\u001b[0;32m---> 68\u001b[0;31m                         'passed to optimizer: ' + str(k))\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;31m# checks that clipnorm >= 0 and clipvalue >= 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: name"
     ]
    }
   ],
   "source": [
    "target_model = load_model('non_federated_cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.52      0.68      4000\n",
      "        1.0       0.67      0.99      0.80      4000\n",
      "\n",
      "avg / total       0.83      0.76      0.74      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the success of the attack.\n",
    "\n",
    "# Prepare examples that were in the training, and out of the training.\n",
    "data_in = X_train[:ATTACK_TEST_DATASET_SIZE], y_train[:ATTACK_TEST_DATASET_SIZE]\n",
    "data_out = attacker_X_test[:ATTACK_TEST_DATASET_SIZE], attacker_y_test[:ATTACK_TEST_DATASET_SIZE]\n",
    "\n",
    "# Compile them into the expected format for the AttackModelBundle.\n",
    "attack_test_data, real_membership_labels = prepare_attack_data(\n",
    "    target_model, data_in, data_out\n",
    ")\n",
    "\n",
    "# Compute the attack accuracy.\n",
    "attack_guesses = amb.predict(attack_test_data)\n",
    "attack_accuracy = np.mean(attack_guesses == real_membership_labels)\n",
    "\n",
    "print(attack_accuracy)\n",
    "print (classification_report(real_membership_labels, attack_guesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
