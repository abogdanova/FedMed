Train size: 340, learning rate: 0.0001, num_epochs: 1500, minibatch_size: 32
[5.344176840782166, 1.7692303419113158, 1.7678334951400756, 1.7664470195770263, 1.7647822380065918, 1.7630166769027709, 1.7617828845977783, 1.7600133776664735, 1.7586076498031615, 1.7569512486457826, 1.7559133410453793, 1.7542962670326232, 1.7532982707023623, 1.7518562793731691, 1.7506065130233763, 1.7487023711204528, 1.747168838977814, 1.7458733320236204, 1.7452104926109315, 1.74316223859787, 1.7425939679145814, 1.7405457735061647, 1.7405479431152344, 1.7369170665740967, 1.736697280406952, 1.734848868846893, 1.7333945989608766, 1.73319286108017, 1.7325489997863772, 1.7304842114448549, 1.7291764616966248, 1.7291805028915406, 1.7261162519454956, 1.7277132034301756, 1.7248871803283692, 1.7228210210800172, 1.7220909118652346, 1.7199067950248719, 1.7183546781539918, 1.7183690309524537, 1.7201789259910583, 1.7177404522895812, 1.715145444869995, 1.7163122534751891, 1.7149218559265138, 1.712232279777527, 1.7115413069725036, 1.7132492065429688, 1.7102858185768124, 1.7092843055725098, 1.709450364112854, 1.7076170563697817, 1.7062113285064697, 1.7042689561843873, 1.7051934719085695, 1.7021137475967405, 1.7014951825141909, 1.7047698259353636, 1.7001844048500063, 1.6998406767845156, 1.698989737033844, 1.695150935649872, 1.6970907688140868, 1.6983699083328248, 1.6959271430969238, 1.6920682191848755, 1.6948256731033327, 1.6940421223640443, 1.6930718064308166, 1.6924765229225158, 1.6928377985954286, 1.6922841310501098, 1.6917142271995542, 1.6924317359924317, 1.6917193293571473, 1.6869217395782468, 1.6870184898376466, 1.6892172336578368, 1.685925269126892, 1.688471019268036, 1.6853610515594482, 1.6850665926933288, 1.6817137837409972, 1.6830085396766659, 1.6845205307006836, 1.6821722745895384, 1.680955994129181, 1.6813525319099423, 1.680933690071106, 1.6806741952896116, 1.6782568573951722, 1.6775189995765687, 1.6798010110855104, 1.6790006756782532, 1.6786861062049865, 1.6787948489189146, 1.674698293209076, 1.6773561239242554, 1.6749415040016173, 1.6750934362411498, 1.6753361582756043, 1.6730631113052368, 1.674472141265869, 1.6753270506858826, 1.674157440662384, 1.6729732394218444, 1.670731687545776, 1.6737398862838748, 1.674220108985901, 1.6709836602211, 1.673526585102081, 1.669272601604462, 1.6712430119514465, 1.670105504989624, 1.669729208946228, 1.6732742786407468, 1.671189343929291, 1.670226514339447, 1.6669677734375001, 1.6727567076683043, 1.6678772211074828, 1.6715215563774106, 1.6698185443878175, 1.6699899315834046, 1.6623374223709109, 1.6653595924377438, 1.6671460628509522, 1.6702641606330875, 1.668951940536499, 1.6641206383705143, 1.6708401083946227, 1.6659782409667971, 1.662434470653534, 1.6679325103759763, 1.6628203868865967, 1.6640979051589964, 1.6683600425720213, 1.668068563938141, 1.6675588369369507, 1.6666589736938475, 1.668052637577057, 1.6626457691192629, 1.661587083339691, 1.667615103721619, 1.6637881994247437, 1.6642749309539793, 1.6645683050155642, 1.6608872294425965, 1.6635200142860413, 1.66225106716156, 1.6618726849555967, 1.6629035115242001, 1.6573447108268735, 1.6631308794021606, 1.6607688188552856, 1.65854594707489, 1.6640756726264958, 1.656923162937164, 1.6590117216110232, 1.6638041615486143, 1.6575573801994326, 1.6600870370864869, 1.658238959312439, 1.6651617407798769, 1.6628631711006163, 1.6616223931312566, 1.6592418789863588, 1.6636576414108275, 1.6663634300231935, 1.6606875538825985, 1.6609275102615357, 1.6612710952758791, 1.6656973958015442, 1.6589010715484616, 1.6641383171081545, 1.6572639346122742, 1.6609098672866822, 1.6604748368263245, 1.6603514075279235, 1.6602705836296079, 1.6554347991943361, 1.6614670515060426, 1.6632743597030641, 1.658994448184967, 1.662581980228424, 1.6569477677345275, 1.657800769805908, 1.6577634692192078, 1.6584516167640686, 1.6595732212066652, 1.6551712512969967, 1.6539061784744262, 1.6539022207260132, 1.6560601592063904, 1.6493714570999147, 1.6556499123573303, 1.6572073221206667, 1.6583296895027158, 1.6559532165527344, 1.6596584081649781, 1.6567395210266114, 1.6579509854316712, 1.6631572604179383, 1.6579755425453186, 1.6590810537338256, 1.6618909955024719, 1.6589868307113647, 1.660212552547455, 1.659858131408691, 1.6579595685005188, 1.6644337773323057, 1.6590017557144163, 1.6588411569595336, 1.6550096631050109, 1.652724874019623, 1.6591745734214782, 1.6619810342788697, 1.6598956227302548, 1.6658297538757323, 1.6575764894485474, 1.6604163885116576, 1.6577688217163087, 1.6627864956855771, 1.655261528491974, 1.6584031224250793, 1.654948914051056, 1.6590270280838013, 1.657284641265869, 1.6599785208702087, 1.6557262778282165, 1.6572365164756775, 1.6570482969284057, 1.661008203029633, 1.6618781328201295, 1.6607764840126036, 1.647344160079956, 1.662530493736267, 1.654468059539795, 1.6602606415748598, 1.6617815136909484, 1.6557809829711914, 1.6599661111831665, 1.663889479637146, 1.6611572980880736, 1.6572641372680663, 1.6554659247398376, 1.653078842163086, 1.6506392717361453, 1.6548083782196046, 1.6533666610717772, 1.6572917938232423, 1.661268949508667, 1.653165888786316, 1.6541167616844177, 1.6628669619560241, 1.6532852053642273, 1.6570129752159117, 1.6554816365242007, 1.6610332012176516, 1.6599161267280576, 1.6581366419792176, 1.6568215131759645, 1.6567810773849487, 1.6617936968803406, 1.6569520950317382, 1.6585048913955691, 1.6569206118583677, 1.6579462051391602, 1.6554803013801578, 1.6578034996986388, 1.6540701985359194, 1.6543634057044985, 1.657814121246338, 1.654016363620758, 1.6581385016441346, 1.658368444442749, 1.6609512329101563, 1.6605599045753479, 1.6565428733825684, 1.652958428859711, 1.6579935550689693, 1.6530131459236146, 1.6579522371292112, 1.6615288257598877, 1.6597157716751096, 1.6584203720092772, 1.6584333539009095, 1.665952920913696, 1.6552202224731447, 1.6499477148056028, 1.6582341313362123, 1.656841456890106, 1.6658239960670471, 1.6602973103523255, 1.6590260744094847, 1.6576351761817933, 1.653111779689789, 1.6594739913940433, 1.6568377971649169, 1.6578930497169493]
Train Accuracy: 0.37058824
Test Accuracy: 0.36666667
