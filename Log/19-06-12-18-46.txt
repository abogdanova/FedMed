Train size: 340, learning rate: 0.0001, num_epochs: 1500, minibatch_size: 32
[4.460612034797668, 1.7684014797210694, 1.76645370721817, 1.76448872089386, 1.7621311664581298, 1.759571695327759, 1.7577773332595825, 1.7552050709724427, 1.7531886816024778, 1.7507539868354793, 1.7492035388946532, 1.746910512447357, 1.7452973127365115, 1.7432217240333556, 1.7414322853088384, 1.7385715246200562, 1.7363256096839905, 1.7343059062957762, 1.7333383798599244, 1.7303298950195314, 1.7293980836868286, 1.7263986229896549, 1.726396310329437, 1.720853340625763, 1.7204776167869567, 1.7175876736640932, 1.7154309153556824, 1.715045166015625, 1.7141924738883971, 1.7110384941101076, 1.7088845133781432, 1.7091328740119935, 1.7041504502296447, 1.7071106791496278, 1.7025525450706482, 1.699315059185028, 1.698172926902771, 1.6949021935462951, 1.6925854444503785, 1.6928712844848632, 1.696064865589142, 1.6922539234161376, 1.6882845044136046, 1.6903184413909913, 1.6884353518486024, 1.6842480540275573, 1.6834044456481931, 1.686736309528351, 1.682255280017853, 1.680847132205963, 1.6812620997428893, 1.6787400960922243, 1.677044892311096, 1.6742499589920044, 1.676025915145874, 1.6715577840805054, 1.6711628675460817, 1.6772041440010073, 1.6697209358215335, 1.670028638839722, 1.6692563295364382, 1.663286375999451, 1.667041563987732, 1.6699653983116152, 1.6662153720855712, 1.6605766654014589, 1.6658089756965637, 1.6654308080673217, 1.6643233537673952, 1.664057970046997, 1.6652577400207518, 1.6653979420661928, 1.6650686383247377, 1.6668102264404296, 1.6661911964416503, 1.6592199325561525, 1.6602470993995666, 1.6641891837120057, 1.6594059348106383, 1.664456856250763, 1.6601833462715148, 1.660412311553955, 1.6556801080703736, 1.6586125254631041, 1.6618542551994322, 1.6585415363311766, 1.6571479558944704, 1.6587683916091922, 1.6583470940589904, 1.659050714969635, 1.6559672355651855, 1.6556640505790714, 1.6596536278724672, 1.6588967800140382, 1.6592731714248654, 1.6601771473884583, 1.6541618227958679, 1.6590223789215088, 1.655532717704773, 1.6566248297691344, 1.6571043133735657, 1.6548724055290227, 1.6573815226554873, 1.6590892910957336, 1.6576909422874453, 1.6564929008483886, 1.6536389112472534, 1.6587433218955994, 1.6599681615829465, 1.6558985948562626, 1.6596623659133911, 1.6545026183128357, 1.657309639453888, 1.6561382412910461, 1.6560775518417359, 1.6615116119384767, 1.6593665957450867, 1.6582192778587344, 1.6540071964263916, 1.6622520565986634, 1.656497180461884, 1.661256790161133, 1.6594158053398131, 1.6595973134040833, 1.650108551979065, 1.6543081283569334, 1.656887483596802, 1.661970055103302, 1.6596689224243164, 1.653936409950256, 1.663434791564941, 1.6570310473442078, 1.6526150345802306, 1.659697437286377, 1.6536826729774474, 1.6554624676704408, 1.6608641266822814, 1.6607723712921143, 1.66060231924057, 1.6603469729423521, 1.6622410655021667, 1.6553313732147217, 1.653971481323242, 1.6613001346588132, 1.6576242327690125, 1.6580603361129762, 1.6591725945472717, 1.6538325548171997, 1.6579654216766355, 1.6563824057579042, 1.656107699871063, 1.657481634616852, 1.6511898279190063, 1.65744686126709, 1.6551551938056945, 1.6524457454681398, 1.659482729434967, 1.6514935731887819, 1.653704047203064, 1.659236752986908, 1.65235196352005, 1.6553106546401977, 1.6530616879463194, 1.6618308067321774, 1.6596524596214293, 1.6580886483192443, 1.6552490353584293, 1.6602517724037171, 1.6633244633674622, 1.6565712451934813, 1.6578008890151976, 1.657974684238434, 1.662934911251068, 1.6549939513206484, 1.6619489789009094, 1.6539570212364196, 1.6582958817481994, 1.6576187491416932, 1.6572740316390993, 1.6572530508041383, 1.652130115032196, 1.6592724561691286, 1.6610079169273377, 1.6564653754234315, 1.660665965080261, 1.653856098651886, 1.6553812861442567, 1.6548685908317564, 1.656475341320038, 1.6577302098274234, 1.6525579571723936, 1.6510253787040707, 1.6513538360595703, 1.6539541840553282, 1.646956789493561, 1.653895318508148, 1.6553274631500243, 1.6563020110130309, 1.653594696521759, 1.657784068584442, 1.6545286893844602, 1.6560936570167544, 1.6618271946907044, 1.65644371509552, 1.6576417803764345, 1.6608609795570375, 1.6581020474433898, 1.659402573108673, 1.6585824728012086, 1.6561708927154541, 1.663643801212311, 1.657784378528595, 1.657417047023773, 1.6537150502204894, 1.6511623978614804, 1.6580940842628478, 1.6607859373092653, 1.6588762879371641, 1.6657106518745421, 1.656313717365265, 1.6594937682151798, 1.6565723299980166, 1.6619562268257142, 1.654048192501068, 1.6572822093963622, 1.6537796854972842, 1.6579758167266847, 1.656860089302063, 1.6592027902603148, 1.654876685142517, 1.6566494941711425, 1.6562643527984617, 1.660743010044098, 1.6616273164749145, 1.6603279113769533, 1.6460675716400148, 1.6623458266258238, 1.6535537600517272, 1.6594843626022335, 1.6608583688735963, 1.6549448728561404, 1.659222662448883, 1.6636048316955567, 1.6608331918716432, 1.6565192341804504, 1.6550830364227296, 1.6523037910461424, 1.6498493075370788, 1.6540935039520264, 1.6524690985679626, 1.6568221807479857, 1.6608594298362733, 1.652488684654236, 1.6534709692001344, 1.6628356456756592, 1.6525120377540592, 1.6564255118370055, 1.655059289932251, 1.6608477711677552, 1.659727668762207, 1.6576012253761294, 1.6564550280570982, 1.65642249584198, 1.661793112754822, 1.6566155433654783, 1.6581867098808287, 1.6562866210937504, 1.6576358914375307, 1.654856812953949, 1.657654583454132, 1.653761088848114, 1.6539171338081358, 1.6575368881225587, 1.653736126422882, 1.65802983045578, 1.658125138282776, 1.6605865120887755, 1.660477662086487, 1.6563032746315003, 1.6524390459060667, 1.657633566856384, 1.652643024921417, 1.6578779101371768, 1.6614601731300356, 1.6593909502029418, 1.658106303215027, 1.6582522153854369, 1.666204524040222, 1.6550288796424866, 1.649494659900665, 1.6579346418380738, 1.656677937507629, 1.6658005475997926, 1.6601345062255861, 1.658985733985901, 1.6575984835624693, 1.6528475046157838, 1.6594456315040589, 1.6564586877822876, 1.6577516555786134]
Train Accuracy: 0.37058824
Test Accuracy: 0.36666667
